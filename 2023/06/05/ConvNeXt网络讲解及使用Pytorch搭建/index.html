<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logosc/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logosc/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"linvilyao.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":14,"offset":10},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础13.1和13.2》，作为随堂笔记。看ConvNeXt对标Swin Transformer怎么给卷积神经网络拉回场子。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建">
<meta property="og:url" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/index.html">
<meta property="og:site_name" content="Linvil&#39;s Blog">
<meta property="og:description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础13.1和13.2》，作为随堂笔记。看ConvNeXt对标Swin Transformer怎么给卷积神经网络拉回场子。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt%E5%AF%B9%E6%AF%94Swin-Tranformer%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%B8%80%E7%B3%BB%E5%88%97%E5%AE%9E%E9%AA%8C%E5%87%86%E7%A1%AE%E7%8E%87%E5%AF%B9%E6%AF%94.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9%E7%89%88ResNet.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/dw%E5%8D%B7%E7%A7%AF.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9ResNet-%E5%A2%9E%E5%A4%A7%E6%B7%B1%E5%BA%A6.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9Inverted-Bottleneck.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/Large_kerner_size.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/Swin-Transformer-Block%E5%92%8CResNet-Block%E4%BB%A5%E5%8F%8AConvNeXt-Block%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9ResNet-%E4%B8%8B%E9%87%87%E6%A0%B7%E5%B1%82.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-T%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E8%A1%A8.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-T%E7%BB%93%E6%9E%84%E5%9B%BE.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-Block%E7%BB%93%E6%9E%84%E5%9B%BE.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-T%E7%BB%93%E6%9E%84%E5%9B%BE.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png">
<meta property="article:published_time" content="2023-06-05T12:48:56.000Z">
<meta property="article:modified_time" content="2023-06-14T04:08:45.492Z">
<meta property="article:author" content="Linvil Yao">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="CNN网络详解">
<meta property="article:tag" content="Pytorch搭建CNN">
<meta property="article:tag" content="ConvNeXt">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt%E5%AF%B9%E6%AF%94Swin-Tranformer%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png">


<link rel="canonical" href="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/","path":"2023/06/05/ConvNeXt网络讲解及使用Pytorch搭建/","title":"深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建 | Linvil's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linvil's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">这是一个用来记录的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">6</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">30</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">47</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">网络结构学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88"><span class="nav-number">1.2.</span> <span class="nav-text">设计方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Macro-design"><span class="nav-number">1.3.</span> <span class="nav-text">Macro design</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNeXt"><span class="nav-number">1.4.</span> <span class="nav-text">ResNeXt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverted-Bottleneck"><span class="nav-number">1.5.</span> <span class="nav-text">Inverted Bottleneck</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Large-Kernel-Sizes"><span class="nav-number">1.6.</span> <span class="nav-text">Large Kernel Sizes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Micro-Design"><span class="nav-number">1.7.</span> <span class="nav-text">Micro Design</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ConvNeXt%E5%8F%82%E6%95%B0"><span class="nav-number">1.8.</span> <span class="nav-text">ConvNeXt参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ConvNeXt-T%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="nav-number">1.9.</span> <span class="nav-text">ConvNeXt-T结构图</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">工程目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">搭建网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6"><span class="nav-number">3.1.</span> <span class="nav-text">模型文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DropPath%E7%B1%BB"><span class="nav-number">3.1.1.</span> <span class="nav-text">DropPath类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LayerNorm%E7%B1%BB"><span class="nav-number">3.1.2.</span> <span class="nav-text">LayerNorm类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Block%E7%B1%BB"><span class="nav-number">3.1.3.</span> <span class="nav-text">Block类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ConvNeXt%E7%B1%BB"><span class="nav-number">3.1.4.</span> <span class="nav-text">ConvNeXt类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.5.</span> <span class="nav-text">实例化模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#train"><span class="nav-number">3.2.</span> <span class="nav-text">train</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="nav-number">3.2.1.</span> <span class="nav-text">训练结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#predict"><span class="nav-number">3.3.</span> <span class="nav-text">predict</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">3.3.1.</span> <span class="nav-text">预测结果</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Linvil Yao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Linvil Yao</p>
  <div class="site-description" itemprop="description">Welcome to Linvil's Blog!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


        </div>
      </div>


  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/26/mmeval%E6%8C%87%E6%A0%87%E4%BB%A3%E7%A0%81%E7%94%9F%E5%95%83%E4%B9%8BF1Score/" title="2023&#x2F;07&#x2F;26&#x2F;mmeval指标代码生啃之F1Score&#x2F;">mmeval指标代码生啃之F1Score</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/24/mmeval%E6%8C%87%E6%A0%87%E4%BB%A3%E7%A0%81%E7%94%9F%E5%95%83%E4%B9%8BAveragePrecision/" title="2023&#x2F;07&#x2F;24&#x2F;mmeval指标代码生啃之AveragePrecision&#x2F;">mmeval指标代码生啃之AveragePrecision</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/20/%E5%BC%80%E5%AD%A6%E7%AF%87%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" title="2023&#x2F;07&#x2F;20&#x2F;开学篇之图像分割评价指标&#x2F;">开学篇之图像分割评价指标</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/19/%E5%BC%80%E5%AD%A6%E7%AF%87%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" title="2023&#x2F;07&#x2F;19&#x2F;开学篇之目标检测评价指标&#x2F;">开学篇之目标检测评价指标</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/18/%E5%BC%80%E5%AD%A6%E7%AF%87%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" title="2023&#x2F;07&#x2F;18&#x2F;开学篇之图像分类评价指标&#x2F;">开学篇之图像分类评价指标</a>
        </li>
    </ul>
  </div>
    </div>


    



  </aside>






    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://linvilyao.github.io/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Linvil Yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linvil's Blog">
      <meta itemprop="description" content="Welcome to Linvil's Blog!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建 | Linvil's Blog">
      <meta itemprop="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础13.1和13.2》，作为随堂笔记。看ConvNeXt对标Swin Transformer怎么给卷积神经网络拉回场子。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-05 20:48:56" itemprop="dateCreated datePublished" datetime="2023-06-05T20:48:56+08:00">2023-06-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-14 12:08:45" itemprop="dateModified" datetime="2023-06-14T12:08:45+08:00">2023-06-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span id="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Valine：</span>
  
    <a title="valine" href="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>22k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

            <div class="post-description">本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础13.1和13.2》，作为随堂笔记。看ConvNeXt对标Swin Transformer怎么给卷积神经网络拉回场子。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1>网络结构学习</h1>
<h2 id="前言">前言</h2>
<p>自从<code>ViT(Vision Transformer)</code>在CV领域大放异彩，越来越多的研究人员开始拥入<code>Transformer</code>的怀抱，而卷积神经网络已经开始慢慢淡出舞台中央。20221月，Facebook AI Research和UC Berkeley一起发表了一篇文章<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a>，在文章中提出了ConvNeXt纯卷积神经网络，它对标的是2021年非常火的Swin Transformer，通过一系列实验比对，在相同的FLOPs下，ConvNeXt相比Swin Transformer拥有更快的推理速度以及更高的准确率，在ImageNet 22K上ConvNeXt-XL达到了87.8%的准确率（下图所示）。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt%E5%AF%B9%E6%AF%94Swin-Tranformer%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png" alt="ConvNeXt对比Swin-Tranformer的性能对比"></p>
<p><code>ConvNeXt</code>其实“毫无亮点”，因为它使用的全部都是现有的结构和方法，没有任何结构或者方法的创新。而且源码也非常的精简，100多行代码就能搭建完成。而<code>Swin Transformer</code>的滑动窗口，相对位置索引等不仅原理理解起来很吃力，源码也非常多，但<code>Swin Transformer</code>确实十分成功并且设计的非常巧妙。</p>
<p>作者认为，基于<code>Transformer</code>架构的模型效果比卷积神经网络要好的原因可能在于随着技术的不断发展，各种新的架构以及优化策略促使<code>Transformer</code>模型的效果更好。所以作者想要使用相同的策略去训练卷积神经网络看看能不能达到和Transformer一样的效果，因此做了一系列实验。</p>
<h2 id="设计方案">设计方案</h2>
<p>作者首先利用训练<code>vision Transformers</code>的策略去训练原始的<code>ResNet50</code>模型，得出效果为78.8，并将此结果作为后续实验的基准<code>baseline</code>。如下图所示，ConvNeXt-T/B达到的精度有82.0%，Swin-T/B为81.3%。</p>
<p>ConvNeXt设计与实验：</p>
<ul>
<li>Macro design</li>
<li>ResNeXt</li>
<li>Inverted bottleneck</li>
<li>Large kerner size</li>
<li>Various layer-wise micro designs</li>
</ul>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%B8%80%E7%B3%BB%E5%88%97%E5%AE%9E%E9%AA%8C%E5%87%86%E7%A1%AE%E7%8E%87%E5%AF%B9%E6%AF%94.png" alt="一系列实验准确率对比"></p>
<h2 id="Macro-design">Macro design</h2>
<p>如上图所示，分为两个小部分，分别是stage ratio和“patchify” stem。对应的举措分别是讲ResNet-50中堆叠的底数由<code>（3，4，6，3）</code>调整为<code>（3，3，9，3）</code>（为了能和Swin Tranformer中的比例保持一致）、将<code>stem换成卷积核大小为4且步距为4</code>的卷积层。</p>
<p><strong>Changing stage compute ratio</strong>，在原<code>ResNet</code>网络中，一般<code>conv4_x</code>（即<code>stage3</code>）堆叠的block的次数是最多的。如下图中的<code>ResNet50</code>中<code>stage1</code>到<code>stage4</code>堆叠block的次数是<code>(3, 4, 6, 3)</code>比例大概是<code>1:1:2:1</code>，但在<code>Swin Transformer</code>中，比如<code>Swin-T</code>的比例是<code>1:1:3:1</code>，<code>Swin-L</code>的比例是<code>1:1:9:1</code>。很明显，在<code>Swin Transformer</code>中，<code>stage3</code>堆叠block的占比更高。所以作者就将<code>ResNet50</code>中的堆叠次数由<code>(3, 4, 6, 3)</code>调整成<code>(3, 3, 9, 3)</code>，和<code>Swin-T</code>拥有相似的<code>FLOPs</code>。进行调整后，准确率由<code>78.8%</code>提升到了<code>79.4%</code>。</p>
<p><strong>Changing stem to “Patchify”</strong>，在之前的卷积神经网络中，一般最初的下采样模块<code>stem</code>一般都是通过一个卷积核大小为<code>7x7</code>步距为2的卷积层以及一个步距为2的最大池化下采样共同组成，高和宽都下采样4倍。但在<code>Transformer</code>模型中一般都是通过一个卷积核非常大且相邻窗口之间没有重叠的（<code>即stride等于kernel_size</code>）卷积层进行下采样。比如在<code>Swin Transformer</code>中采用的是一个卷积核大小为<code>4x4</code>步距为4的卷积层构成<code>patchify</code>，同样是下采样4倍。所以作者将<code>ResNet</code>中的<code>stem</code>也换成了和<code>Swin Transformer</code>一样的<code>patchify</code>。替换后准确率从<code>79.4%</code> 提升到<code>79.5%</code>，并且FLOPs也降低了一点。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9%E7%89%88ResNet.png" alt="修改版ResNet"></p>
<h2 id="ResNeXt">ResNeXt</h2>
<p><strong>Depth conv</strong></p>
<p>作者借鉴了<code>ResNeXt</code>中的组卷积<code>grouped convolution</code>，因为<code>ResNeXt</code>相比普通的<code>ResNet</code>而言在FLOPs以及accuracy之间做到了更好的平衡。这里作者采用的是更激进的<code>depthwise convolution</code>（<strong>groups数和输入特征矩阵的深度相等</strong>），即group数和通道数channel相同，且作者认为<code>depthwise convolution</code>和<code>self-attention</code>中的加权求和操作很相似。</p>
<p>下图（上左）所采用的是ResNet所采用的瓶颈结构，即两头粗中间细的结构（<code>256-&gt;64-&gt;256</code>）。而在ResNeXt中采用的是下图（上右）的结构。二者之间唯一的区别就是在中间3x3的卷积层部分，在ResNet中，3x3的卷积层就是一个普通卷积层，但在ResNeXt中采用的是组卷积。</p>
<p>DW卷积如下图（下）所示，这里就不细讲了，可以回到ResNeXt中去看看详细过程。</p>
<p>通过上述改成激进的DW卷积之后，准确率从<code>79.5%</code>降到了<code>78%</code>。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/dw%E5%8D%B7%E7%A7%AF.png" alt="DW卷积"></p>
<p><strong>增大width（图片channel）</strong>，将ResNet50对标Swin-T，前者第一个Stage输入特征矩阵的深度为64，后者第一个Stage输入的channel为96。因此，作者将ResNet的每一个Stage的输入特征矩阵的channel与Swin-T中每个Stage的输入特征矩阵的channel保持一致。于是准确率从<code>78%</code>升到了<code>80.5%</code>，虽然同时FLOPs也增长了很多。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9ResNet-%E5%A2%9E%E5%A4%A7%E6%B7%B1%E5%BA%A6.png" alt="修改ResNet-增大深度"></p>
<h2 id="Inverted-Bottleneck">Inverted Bottleneck</h2>
<p>作者认为<code>Transformer block</code>中的<code>MLP</code>模块非常像<code>MobileNetV2</code>中的<code>Inverted Bottleneck</code>模块，即两头细中间粗。下图a是<code>ReNet</code>中采用的<code>Bottleneck</code>模块，b是<code>MobileNetV2</code>采用的<code>Inverted Botleneck</code>模块，c是<code>ConvNeXt</code>采用的是<code>Inverted Bottleneck</code>模块。</p>
<p>作者采用<code>Inverted Bottleneck</code>模块后，在较小的模型上准确率由<code>80.5%</code>提升到了<code>80.6%</code>，在较大的模型上准确率由<code>81.9%</code>提升到<code>82.6%</code>。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9Inverted-Bottleneck.png" alt="修改Inverted Bottleneck"></p>
<h2 id="Large-Kernel-Sizes">Large Kernel Sizes</h2>
<p>在<code>Transformer</code>中一般都是对全局做<code>self-attention</code>，比如<code>Vision Transformer</code>。即使是<code>Swin Transformer</code>也有<code>7x7</code>大小的窗口。但现在主流的卷积神经网络都是采用<code>3x3</code>大小的窗口，因为之前<code>VGG</code>论文中说通过堆叠多个<code>3x3</code>的窗口可以替代一个更大的窗口，而且现在的GPU设备针对<code>3x3</code>大小的卷积核做了很多的优化，所以会更高效。接着作者做了如下两个改动：</p>
<p><strong>Moving up depthwise conv layer</strong>，即将<code>depthwise conv</code>模块上移，原来是先通过<code>1x1 conv</code> -&gt; 再<code>depthwise conv</code> -&gt;然后 <code>1x1 conv</code>，现在变成了<code>depthwise conv</code> -&gt; <code>1x1 conv</code> -&gt; <code>1x1 conv</code>。这么做是因为在<code>Transformer</code>中，<code>MSA</code>模块是放在<code>MLP</code>模块之前的，所以这里进行效仿，将<code>depthwise conv</code>上移（也就是上图c）。这样改动后，准确率由<code>80.6%</code>下降到了<code>79.9%</code>，同时FLOPs也减小了。</p>
<p><strong>Increasing the kernel size</strong>，接着将<code>depthwise conv</code>的卷积核大小由<code>3x3</code>改成了<code>7x7</code>（和<code>Swin Transformer</code>一样），作者也做了一系列实验，除了3x3之外，还包括<code> 5, 7, 9, 11</code>发现取到7时准确率就达到了饱和。并且准确率从<code>79.9% (3×3)</code> 增长到 <code>80.6% (7×7)</code>（正好和Swin Transformer里的窗口大小是一致的，简直是玄学）。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/Large_kerner_size.png" alt="Large kerner size"></p>
<h2 id="Micro-Design">Micro Design</h2>
<p>接下来作者在聚焦到一些更细小的差异，比如激活函数将ReLU更改为GELU以及Normalization。</p>
<ul>
<li><strong>Replacing ReLU with GELU</strong>，在<code>Transformer</code>中激活函数基本用的都是<code>GELU</code>，而在卷积神经网络中最常用的是<code>ReLU</code>，于是作者将激活函数替换成了<code>GELU</code>，发现准确率没变化。</li>
<li><strong>Fewer activation functions</strong>，使用更少的激活函数。在卷积神经网络中，一般会在每个卷积层或全连接后都接上一个激活函数。但在<code>Transformer</code>中并不是每个模块后都跟有激活函数（如下图），比如<code>MLP</code>中只有第一个全连接层后跟了<code>GELU</code>激活函数。接着作者在<code>ConvNeXt Block</code>中也减少激活函数的使用，如下图所示，减少后发现准确率从<code>80.6%</code>增长到<code>81.3%</code>。</li>
<li><strong>Fewer normalization layers</strong>，使用更少的Normalization Layer。同样在<code>Transformer</code>中，Normalization使用的也比较少，接着作者也减少了<code>ConvNeXt Block</code>中的Normalization层，只保留了<code>depthwise conv</code>后的Normalization层。此时准确率已经达到了<code>81.4%</code>，已经超过了<code>Swin-T</code>。</li>
<li><strong>Substituting BN with LN</strong>，将BN替换成LN。Batch Normalization（BN）在卷积神经网络中是非常常用的模块，它可以加速网络的收敛并减少过拟合（但用的不好也是个大坑）。但在<code>Transformer</code>中基本都用的Layer Normalization（LN），因为最开始<code>Transformer</code>是应用在NLP领域的，BN又不适用于NLP相关任务。接着作者将BN全部替换成了LN，发现准确率提升达到了<code>81.5%</code>。</li>
</ul>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/Swin-Transformer-Block%E5%92%8CResNet-Block%E4%BB%A5%E5%8F%8AConvNeXt-Block%E7%BB%93%E6%9E%84.png" alt="Swin Transformer Block和ResNet Block以及ConvNeXt Block结构"></p>
<ul>
<li><strong>Separate downsampling layers</strong>，单独的下采样层。在<code>ResNet</code>网络中<code>stage2-4</code>的第一个block都具有下采样功能（下图左），且都是通过将主分支上<code>3x3</code>的卷积层步距设置成2，捷径分支上<code>1x1</code>的卷积层步距设置成2进行下采样的。但在<code>Swin Transformer</code>中是通过一个单独的<code>Patch Merging</code>实现的（下图右）。接着作者就为<code>ConvNext</code>网络单独使用了一个下采样层，就是通过一个Laryer Normalization加上一个卷积核大小为2步距为2的卷积层构成。更改后准确率就提升到了<code>82.0%</code>，对应的就是ConvNeXt-T这个精度。</li>
</ul>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E4%BF%AE%E6%94%B9ResNet-%E4%B8%8B%E9%87%87%E6%A0%B7%E5%B1%82.png" alt="修改ResNet-下采样层"></p>
<h2 id="ConvNeXt参数">ConvNeXt参数</h2>
<p>对于<code>ConvNeXt</code>网络，作者提出了<code>T/S/B/L</code>四个版本，计算复杂度刚好和<code>Swin Transformer</code>中的<code>T/S/B/L</code>相似。C代表4个<code>stage</code>中输入的通道数，B代表每个<code>stage</code>重复堆叠block的次数。</p>
<ul>
<li><strong>ConvNeXt-T</strong>: C = (96, 192, 384, 768), B = (3, 3, 9, 3)</li>
<li><strong>ConvNeXt-S</strong>: C = (96, 192, 384, 768), B = (3, 3, 27, 3)</li>
<li><strong>ConvNeXt-B</strong>: C = (128, 256, 512, 1024), B = (3, 3, 27, 3)</li>
<li><strong>ConvNeXt-L</strong>: C = (192, 384, 768, 1536), B = (3, 3, 27, 3)</li>
<li><strong>ConvNeXt-XL</strong>: C = (256, 512, 1024, 2048), B = (3, 3, 27, 3)</li>
</ul>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-T%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E8%A1%A8.png" alt="ConvNeXt-T配置参数表"></p>
<h2 id="ConvNeXt-T结构图">ConvNeXt-T结构图</h2>
<p>up根据官方源码手绘的<code>ConvNeXt-T</code>网络结构图，仔细观察<code>ConvNeXt Block</code>会发现其中还有一个<code>Layer Scale</code>操作（论文中并没有提到），<strong>其实它就是将输入的特征层乘上一个可训练的参数，该参数就是一个向量</strong>，元素个数与特征层channel相同，即对每个channel的数据进行缩放。</p>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-T%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="ConvNeXt-T结构图"></p>
<h1>工程目录</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── Test14_ConvNeXt</span><br><span class="line">	├── model.py（模型文件）   </span><br><span class="line">	├── my_dataset.py（数据处理文件）    </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,ConvNeXt.pth文件）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── utils.py（工具文件，用得上就对了）  </span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	└── convnext_tiny_1k_224_ema.pth（迁移学习，提前下载好convnext_tiny_1k_224_ema.pth权重脚本）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<h1>搭建网络</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">original code from facebook research:</span></span><br><span class="line"><span class="string">https://github.com/facebookresearch/ConvNeXt</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_last&quot;</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, drop_rate=<span class="number">0.</span>, layer_scale_init_value=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNeXt</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_chans: <span class="built_in">int</span> = <span class="number">3</span>, num_classes: <span class="built_in">int</span> = <span class="number">1000</span>, depths: <span class="built_in">list</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 dims: <span class="built_in">list</span> = <span class="literal">None</span>, drop_path_rate: <span class="built_in">float</span> = <span class="number">0.</span>, layer_scale_init_value: <span class="built_in">float</span> = <span class="number">1e-6</span>,</span></span><br><span class="line"><span class="params">                 head_init_scale: <span class="built_in">float</span> = <span class="number">1.</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_tiny</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_small</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_base</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_large</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_xlarge</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h2 id="模型文件">模型文件</h2>
<h3 id="DropPath类">DropPath类</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,</span></span><br><span class="line"><span class="string">    the original name is misleading as &#x27;Drop Connect&#x27; is a different form of dropout in a separate paper...</span></span><br><span class="line"><span class="string">    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I&#x27;ve opted for</span></span><br><span class="line"><span class="string">    changing the layer and argument names to &#x27;drop path&#x27; rather than mix DropConnect as a layer name and use</span></span><br><span class="line"><span class="string">    &#x27;survival rate&#x27; as the argument.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)  <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DropPath, self).__init__()</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> drop_path(x, self.drop_prob, self.training)</span><br></pre></td></tr></table></figure>
<h3 id="LayerNorm类">LayerNorm类</h3>
<p>实际上，官方有现成的LayerNorm方法，但是Pytorch实现的LayerNorm方法默认是从最后一个维度开始做Normalization。在ConvNeXt网络当中，是对channel维度进行LayerNorm处理的，如果说channel是放在最后一个维度的话，就可以用官方给的LayerNorm方法，但如果channel这个维度没有放在最后的话，就不能使用官方的方法，所以作者重写了LayerNorm方法。</p>
<p><code>data_format</code>有两种形式，分别为<code>channels_last</code>和<code>channels_first</code>，分别对应着channel这个维度放在最后面（<code>batch_size, height, width, channels</code>）和没有放在最后一个位置（<code>batch_size, channels, height, width</code>，这通常是Pytorch默认的通道排列顺序）。</p>
<p><code>weight</code>和<code>bias</code>分别对应着LayerNorm过程中的$\alpha$和$\beta$。</p>
<p>如果data_format是channels_first时，作者重写了这块的代码，首先对channel这个维度求均值（也就是对应dim为1的位置）得到<code>mean</code>，接下来求方差<code>var</code>，以及<code>标准差</code>处理。</p>
<p>最后再乘以<code>weight</code>并加上<code>bias</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; LayerNorm that supports two data formats: channels_last (default) or channels_first.</span></span><br><span class="line"><span class="string">    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with</span></span><br><span class="line"><span class="string">    shape (batch_size, height, width, channels) while channels_first corresponds to inputs</span></span><br><span class="line"><span class="string">    with shape (batch_size, channels, height, width).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_last&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.ones(normalized_shape), requires_grad=<span class="literal">True</span>)</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(normalized_shape), requires_grad=<span class="literal">True</span>)</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.data_format = data_format</span><br><span class="line">        <span class="keyword">if</span> self.data_format <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;channels_last&quot;</span>, <span class="string">&quot;channels_first&quot;</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;not support data format &#x27;<span class="subst">&#123;self.data_format&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">        self.normalized_shape = (normalized_shape,)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">if</span> self.data_format == <span class="string">&quot;channels_last&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)</span><br><span class="line">        <span class="keyword">elif</span> self.data_format == <span class="string">&quot;channels_first&quot;</span>:</span><br><span class="line">            <span class="comment"># [batch_size, channels, height, width]</span></span><br><span class="line">            mean = x.mean(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = (x - mean).<span class="built_in">pow</span>(<span class="number">2</span>).mean(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            x = (x - mean) / torch.sqrt(var + self.eps)</span><br><span class="line">            x = self.weight[:, <span class="literal">None</span>, <span class="literal">None</span>] * x + self.bias[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Block类">Block类</h3>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-Block%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="ConvNeXt Block结构图"></p>
<p><code>self.gamma</code>对应的是上图的<code>Layer Scale</code>，元素个数和输入特征层channel的个数是相同的（<strong>其实它就是将输入的特征层乘上一个可训练的参数，该参数就是一个向量</strong>，元素个数与特征层channel相同，即对每个channel的数据进行缩放）</p>
<p>在正向传播过程中，通过第一个<code>permute</code>方法调换Pytorch默认的通道排列顺序，即<code>[N, C, H, W] -&gt; [N, H, W, C]</code>，再通过第二个<code>permute</code>方法将修改后的通道排列顺序给还原回去，即<code>[N, H, W, C] -&gt; [N, C, H, W]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; ConvNeXt Block. There are two equivalent implementations:</span></span><br><span class="line"><span class="string">    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)</span></span><br><span class="line"><span class="string">    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back</span></span><br><span class="line"><span class="string">    We use (2) as we find it slightly faster in PyTorch</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        drop_rate (float): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, drop_rate=<span class="number">0.</span>, layer_scale_init_value=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dwconv = nn.Conv2d(dim, dim, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, groups=dim)  <span class="comment"># depthwise conv</span></span><br><span class="line">        self.norm = LayerNorm(dim, eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_last&quot;</span>)</span><br><span class="line">        self.pwconv1 = nn.Linear(dim, <span class="number">4</span> * dim)  <span class="comment"># pointwise/1x1 convs, implemented with linear layers</span></span><br><span class="line">        self.act = nn.GELU()</span><br><span class="line">        self.pwconv2 = nn.Linear(<span class="number">4</span> * dim, dim)</span><br><span class="line">        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),</span><br><span class="line">                                  requires_grad=<span class="literal">True</span>) <span class="keyword">if</span> layer_scale_init_value &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.drop_path = DropPath(drop_rate) <span class="keyword">if</span> drop_rate &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        shortcut = x</span><br><span class="line">        x = self.dwconv(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># [N, C, H, W] -&gt; [N, H, W, C]</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.pwconv1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.pwconv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.gamma <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.gamma * x</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [N, H, W, C] -&gt; [N, C, H, W]</span></span><br><span class="line"></span><br><span class="line">        x = shortcut + self.drop_path(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="ConvNeXt类">ConvNeXt类</h3>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/ConvNeXt-T%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="ConvNeXt-T结构图"></p>
<p><strong>ConvNeXt-T</strong>: C = (96, 192, 384, 768), B = (3, 3, 9, 3)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNeXt</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; ConvNeXt</span></span><br><span class="line"><span class="string">        A PyTorch impl of : `A ConvNet for the 2020s`  -</span></span><br><span class="line"><span class="string">          https://arxiv.org/pdf/2201.03545.pdf</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3</span></span><br><span class="line"><span class="string">        num_classes (int): Number of classes for classification head. Default: 1000</span></span><br><span class="line"><span class="string">        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]</span></span><br><span class="line"><span class="string">        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]</span></span><br><span class="line"><span class="string">        drop_path_rate (float): Stochastic depth rate. Default: 0.</span></span><br><span class="line"><span class="string">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span></span><br><span class="line"><span class="string">        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_chans: <span class="built_in">int</span> = <span class="number">3</span>, num_classes: <span class="built_in">int</span> = <span class="number">1000</span>, depths: <span class="built_in">list</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 dims: <span class="built_in">list</span> = <span class="literal">None</span>, drop_path_rate: <span class="built_in">float</span> = <span class="number">0.</span>, layer_scale_init_value: <span class="built_in">float</span> = <span class="number">1e-6</span>,</span></span><br><span class="line"><span class="params">                 head_init_scale: <span class="built_in">float</span> = <span class="number">1.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.downsample_layers = nn.ModuleList()  <span class="comment"># stem and 3 intermediate downsampling conv layers</span></span><br><span class="line">        stem = nn.Sequential(nn.Conv2d(in_chans, dims[<span class="number">0</span>], kernel_size=<span class="number">4</span>, stride=<span class="number">4</span>),</span><br><span class="line">                             LayerNorm(dims[<span class="number">0</span>], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>))</span><br><span class="line">        self.downsample_layers.append(stem)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对应stage2-stage4前的3个downsample</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            downsample_layer = nn.Sequential(LayerNorm(dims[i], eps=<span class="number">1e-6</span>, data_format=<span class="string">&quot;channels_first&quot;</span>),</span><br><span class="line">                                             nn.Conv2d(dims[i], dims[i+<span class="number">1</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">            self.downsample_layers.append(downsample_layer)</span><br><span class="line"></span><br><span class="line">        self.stages = nn.ModuleList()  <span class="comment"># 4 feature resolution stages, each consisting of multiple blocks</span></span><br><span class="line">        <span class="comment"># 等差数列</span></span><br><span class="line">        dp_rates = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]</span><br><span class="line">        cur = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 构建每个stage中堆叠的block</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            stage = nn.Sequential(</span><br><span class="line">                *[Block(dim=dims[i], drop_rate=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value)</span><br><span class="line">                  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(depths[i])]</span><br><span class="line">            )</span><br><span class="line">            self.stages.append(stage)</span><br><span class="line">            <span class="comment"># cur代表在当前Stage之前构建好了的block的个数</span></span><br><span class="line">            cur += depths[i]</span><br><span class="line"></span><br><span class="line">        self.norm = nn.LayerNorm(dims[-<span class="number">1</span>], eps=<span class="number">1e-6</span>)  <span class="comment"># final norm layer</span></span><br><span class="line">        self.head = nn.Linear(dims[-<span class="number">1</span>], num_classes)</span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line">        <span class="comment"># 对head，也就是Linear层对weight和bias乘上了head_init_scale（默认为1）</span></span><br><span class="line">        self.head.weight.data.mul_(head_init_scale)</span><br><span class="line">        self.head.bias.data.mul_(head_init_scale)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (nn.Conv2d, nn.Linear)):</span><br><span class="line">            nn.init.trunc_normal_(m.weight, std=<span class="number">0.2</span>)</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            x = self.downsample_layers[i](x)</span><br><span class="line">            x = self.stages[i](x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 相当于做了个Goble Avg Pooling操作</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x.mean([-<span class="number">2</span>, -<span class="number">1</span>]))  <span class="comment"># global average pooling, (N, C, H, W) -&gt; (N, C)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="实例化模型">实例化模型</h3>
<ul>
<li><strong>ConvNeXt-T</strong>: C = (96, 192, 384, 768), B = (3, 3, 9, 3)</li>
<li><strong>ConvNeXt-S</strong>: C = (96, 192, 384, 768), B = (3, 3, 27, 3)</li>
<li><strong>ConvNeXt-B</strong>: C = (128, 256, 512, 1024), B = (3, 3, 27, 3)</li>
<li><strong>ConvNeXt-L</strong>: C = (192, 384, 768, 1536), B = (3, 3, 27, 3)</li>
<li><strong>ConvNeXt-XL</strong>: C = (256, 512, 1024, 2048), B = (3, 3, 27, 3)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_tiny</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth</span></span><br><span class="line">    model = ConvNeXt(depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">                     dims=[<span class="number">96</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>],</span><br><span class="line">                     num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_small</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth</span></span><br><span class="line">    model = ConvNeXt(depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">27</span>, <span class="number">3</span>],</span><br><span class="line">                     dims=[<span class="number">96</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>],</span><br><span class="line">                     num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_base</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth</span></span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth</span></span><br><span class="line">    model = ConvNeXt(depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">27</span>, <span class="number">3</span>],</span><br><span class="line">                     dims=[<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>],</span><br><span class="line">                     num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_large</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth</span></span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth</span></span><br><span class="line">    model = ConvNeXt(depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">27</span>, <span class="number">3</span>],</span><br><span class="line">                     dims=[<span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>, <span class="number">1536</span>],</span><br><span class="line">                     num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convnext_xlarge</span>(<span class="params">num_classes: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth</span></span><br><span class="line">    model = ConvNeXt(depths=[<span class="number">3</span>, <span class="number">3</span>, <span class="number">27</span>, <span class="number">3</span>],</span><br><span class="line">                     dims=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</span><br><span class="line">                     num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> </span><br></pre></td></tr></table></figure>
<h2 id="train">train</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> my_dataset <span class="keyword">import</span> MyDataSet</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> convnext_tiny <span class="keyword">as</span> create_model</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> read_split_data, create_lr_scheduler, get_params_groups, train_one_epoch, evaluate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    device = torch.device(args.device <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;using <span class="subst">&#123;device&#125;</span> device.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(<span class="string">&quot;./weights&quot;</span>) <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">        os.makedirs(<span class="string">&quot;./weights&quot;</span>)</span><br><span class="line"></span><br><span class="line">    tb_writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">    train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args.data_path)</span><br><span class="line"></span><br><span class="line">    img_size = <span class="number">224</span></span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(img_size),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(<span class="built_in">int</span>(img_size * <span class="number">1.143</span>)),</span><br><span class="line">                                   transforms.CenterCrop(img_size),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化训练数据集</span></span><br><span class="line">    train_dataset = MyDataSet(images_path=train_images_path,</span><br><span class="line">                              images_class=train_images_label,</span><br><span class="line">                              transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化验证数据集</span></span><br><span class="line">    val_dataset = MyDataSet(images_path=val_images_path,</span><br><span class="line">                            images_class=val_images_label,</span><br><span class="line">                            transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line"></span><br><span class="line">    batch_size = args.batch_size</span><br><span class="line">    nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                               batch_size=batch_size,</span><br><span class="line">                                               shuffle=<span class="literal">True</span>,</span><br><span class="line">                                               pin_memory=<span class="literal">True</span>,</span><br><span class="line">                                               num_workers=nw,</span><br><span class="line">                                               collate_fn=train_dataset.collate_fn)</span><br><span class="line"></span><br><span class="line">    val_loader = torch.utils.data.DataLoader(val_dataset,</span><br><span class="line">                                             batch_size=batch_size,</span><br><span class="line">                                             shuffle=<span class="literal">False</span>,</span><br><span class="line">                                             pin_memory=<span class="literal">True</span>,</span><br><span class="line">                                             num_workers=nw,</span><br><span class="line">                                             collate_fn=val_dataset.collate_fn)</span><br><span class="line"></span><br><span class="line">    model = create_model(num_classes=args.num_classes).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.weights != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="keyword">assert</span> os.path.exists(args.weights), <span class="string">&quot;weights file: &#x27;&#123;&#125;&#x27; not exist.&quot;</span>.<span class="built_in">format</span>(args.weights)</span><br><span class="line">        weights_dict = torch.load(args.weights, map_location=device)[<span class="string">&quot;model&quot;</span>]</span><br><span class="line">        <span class="comment"># 删除有关分类类别的权重</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">list</span>(weights_dict.keys()):</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;head&quot;</span> <span class="keyword">in</span> k:</span><br><span class="line">                <span class="keyword">del</span> weights_dict[k]</span><br><span class="line">        <span class="built_in">print</span>(model.load_state_dict(weights_dict, strict=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.freeze_layers:</span><br><span class="line">        <span class="keyword">for</span> name, para <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="comment"># 除head外，其他权重全部冻结</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;head&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">                para.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;training &#123;&#125;&quot;</span>.<span class="built_in">format</span>(name))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># pg = [p for p in model.parameters() if p.requires_grad]</span></span><br><span class="line">    pg = get_params_groups(model, weight_decay=args.wd)</span><br><span class="line">    optimizer = optim.AdamW(pg, lr=args.lr, weight_decay=args.wd)</span><br><span class="line">    lr_scheduler = create_lr_scheduler(optimizer, <span class="built_in">len</span>(train_loader), args.epochs,</span><br><span class="line">                                       warmup=<span class="literal">True</span>, warmup_epochs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        <span class="comment"># train</span></span><br><span class="line">        train_loss, train_acc = train_one_epoch(model=model,</span><br><span class="line">                                                optimizer=optimizer,</span><br><span class="line">                                                data_loader=train_loader,</span><br><span class="line">                                                device=device,</span><br><span class="line">                                                epoch=epoch,</span><br><span class="line">                                                lr_scheduler=lr_scheduler)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        val_loss, val_acc = evaluate(model=model,</span><br><span class="line">                                     data_loader=val_loader,</span><br><span class="line">                                     device=device,</span><br><span class="line">                                     epoch=epoch)</span><br><span class="line"></span><br><span class="line">        tags = [<span class="string">&quot;train_loss&quot;</span>, <span class="string">&quot;train_acc&quot;</span>, <span class="string">&quot;val_loss&quot;</span>, <span class="string">&quot;val_acc&quot;</span>, <span class="string">&quot;learning_rate&quot;</span>]</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">0</span>], train_loss, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">1</span>], train_acc, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">2</span>], val_loss, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">3</span>], val_acc, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">4</span>], optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>], epoch)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> best_acc &lt; val_acc:</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">&quot;./weights/best_model.pth&quot;</span>)</span><br><span class="line">            best_acc = val_acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_classes&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">3</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-4</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--wd&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集所在根目录</span></span><br><span class="line">    <span class="comment"># https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--data-path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        default=<span class="string">&quot;D:/python_test/deep-learning-for-image-processing/data_set/flower_data/flower_photos&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预训练权重路径，如果不想载入就设置为空字符</span></span><br><span class="line">    <span class="comment"># 链接: https://pan.baidu.com/s/1aNqQW4n_RrUlWUBNlaJRHA  密码: i83t</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weights&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;./convnext_tiny_1k_224_ema.pth&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;initial weights path&#x27;</span>)</span><br><span class="line">    <span class="comment"># 是否冻结head以外所有权重</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--freeze-layers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">False</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--device&#x27;</span>, default=<span class="string">&#x27;cuda:0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;device id (i.e. 0 or 0,1 or cpu)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(opt)</span><br></pre></td></tr></table></figure>
<h3 id="训练结果">训练结果</h3>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="训练结果"></p>
<h2 id="predict">predict</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> convnext_tiny <span class="keyword">as</span> create_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;using <span class="subst">&#123;device&#125;</span> device.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    num_classes = <span class="number">5</span></span><br><span class="line">    img_size = <span class="number">224</span></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(<span class="built_in">int</span>(img_size * <span class="number">1.14</span>)),</span><br><span class="line">         transforms.CenterCrop(img_size),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    img_path = <span class="string">&quot;tulip.jpg&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    img = data_transform(img)</span><br><span class="line">    <span class="comment"># expand batch dimension</span></span><br><span class="line">    img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    model = create_model(num_classes=num_classes).to(device)</span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./weights/best_model.pth&quot;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_weight_path, map_location=device))</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># predict class</span></span><br><span class="line">        output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">        predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">        predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">    print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                                 predict[predict_cla].numpy())</span><br><span class="line">    plt.title(print_res)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                                  predict[i].numpy()))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="预测结果">预测结果</h3>
<p><img src="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="预测结果"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
              <a href="/tags/CNN%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" rel="tag"><i class="fa fa-tag"></i> CNN网络详解</a>
              <a href="/tags/Pytorch%E6%90%AD%E5%BB%BACNN/" rel="tag"><i class="fa fa-tag"></i> Pytorch搭建CNN</a>
              <a href="/tags/ConvNeXt/" rel="tag"><i class="fa fa-tag"></i> ConvNeXt</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/04/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BASwin-Transformer%E7%BD%91%E7%BB%9C/" rel="prev" title="深度学习模型之CNN（二十四）使用Pytorch搭建Swin Transformer网络">
                  <i class="fa fa-chevron-left"></i> 深度学习模型之CNN（二十四）使用Pytorch搭建Swin Transformer网络
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/10/MobileViT%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E9%80%9A%E8%BF%87Pytorch%E6%90%AD%E5%BB%BAMobileViT%E7%BD%91%E7%BB%9C/" rel="next" title="深度学习模型之CNN（二十六）MobileViT网络讲解及通过Pytorch搭建">
                  深度学习模型之CNN（二十六）MobileViT网络讲解及通过Pytorch搭建 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="valine-comments"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Linvil Yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("04/21/2023 22:22:22");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "已运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","app_key":"iNfyeaWVmA11Duj7fzr0B1r1","server_url":"https://16p3s6fl.lc-cn-n1-shared.com","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '32px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>


<script class="next-config" data-name="valine" type="application/json">{"enable":true,"appId":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","appKey":"iNfyeaWVmA11Duj7fzr0B1r1","serverURLs":"https://16p3s6fl.lc-cn-n1-shared.com","placeholder":"请写下您的评论","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"lang":null,"visitor":false,"comment_count":true,"recordIP":true,"enableQQ":true,"requiredFields":[],"el":"#valine-comments","path":"/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.valine.el)
    .then(() => NexT.utils.getScript(
      'https://fastly.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
      { condition: window.Valine }
    ))
    .then(() => {
      new Valine(CONFIG.valine);
    });
});
</script>

</body>
</html>
