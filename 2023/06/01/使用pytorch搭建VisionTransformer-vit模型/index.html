<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logosc/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logosc/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"linvilyao.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":14,"offset":10},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础11.2》，作为随堂笔记，使用pytorch搭建Vision Transformer(vit)模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习模型之CNN（二十二）使用pytorch搭建Vision Transformer(vit)模型">
<meta property="og:url" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Linvil&#39;s Blog">
<meta property="og:description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础11.2》，作为随堂笔记，使用pytorch搭建Vision Transformer(vit)模型。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Patch-Embedding%E5%B1%82.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Muti-Head_Transformer.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Attention%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Muti-Head_Transformer%E4%B8%ADconcat%E6%8B%BC%E6%8E%A5.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Attention%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/MLP_Block%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Encoder_Block%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/ResNet-50+ViT-B16-hybrid-model.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png">
<meta property="article:published_time" content="2023-06-01T14:43:00.000Z">
<meta property="article:modified_time" content="2023-06-15T09:10:47.983Z">
<meta property="article:author" content="Linvil Yao">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="Pytorch搭建CNN">
<meta property="article:tag" content="Vision Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Patch-Embedding%E5%B1%82.png">


<link rel="canonical" href="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/","path":"2023/06/01/使用pytorch搭建VisionTransformer-vit模型/","title":"深度学习模型之CNN（二十二）使用pytorch搭建Vision Transformer(vit)模型"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习模型之CNN（二十二）使用pytorch搭建Vision Transformer(vit)模型 | Linvil's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linvil's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">这是一个用来记录的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">5</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">27</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">43</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">工程目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">vit_model.py</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DropPath%E7%B1%BB"><span class="nav-number">2.1.</span> <span class="nav-text">DropPath类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PatchEmbed%E7%B1%BB"><span class="nav-number">2.2.</span> <span class="nav-text">PatchEmbed类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.1.</span> <span class="nav-text">初始化函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.2.</span> <span class="nav-text">正向传播函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention%E7%B1%BB"><span class="nav-number">2.3.</span> <span class="nav-text">Attention类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0-2"><span class="nav-number">2.3.1.</span> <span class="nav-text">初始化函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E5%87%BD%E6%95%B0-2"><span class="nav-number">2.3.2.</span> <span class="nav-text">正向传播函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mlp%E7%B1%BB"><span class="nav-number">2.4.</span> <span class="nav-text">Mlp类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Block%E7%B1%BB"><span class="nav-number">2.5.</span> <span class="nav-text">Block类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VisionTransformer%E7%B1%BB"><span class="nav-number">2.6.</span> <span class="nav-text">VisionTransformer类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0-3"><span class="nav-number">2.6.1.</span> <span class="nav-text">初始化函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E5%87%BD%E6%95%B0-3"><span class="nav-number">2.6.2.</span> <span class="nav-text">正向传播函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.7.</span> <span class="nav-text">实例化模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">train.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">predict.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">flops.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">utils.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text">my_dataset.py</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Linvil Yao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Linvil Yao</p>
  <div class="site-description" itemprop="description">Welcome to Linvil's Blog!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">43</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


        </div>
      </div>


  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/18/%E5%BC%80%E5%AD%A6%E7%AF%87%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" title="2023&#x2F;07&#x2F;18&#x2F;开学篇之图像分类评价指标&#x2F;">开学篇之图像分类评价指标</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/27/PyTorch%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89Dataset%E7%B1%BB%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="2023&#x2F;06&#x2F;27&#x2F;PyTorch入门学习（一）Dataset类代码实战&#x2F;">PyTorch入门学习（一）Dataset类代码实战</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/17/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0v2-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="2023&#x2F;06&#x2F;17&#x2F;动手学深度学习v2-数据操作及数据预处理&#x2F;">动手学深度学习v2（一）数据操作及数据预处理</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/15/%E8%A1%A5%E5%85%85%E4%B9%8B%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0%E8%A7%A3%E9%87%8A/" title="2023&#x2F;06&#x2F;15&#x2F;补充之一些函数解释&#x2F;">一些函数解释（更新ing）</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/14/%E8%A1%A5%E5%85%85%E4%B9%8BTensor%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2/" title="2023&#x2F;06&#x2F;14&#x2F;补充之Tensor的维度变换&#x2F;">Tensor的维度变换</a>
        </li>
    </ul>
  </div>
    </div>


    



  </aside>






    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://linvilyao.github.io/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Linvil Yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linvil's Blog">
      <meta itemprop="description" content="Welcome to Linvil's Blog!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习模型之CNN（二十二）使用pytorch搭建Vision Transformer(vit)模型 | Linvil's Blog">
      <meta itemprop="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础11.2》，作为随堂笔记，使用pytorch搭建Vision Transformer(vit)模型。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习模型之CNN（二十二）使用pytorch搭建Vision Transformer(vit)模型
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-01 22:43:00" itemprop="dateCreated datePublished" datetime="2023-06-01T22:43:00+08:00">2023-06-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-15 17:10:47" itemprop="dateModified" datetime="2023-06-15T17:10:47+08:00">2023-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span id="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习模型之CNN（二十二）使用pytorch搭建Vision Transformer(vit)模型" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Valine：</span>
  
    <a title="valine" href="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>37k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>34 分钟</span>
    </span>
</div>

            <div class="post-description">本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础11.2》，作为随堂笔记，使用pytorch搭建Vision Transformer(vit)模型。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1>工程目录</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── vision transformer</span><br><span class="line">	├── vit_model.py（模型文件）  </span><br><span class="line">	├── my_dataset.py（数据处理文件）  </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,vision_transformer.pth文件）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── utils.py（工具文件，用得上就对了）</span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	└── vit_base_patch16_224_in21k.pth（迁移学习，提前下载好vit_base_patch16_224_in21k.pth权重脚本）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<h1>vit_model.py</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">original code from rwightman:</span></span><br><span class="line"><span class="string">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 dim,   <span class="comment"># 输入token的dim</span></span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">                 qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 proj_drop_ratio=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 dim,</span></span><br><span class="line"><span class="params">                 num_heads,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">                 qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.0</span>, qkv_bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>, representation_size=<span class="literal">None</span>, distilled=<span class="literal">False</span>, drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>, drop_path_ratio=<span class="number">0.</span>, embed_layer=PatchEmbed, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 act_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_init_vit_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch16_224</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch16_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch32_224</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch32_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_large_patch16_224</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_large_patch16_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_large_patch32_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_huge_patch14_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h2 id="DropPath类">DropPath类</h2>
<p>DropPath类在正向传播过程中直接调用drop_path方法，就是一个Stochastic Depth，在EfficientNet中有详细讲解，这里照搬过来。</p>
<blockquote>
<p>正向传播过程中将输入的特征矩阵经历了一个又一个block，每一个block都可以认为是一个残差结构。例如主分支通过$f$函数进行输出，shortcut直接从输入引到输出，在此过程中，会以一定的概率来对主分支进行丢弃（直接放弃整个主分支，相当于直接将上一层的输出引入到下一层的输入，相当于没有这一层）。即Stochastic Depth（随机深度，指的是网络的depth，因为会随机丢弃任意一层block）。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,</span></span><br><span class="line"><span class="string">    the original name is misleading as &#x27;Drop Connect&#x27; is a different form of dropout in a separate paper...</span></span><br><span class="line"><span class="string">    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I&#x27;ve opted for</span></span><br><span class="line"><span class="string">    changing the layer and argument names to &#x27;drop path&#x27; rather than mix DropConnect as a layer name and use</span></span><br><span class="line"><span class="string">    &#x27;survival rate&#x27; as the argument.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)  <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DropPath, self).__init__()</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> drop_path(x, self.drop_prob, self.training)</span><br></pre></td></tr></table></figure>
<h2 id="PatchEmbed类">PatchEmbed类</h2>
<p>对应Patch Embedding层（经过一个卷积核大小为16x16，stride = 16的卷积层，再进行一个Flatten展平处理，使得输入的RGB彩色图像shape从<code>[224，224，3]-&gt;[14，14，768]-&gt;[196，768]</code>）</p>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Patch-Embedding%E5%B1%82.png" alt="Patch-Embedding层"></p>
<h3 id="初始化函数">初始化函数</h3>
<ul>
<li><code>grid_size</code>：img_size除以patch_size，即224 // 16 = 14，因此grid_size为14x14，对应卷积层输出的特征矩阵宽高；</li>
<li><code>num_patches</code>：计算patches的数目，即14x14 = 196；</li>
<li><code>proj</code>：定义卷积层；</li>
<li><code>norm</code>：norm_layer默认为None，如果有传入norm_layer，则会初始化norm_layer，如果没有传入，则nn.Identity()表示不做处理。</li>
</ul>
<h3 id="正向传播函数">正向传播函数</h3>
<p>将特征矩阵传入正向传播函数，首先对输入x进行判断，如果输入特征矩阵的宽、高与预先设定的值不一样的话，程序会报错处理。</p>
<blockquote>
<p>注意：这里所讲的ViT模型，并不与之前所讲的CNN模型那样，可以更改输入图片的大小的。<strong>在ViT模型中，输入图片大小必须是固定的（因为之后的全连接层并没有再特殊处理，所以要求图片大小固定）</strong>。</p>
</blockquote>
<p>接下来将传入的数据走到卷积层得到的Tensor是[B，C，H，W]（Batch，Channel，Height，Width），之后进行flatten展平处理（<code>flatten(2)</code>：将位置在2及以后的信息进行展平，该处意为将H和W进行展平，即<code>[B，C，H，W]-&gt;[B，C，HW]</code>），再通过transpose多个位置替换函数（<code>transpose（1，2）</code>的1和2位置交换，该处意为C和HW交换，即<code>[B，C，HW]-&gt;[B，HW，C]</code>） 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = (img_size, img_size)</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="comment"># 14x14</span></span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 14x14 = 196</span></span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 相当于将一整张图片分成14个大小为16x16的patch</span></span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="comment"># 因为norm_layer=None，所以不做任何处理</span></span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># batch，3，224，224</span></span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># flatten: [B, C, H, W] -&gt; [B, C, HW] == [B，768，14，14]-&gt;[B,768,196]</span></span><br><span class="line">        <span class="comment"># transpose: [B, C, HW] -&gt; [B, HW, C] == [B,768,196]-&gt;[B,196,768]</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Attention类">Attention类</h2>
<p>用来实现Transformer当中的Muti-Head Transformer模块。</p>
<h3 id="初始化函数-2">初始化函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 dim,   <span class="comment"># 输入token的dim</span></span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">                 qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 proj_drop_ratio=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop_ratio)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop_ratio)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>dim</code>：token的dimension；</li>
<li><code>qkv_bias</code>：生成qkv时是否使用偏置；</li>
<li><code>head_dim</code>：针对每一个head的dimension，就是传入的dim // num_heads = 768//8=96；</li>
</ul>
<blockquote>
<p>通过$W^q$，$W^k$，$W^v$来生成$q$，$k$，$v$，接着根据head的数目，将$q$，$k$，$v$均分成多少份（例如下图有2个head，则将$q$，$k$，$v$均分为2部分。针对每一个部分也就是每一个head所采用的$qkv$的dimension = 最开始的dimension除以$qkv$的个数），即<code>head_dim = dim // num_heads</code>，得到了每一个head的$qkv$所对应的dimension。</p>
<p>dim即自己设定的总编码的大小（这里为768），除以heads就是表示用几个att分别得到的子编码来组成这个768</p>
</blockquote>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Muti-Head_Transformer.png" alt="Muti-Head_Transformer中的head"></p>
<ul>
<li><code>qk_scale</code>：如果传入了qk_scale，则将<code>self.scale = qk_scale</code>，否则self.scale = head_dim的开平方分之一，即$self.scale = 1/\sqrt head_dim$；</li>
</ul>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Attention%E5%85%AC%E5%BC%8F.png" alt="Attention公式"></p>
<ul>
<li><code>self.qkv</code>：$qkv$生成是直接通过一个全连接层实现的。</li>
</ul>
<blockquote>
<p>注意：在其他人源码中有些是需要通过三个全连接层来分别得到$qkv$，但是在此处，直接使用一个全连接层直接得到$qkv$。实际上没有区别，因此此处的全连接层的节点个数是dim*3，和使用3个节点个数为dim的全连接层的效果是一样的（分为3个可能是为了并行化效果更好）。</p>
</blockquote>
<ul>
<li><code>attn_drop</code>：定义一个Dropout层，失活性为传入的attn_drop_ratio；</li>
<li><code>proj</code>：再定义一个全连接层，输入输出节点个数都是dim；</li>
</ul>
<blockquote>
<p>在Muti-Head Transformer中，会将每一个得到的head进行concat拼接，之后用一个$W^o$进行映射</p>
</blockquote>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Muti-Head_Transformer%E4%B8%ADconcat%E6%8B%BC%E6%8E%A5.png" alt="Muti-Head_Transformer中concat拼接"></p>
<ul>
<li><code>proj_drop</code>：再定义一个Dropout层，失活性为传入的proj_drop_ratio。</li>
</ul>
<h3 id="正向传播函数-2">正向传播函数</h3>
<p>传入的x实际上为<code>[batch_size, num_patches + 1, total_embed_dim]</code>。<code>batch_size</code>指训练时这一批数据传入的图片的数目；<code>num_patches</code>指传入图片高宽除以Patches高宽的得数（这里即$（224 // 16）^2 = (14*14)^2 = 196$），+1是指经过Patch Embedding层之后会拼接上[class]token，即196+1 = 197；<code>total_embed_dim</code>指768。</p>
<p><code>qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)</code></p>
<blockquote>
<p>qkv()：先通过qkv全连接层生成qkv（<code>qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]</code>）；</p>
<p>reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]；</p>
<p>permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]（permute函数可批量调整顺序，这里这么改是为了方便后续做运算），第<strong>一个维度3，表示包含三个张量，分别对应Queries（Q）、Keys（K）、Values（V）</strong></p>
</blockquote>
<p><code>q, k, v = qkv[0], qkv[1], qkv[2]</code>：通过切片的方式拿到qkv的数据</p>
<blockquote>
<p>q, k, v此时的shape为[batch_size, num_heads, num_patches + 1, embed_dim_per_head]</p>
</blockquote>
<ul>
<li>transpose函数：可将其中两个位置相互交换；</li>
</ul>
<blockquote>
<p>例如：transpose(-2, -1)：将最后两个维度进行调换。transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]</p>
</blockquote>
<ul>
<li>permute函数：批量调整顺序</li>
</ul>
<p><code>attn = (q @ k.transpose(-2, -1)) * self.scale</code>。</p>
<blockquote>
<p><code>@</code>：矩阵乘法的符号</p>
<p>k的shape为[batch_size, num_heads, num_patches + 1, embed_dim_per_head]，经过transpose函数将最后两个维度调换之后，为[batch_size, num_heads, embed_dim_per_head, num_patches+ 1 ]，<strong>即实现矩阵的转置</strong>；</p>
<p>经过<strong>q与k的转置矩阵相乘</strong>之后（实际为最后两个维度相乘），即[batch_size, num_heads, num_patches + 1, embed_dim_per_head]*[batch_size, num_heads, embed_dim_per_head, num_patches+ 1 ] = [batch_size, num_heads, <strong>num_patches + 1, num_patches+ 1</strong> ]（<strong>因为矩阵中，axb的矩阵乘以bxa的矩阵，得出来的值为axa</strong>）</p>
<p>之后再乘上scale，再经过softmax处理，最终呈现的原理就是下图所示</p>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Attention%E5%85%AC%E5%BC%8F.png" alt="Attention公式"></p>
</blockquote>
<p><code>attn.softmax(dim=-1)</code>：其中的<code>dim = -1</code>指的是在矩阵的每一行进行softmax处理，如果是<code>dim = -2</code>，则是在每一列进行softmax处理。</p>
<p>之后再根据每个v的权重经过Dropout层。</p>
<p>为了实现上图的公式，还需要根据softmax之后，针对每一个V的权重来进行加权求和的操作。即<code>(attn @ v).transpose(1, 2).reshape(B, N, C)</code>中的<code>attn @ v</code>。这里经过加权求和之后，shape变为[batch_size, num_heads, num_patches + 1, embed_dim_per_head]。最后经过reshape操作，就是将最后两个维度拼接在一起。</p>
<p><code>self.proj(x)</code>：有时候需要通过一个$W^o$去映射，因此这里通过proj全连接层得到结果；</p>
<p><code>proj_drop</code>：再通过一个Dropout层得到最终输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># [batch_size, num_patches + 1, total_embed_dim]</span></span><br><span class="line">    <span class="comment"># [224,197,768]</span></span><br><span class="line">    B, N, C = x.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)</span></span><br><span class="line">    <span class="comment"># qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]</span></span><br><span class="line">    <span class="comment"># reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line">    <span class="comment"># permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">    qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># [batch_size, num_heads, num_patches + 1, embed_dim_per_head] = [batch，8，197，96]</span></span><br><span class="line">    q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># transpose（转置）: -&gt; [batch_size, num_heads, embed_dim_per_head, num_patches + 1]</span></span><br><span class="line">    <span class="comment"># @: 矩阵相乘multiply -&gt; [batch_size, num_heads, num_patches + 1, num_patches + 1]</span></span><br><span class="line">    attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">    attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">    attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># attn @ v这一步结束公式，之后的操作是还原Tensor通道排列顺序</span></span><br><span class="line">    <span class="comment"># @: multiply -&gt; [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">    <span class="comment"># transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]</span></span><br><span class="line">    <span class="comment"># reshape: -&gt; [batch_size, num_patches + 1, total_embed_dim]</span></span><br><span class="line">    x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">    x = self.proj(x)</span><br><span class="line">    x = self.proj_drop(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Mlp类">Mlp类</h2>
<p>对应为上一篇文中所讲的Encoder Block中的MLP Block，也就是下图所示的结构：</p>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/MLP_Block%E7%BB%93%E6%9E%84.png" alt="MLP Block结构"></p>
<ul>
<li><code>hidden_features</code>：对应的是第一个全连接层的节点个数，通常为in_features节点个数的4倍；</li>
<li><code>out_features</code>：和in_features节点个数一致</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Block类中没有输入out_features，所以默认out_features=None</span></span><br><span class="line">        <span class="comment"># 所以这条语句out_features = in_features</span></span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        <span class="comment"># hidden_features = hidden_features = 3072</span></span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Block类">Block类</h2>
<p>对应为上一篇文中所讲的Encoder Block，结构如下图所示：</p>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/Encoder_Block%E7%BB%93%E6%9E%84.png" alt="Encoder Block结构"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 dim,</span></span><br><span class="line"><span class="params">                 num_heads,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">                 qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># 在VisionTransoformer类中传入值为从0到drop_path_ratio的12次等差数列</span></span></span><br><span class="line"><span class="params">                 drop_path_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path = DropPath(drop_path_ratio) <span class="keyword">if</span> drop_path_ratio &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        <span class="comment"># mlp_hidden_dim = 768*4=3072</span></span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 两个残差结构</span></span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="VisionTransformer类">VisionTransformer类</h2>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/ResNet-50+ViT-B16-hybrid-model.png" alt="VisionTransformer模型结构"></p>
<h3 id="初始化函数-3">初始化函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.0</span>, qkv_bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>, representation_size=<span class="literal">None</span>, distilled=<span class="literal">False</span>, drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>, drop_path_ratio=<span class="number">0.</span>, embed_layer=PatchEmbed, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 act_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            img_size (int, tuple): input image size</span></span><br><span class="line"><span class="string">            patch_size (int, tuple): patch size</span></span><br><span class="line"><span class="string">            in_c (int): number of input channels</span></span><br><span class="line"><span class="string">            num_classes (int): number of classes for classification head</span></span><br><span class="line"><span class="string">            embed_dim (int): embedding dimension</span></span><br><span class="line"><span class="string">            depth (int): depth of transformer</span></span><br><span class="line"><span class="string">            num_heads (int): number of attention heads</span></span><br><span class="line"><span class="string">            mlp_ratio (int): ratio of mlp hidden dim to embedding dim</span></span><br><span class="line"><span class="string">            qkv_bias (bool): enable bias for qkv if True</span></span><br><span class="line"><span class="string">            qk_scale (float): override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set</span></span><br><span class="line"><span class="string">            distilled (bool): model includes a distillation token and head as in DeiT models</span></span><br><span class="line"><span class="string">            drop_ratio (float): dropout rate</span></span><br><span class="line"><span class="string">            attn_drop_ratio (float): attention dropout rate</span></span><br><span class="line"><span class="string">            drop_path_ratio (float): stochastic depth rate</span></span><br><span class="line"><span class="string">            embed_layer (nn.Module): patch embedding layer</span></span><br><span class="line"><span class="string">            norm_layer: (nn.Module): normalization layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_features = self.embed_dim = embed_dim  <span class="comment"># num_features for consistency with other models</span></span><br><span class="line">        self.num_tokens = <span class="number">2</span> <span class="keyword">if</span> distilled <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        norm_layer = norm_layer <span class="keyword">or</span> partial(nn.LayerNorm, eps=<span class="number">1e-6</span>)</span><br><span class="line">        act_layer = act_layer <span class="keyword">or</span> nn.GELU</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embed_layer=PatchEmbed</span></span><br><span class="line">        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)</span><br><span class="line">        <span class="comment"># num_patches = 196</span></span><br><span class="line">        num_patches = self.patch_embed.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.dist_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim)) <span class="keyword">if</span> distilled <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># num_patches + self.num_tokens = 196+1=197</span></span><br><span class="line">        self.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + self.num_tokens, embed_dim))</span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_ratio)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dpr是一个列表，按次序递增的drop_path_ratio等差数列。</span></span><br><span class="line">        <span class="comment"># x.item()就是指for x in torch.linspace(0, drop_path_ratio, depth)中的x，只不过item()更精确</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_ratio, depth)]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line">        <span class="comment"># 进入Encoder Block（depth = 12，所以重复12次）</span></span><br><span class="line">        self.blocks = nn.Sequential(*[</span><br><span class="line">            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],</span><br><span class="line">                  norm_layer=norm_layer, act_layer=act_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)</span><br><span class="line">        ])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Representation layer</span></span><br><span class="line">        <span class="comment"># representation_size=None, distilled=False</span></span><br><span class="line">        <span class="keyword">if</span> representation_size <span class="keyword">and</span> <span class="keyword">not</span> distilled:</span><br><span class="line">            self.has_logits = <span class="literal">True</span></span><br><span class="line">            self.num_features = representation_size</span><br><span class="line">            self.pre_logits = nn.Sequential(OrderedDict([</span><br><span class="line">                (<span class="string">&quot;fc&quot;</span>, nn.Linear(embed_dim, representation_size)),</span><br><span class="line">                (<span class="string">&quot;act&quot;</span>, nn.Tanh())</span><br><span class="line">            ]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.has_logits = <span class="literal">False</span></span><br><span class="line">            self.pre_logits = nn.Identity()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Classifier head(s)</span></span><br><span class="line">        self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.head_dist = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> distilled:</span><br><span class="line">            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Weight init</span></span><br><span class="line">        nn.init.trunc_normal_(self.pos_embed, std=<span class="number">0.02</span>)</span><br><span class="line">        <span class="keyword">if</span> self.dist_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.trunc_normal_(self.dist_token, std=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">        nn.init.trunc_normal_(self.cls_token, std=<span class="number">0.02</span>)</span><br><span class="line">        self.apply(_init_vit_weights)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>depth</code>：在Transformer Encoder中重复Encoder Block多少次，这里为12次；</li>
<li><code>representation_size</code>：对应MLP Head中的Pre-Logits当中全连接层的节点个数，如果为None，则不会构建MLP Head中的Pre-Logits，即在MLP Head中只有一个全连接层；</li>
<li><code>embed_layer</code>：指Patch Embedding层。</li>
</ul>
<p><code>norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)</code>：默认为norm_layer ，使用partial函数将nn.LayerNorm传入的eps默认参数改为1e-6</p>
<blockquote>
<p>nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)</p>
<p>LayerNorm也是归一化的一种方法，与BatchNorm不同的是它是对每单个batch进行的归一化，而batchnorm是对所有batch一起进行归一化的</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">norm_layer = norm_layer <span class="keyword">or</span> partial(nn.LayerNorm, eps=<span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure>
<p><code>self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))</code>：通过nn.Parameter构建了一个可训练的参数，直接使用一个零矩阵进行初始化，shape大小为[1，1，embed_dim]（batch维度，[class]token中1x768的1，768）；</p>
<p><code>self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))</code>：结构图中，Position Embedding的shape是和拼接之后的shape一样，都是[197，768]。同样根据nn.Parameter构建了一个可训练的参数，直接使用一个零矩阵进行初始化，第一个1是batch维度（可以不用管），<code>num_patches + self.num_tokens</code>=<code>14x14+1</code> = <code>196+1=197</code>，embed_dim即传入值；</p>
<p><code>self.pos_drop = nn.Dropout(p=drop_ratio)</code>：此处的Dropout层指的是Transformer Encoder之前的Droupout层；</p>
<p><code>dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]</code>：根据传入的drop_path_ratio构建一个等差序列，范围是从0到drop_path_ratio，这个序列当中总共由depth个元素。也就是说，在Transformer Encoder当中，每一个Encoder Block所采用的drop_path方法所使用的drop_path_ratio是递增的。此刻默认为0；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.blocks = nn.Sequential(*[</span><br><span class="line">        Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, 				   		qk_scale=qk_scale,drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, 							drop_path_ratio=dpr[i],norm_layer=norm_layer, act_layer=act_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)</span><br><span class="line">        ])</span><br></pre></td></tr></table></figure>
<p>构建Block，即Transformer Encoder堆叠12次。通过循环depth达到重复blocks的效果，每循环一次，都会在列表当中添加一个Block，也就是Encoder Block（）其中传入参数都是不变的，除非drop_path_ratio=dpr[i]，是递增的。再通过nn.Sequential将列表中的所有模块打包成一个整体，赋值给self.blocks；</p>
<p><code>representation_size</code>：对应MLP Head中的Pre-Logits当中全连接层的节点个数，如果为None，则不会构建MLP Head中的Pre-Logits。</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;self.pre_logits = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&quot;fc&quot;</span>, nn.Linear(embed_dim, representation_size)),</span><br><span class="line">          (<span class="string">&quot;act&quot;</span>, nn.Tanh())</span><br><span class="line">      ]))</span><br></pre></td></tr></table></figure>
<p>通过nn.Sequential方法再加上OrderedDict有序字典来构造pre_logits，即一个全连接层+Tanh激活函数</p>
</blockquote>
<p><code>self.head</code>：最后的全连接层；</p>
<h3 id="正向传播函数-3">正向传播函数</h3>
<p><strong>forward_features函数</strong></p>
<ul>
<li>首先将输入x传递给patch_embed，即对应着Patch Embedding结构；</li>
<li>接下来将cls_token进行expand处理，在之前构建的cls_token的shape为[1，1，768]，那么这条语句会根据传入的batch_size的个数去expand这的cls_token，也就是说将cls_token在batch维度复制batch_size份，即shape变成了[B，1，768]；</li>
<li>self.dist_token在这默认为None，即对cls_token与x在1维度进行拼接，即196的维度，[B，197，768]；</li>
<li>将拼接之后得出的数据加上self.pos_embed，再通过一个pos.drop，即dropout层；</li>
<li><code>self.pre_logits(x[:, 0])</code>：通过该条语句获取输出，将x参数的第二个维度（除开第一个维度的batch）上的索引为0的数据</li>
</ul>
<p><strong>forward函数</strong></p>
<p>直接forward_features函数处理，最后的<code>x = self.head(x)</code>，即结构图中最后一个全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># [B, C, H, W] -&gt; [B, num_patches, embed_dim]</span></span><br><span class="line">    x = self.patch_embed(x)  <span class="comment"># [B, 196, 768]</span></span><br><span class="line">    <span class="comment"># [1, 1, 768] -&gt; [B, 1, 768]</span></span><br><span class="line">    cls_token = self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> self.dist_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)  <span class="comment"># [B, 197, 768]</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = torch.cat((cls_token, self.dist_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    x = self.pos_drop(x + self.pos_embed)</span><br><span class="line">    x = self.blocks(x)</span><br><span class="line">    x = self.norm(x)</span><br><span class="line">    <span class="keyword">if</span> self.dist_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> self.pre_logits(x[:, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>], x[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.forward_features(x)</span><br><span class="line">    <span class="keyword">if</span> self.head_dist <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        x, x_dist = self.head(x[<span class="number">0</span>]), self.head_dist(x[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">            <span class="comment"># during inference, return the average of both classifier predictions</span></span><br><span class="line">            <span class="keyword">return</span> x, x_dist</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (x + x_dist) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = self.head(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="实例化模型">实例化模型</h2>
<ul>
<li><code>Layers</code>是<strong>Transformer Encoder中重复堆叠Encoder Block的次数</strong>；</li>
<li><code>Hidden Size</code>是<strong>通过Embedding层后每个token的dim</strong> (向量的长度)；</li>
<li><code>MLP size</code>是Transformer Encoder中<strong>MLP Block第一个全连接的节点个数(是Hidden Size的四倍)</strong>；</li>
<li><code>Heads</code>代表Transformer中<strong>Multi-Head Attention的heads数</strong></li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">Patch Size</th>
<th style="text-align:center">Layers</th>
<th style="text-align:center">Hidden Size D</th>
<th style="text-align:center">MLP size</th>
<th style="text-align:center">Heads</th>
<th style="text-align:center">Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ViT-Base</td>
<td style="text-align:center">16x16</td>
<td style="text-align:center">12</td>
<td style="text-align:center">768</td>
<td style="text-align:center">3072</td>
<td style="text-align:center">12</td>
<td style="text-align:center">86M</td>
</tr>
<tr>
<td style="text-align:center">ViT-Large</td>
<td style="text-align:center">16x16</td>
<td style="text-align:center">24</td>
<td style="text-align:center">1024</td>
<td style="text-align:center">4096</td>
<td style="text-align:center">16</td>
<td style="text-align:center">307M</td>
</tr>
<tr>
<td style="text-align:center">ViT-Huge</td>
<td style="text-align:center">14x14</td>
<td style="text-align:center">32</td>
<td style="text-align:center">1280</td>
<td style="text-align:center">5120</td>
<td style="text-align:center">16</td>
<td style="text-align:center">632M</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch16_224</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    链接: https://pan.baidu.com/s/1zqb08naP0RPqqfSXfkB2EA  密码: eu9f</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">16</span>,</span><br><span class="line">                              embed_dim=<span class="number">768</span>,</span><br><span class="line">                              depth=<span class="number">12</span>,</span><br><span class="line">                              num_heads=<span class="number">12</span>,</span><br><span class="line">                              representation_size=<span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch16_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">16</span>,</span><br><span class="line">                              embed_dim=<span class="number">768</span>,</span><br><span class="line">                              depth=<span class="number">12</span>,</span><br><span class="line">                              num_heads=<span class="number">12</span>,</span><br><span class="line">                              representation_size=<span class="number">768</span> <span class="keyword">if</span> has_logits <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch32_224</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    链接: https://pan.baidu.com/s/1hCv0U8pQomwAtHBYc4hmZg  密码: s5hl</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">32</span>,</span><br><span class="line">                              embed_dim=<span class="number">768</span>,</span><br><span class="line">                              depth=<span class="number">12</span>,</span><br><span class="line">                              num_heads=<span class="number">12</span>,</span><br><span class="line">                              representation_size=<span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_base_patch32_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch32_224_in21k-8db57226.pth</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">32</span>,</span><br><span class="line">                              embed_dim=<span class="number">768</span>,</span><br><span class="line">                              depth=<span class="number">12</span>,</span><br><span class="line">                              num_heads=<span class="number">12</span>,</span><br><span class="line">                              representation_size=<span class="number">768</span> <span class="keyword">if</span> has_logits <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_large_patch16_224</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    链接: https://pan.baidu.com/s/1cxBgZJJ6qUWPSBNcE4TdRQ  密码: qqt8</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">16</span>,</span><br><span class="line">                              embed_dim=<span class="number">1024</span>,</span><br><span class="line">                              depth=<span class="number">24</span>,</span><br><span class="line">                              num_heads=<span class="number">16</span>,</span><br><span class="line">                              representation_size=<span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_large_patch16_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch16_224_in21k-606da67d.pth</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">16</span>,</span><br><span class="line">                              embed_dim=<span class="number">1024</span>,</span><br><span class="line">                              depth=<span class="number">24</span>,</span><br><span class="line">                              num_heads=<span class="number">16</span>,</span><br><span class="line">                              representation_size=<span class="number">1024</span> <span class="keyword">if</span> has_logits <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_large_patch32_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    weights ported from official Google JAX impl:</span></span><br><span class="line"><span class="string">    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">32</span>,</span><br><span class="line">                              embed_dim=<span class="number">1024</span>,</span><br><span class="line">                              depth=<span class="number">24</span>,</span><br><span class="line">                              num_heads=<span class="number">16</span>,</span><br><span class="line">                              representation_size=<span class="number">1024</span> <span class="keyword">if</span> has_logits <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit_huge_patch14_224_in21k</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">21843</span>, has_logits: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).</span></span><br><span class="line"><span class="string">    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.</span></span><br><span class="line"><span class="string">    NOTE: converted weights not currently available, too large for github release hosting.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = VisionTransformer(img_size=<span class="number">224</span>,</span><br><span class="line">                              patch_size=<span class="number">14</span>,</span><br><span class="line">                              embed_dim=<span class="number">1280</span>,</span><br><span class="line">                              depth=<span class="number">32</span>,</span><br><span class="line">                              num_heads=<span class="number">16</span>,</span><br><span class="line">                              representation_size=<span class="number">1280</span> <span class="keyword">if</span> has_logits <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                              num_classes=num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1><a target="_blank" rel="noopener" href="http://train.py">train.py</a></h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.optim.lr_scheduler <span class="keyword">as</span> lr_scheduler</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> my_dataset <span class="keyword">import</span> MyDataSet</span><br><span class="line"><span class="keyword">from</span> vit_model <span class="keyword">import</span> vit_base_patch16_224_in21k <span class="keyword">as</span> create_model</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> read_split_data, train_one_epoch, evaluate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    device = torch.device(args.device <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(<span class="string">&quot;./weights&quot;</span>) <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">        os.makedirs(<span class="string">&quot;./weights&quot;</span>)</span><br><span class="line"></span><br><span class="line">    tb_writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">    train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args.data_path)</span><br><span class="line"></span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                   transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])])&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化训练数据集</span></span><br><span class="line">    train_dataset = MyDataSet(images_path=train_images_path,</span><br><span class="line">                              images_class=train_images_label,</span><br><span class="line">                              transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化验证数据集</span></span><br><span class="line">    val_dataset = MyDataSet(images_path=val_images_path,</span><br><span class="line">                            images_class=val_images_label,</span><br><span class="line">                            transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line"></span><br><span class="line">    batch_size = args.batch_size</span><br><span class="line">    nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                               batch_size=batch_size,</span><br><span class="line">                                               shuffle=<span class="literal">True</span>,</span><br><span class="line">                                               pin_memory=<span class="literal">True</span>,</span><br><span class="line">                                               num_workers=nw,</span><br><span class="line">                                               collate_fn=train_dataset.collate_fn)</span><br><span class="line"></span><br><span class="line">    val_loader = torch.utils.data.DataLoader(val_dataset,</span><br><span class="line">                                             batch_size=batch_size,</span><br><span class="line">                                             shuffle=<span class="literal">False</span>,</span><br><span class="line">                                             pin_memory=<span class="literal">True</span>,</span><br><span class="line">                                             num_workers=nw,</span><br><span class="line">                                             collate_fn=val_dataset.collate_fn)</span><br><span class="line"></span><br><span class="line">    model = create_model(num_classes=args.num_classes, has_logits=<span class="literal">False</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.weights != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="keyword">assert</span> os.path.exists(args.weights), <span class="string">&quot;weights file: &#x27;&#123;&#125;&#x27; not exist.&quot;</span>.<span class="built_in">format</span>(args.weights)</span><br><span class="line">        weights_dict = torch.load(args.weights, map_location=device)</span><br><span class="line">        <span class="comment"># 删除不需要的权重</span></span><br><span class="line">        del_keys = [<span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>] <span class="keyword">if</span> model.has_logits \</span><br><span class="line">            <span class="keyword">else</span> [<span class="string">&#x27;pre_logits.fc.weight&#x27;</span>, <span class="string">&#x27;pre_logits.fc.bias&#x27;</span>, <span class="string">&#x27;head.weight&#x27;</span>, <span class="string">&#x27;head.bias&#x27;</span>]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> del_keys:</span><br><span class="line">            <span class="keyword">del</span> weights_dict[k]</span><br><span class="line">        <span class="built_in">print</span>(model.load_state_dict(weights_dict, strict=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.freeze_layers:</span><br><span class="line">        <span class="keyword">for</span> name, para <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="comment"># 除head, pre_logits外，其他权重全部冻结</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;head&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name <span class="keyword">and</span> <span class="string">&quot;pre_logits&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">                para.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;training &#123;&#125;&quot;</span>.<span class="built_in">format</span>(name))</span><br><span class="line"></span><br><span class="line">    pg = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    optimizer = optim.SGD(pg, lr=args.lr, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5E-5</span>)</span><br><span class="line">    <span class="comment"># Scheduler https://arxiv.org/pdf/1812.01187.pdf</span></span><br><span class="line">    lf = <span class="keyword">lambda</span> x: ((<span class="number">1</span> + math.cos(x * math.pi / args.epochs)) / <span class="number">2</span>) * (<span class="number">1</span> - args.lrf) + args.lrf  <span class="comment"># cosine</span></span><br><span class="line">    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        <span class="comment"># train</span></span><br><span class="line">        train_loss, train_acc = train_one_epoch(model=model,</span><br><span class="line">                                                optimizer=optimizer,</span><br><span class="line">                                                data_loader=train_loader,</span><br><span class="line">                                                device=device,</span><br><span class="line">                                                epoch=epoch)</span><br><span class="line"></span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        val_loss, val_acc = evaluate(model=model,</span><br><span class="line">                                     data_loader=val_loader,</span><br><span class="line">                                     device=device,</span><br><span class="line">                                     epoch=epoch)</span><br><span class="line"></span><br><span class="line">        tags = [<span class="string">&quot;train_loss&quot;</span>, <span class="string">&quot;train_acc&quot;</span>, <span class="string">&quot;val_loss&quot;</span>, <span class="string">&quot;val_acc&quot;</span>, <span class="string">&quot;learning_rate&quot;</span>]</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">0</span>], train_loss, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">1</span>], train_acc, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">2</span>], val_loss, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">3</span>], val_acc, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">4</span>], optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>], epoch)</span><br><span class="line"></span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;./weights/model-&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_classes&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.001</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lrf&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集所在根目录</span></span><br><span class="line">    <span class="comment"># https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--data-path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        default=<span class="string">&quot;D:/python_test/deep-learning-for-image-processing/data_set/flower_data/flower_photos&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--model-name&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;create model name&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预训练权重路径，如果不想载入就设置为空字符</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weights&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;./vit_base_patch16_224_in21k.pth&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;initial weights path&#x27;</span>)</span><br><span class="line">    <span class="comment"># 是否冻结权重</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--freeze-layers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">True</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--device&#x27;</span>, default=<span class="string">&#x27;cuda:0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;device id (i.e. 0 or 0,1 or cpu)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(opt)</span><br></pre></td></tr></table></figure>
<p><strong>训练结果</strong></p>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="训练结果"></p>
<h1><a target="_blank" rel="noopener" href="http://predict.py">predict.py</a></h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> vit_model <span class="keyword">import</span> vit_base_patch16_224_in21k <span class="keyword">as</span> create_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(<span class="number">256</span>),</span><br><span class="line">         transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    img_path = <span class="string">&quot;tulip.jpg&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    img = data_transform(img)</span><br><span class="line">    <span class="comment"># expand batch dimension</span></span><br><span class="line">    img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    model = create_model(num_classes=<span class="number">5</span>, has_logits=<span class="literal">False</span>).to(device)</span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./weights/model-9.pth&quot;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_weight_path, map_location=device))</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># predict class</span></span><br><span class="line">        output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">        predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">        predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">    print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                                 predict[predict_cla].numpy())</span><br><span class="line">    plt.title(print_res)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                                  predict[i].numpy()))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p><strong>预测结果</strong></p>
<p><img src="/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="预测结果"></p>
<h1><a target="_blank" rel="noopener" href="http://flops.py">flops.py</a></h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> fvcore.nn <span class="keyword">import</span> FlopCountAnalysis</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> vit_model <span class="keyword">import</span> Attention</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># Self-Attention</span></span><br><span class="line">    a1 = Attention(dim=<span class="number">512</span>, num_heads=<span class="number">1</span>)</span><br><span class="line">    a1.proj = torch.nn.Identity()  <span class="comment"># remove Wo</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multi-Head Attention</span></span><br><span class="line">    a2 = Attention(dim=<span class="number">512</span>, num_heads=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size, num_tokens, total_embed_dim]</span></span><br><span class="line">    t = (torch.rand(<span class="number">32</span>, <span class="number">1024</span>, <span class="number">512</span>),)</span><br><span class="line"></span><br><span class="line">    flops1 = FlopCountAnalysis(a1, t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Self-Attention FLOPs:&quot;</span>, flops1.total())</span><br><span class="line"></span><br><span class="line">    flops2 = FlopCountAnalysis(a2, t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Multi-Head Attention FLOPs:&quot;</span>, flops2.total())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1><a target="_blank" rel="noopener" href="http://utils.py">utils.py</a></h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_split_data</span>(<span class="params">root: <span class="built_in">str</span>, val_rate: <span class="built_in">float</span> = <span class="number">0.2</span></span>):</span><br><span class="line">    random.seed(<span class="number">0</span>)  <span class="comment"># 保证随机结果可复现</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(root), <span class="string">&quot;dataset root: &#123;&#125; does not exist.&quot;</span>.<span class="built_in">format</span>(root)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历文件夹，一个文件夹对应一个类别</span></span><br><span class="line">    flower_class = [cla <span class="keyword">for</span> cla <span class="keyword">in</span> os.listdir(root) <span class="keyword">if</span> os.path.isdir(os.path.join(root, cla))]</span><br><span class="line">    <span class="comment"># 排序，保证各平台顺序一致</span></span><br><span class="line">    flower_class.sort()</span><br><span class="line">    <span class="comment"># 生成类别名称以及对应的数字索引</span></span><br><span class="line">    class_indices = <span class="built_in">dict</span>((k, v) <span class="keyword">for</span> v, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(flower_class))</span><br><span class="line">    json_str = json.dumps(<span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> class_indices.items()), indent=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">    train_images_path = []  <span class="comment"># 存储训练集的所有图片路径</span></span><br><span class="line">    train_images_label = []  <span class="comment"># 存储训练集图片对应索引信息</span></span><br><span class="line">    val_images_path = []  <span class="comment"># 存储验证集的所有图片路径</span></span><br><span class="line">    val_images_label = []  <span class="comment"># 存储验证集图片对应索引信息</span></span><br><span class="line">    every_class_num = []  <span class="comment"># 存储每个类别的样本总数</span></span><br><span class="line">    supported = [<span class="string">&quot;.jpg&quot;</span>, <span class="string">&quot;.JPG&quot;</span>, <span class="string">&quot;.png&quot;</span>, <span class="string">&quot;.PNG&quot;</span>]  <span class="comment"># 支持的文件后缀类型</span></span><br><span class="line">    <span class="comment"># 遍历每个文件夹下的文件</span></span><br><span class="line">    <span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">        cla_path = os.path.join(root, cla)</span><br><span class="line">        <span class="comment"># 遍历获取supported支持的所有文件路径</span></span><br><span class="line">        images = [os.path.join(root, cla, i) <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(cla_path)</span><br><span class="line">                  <span class="keyword">if</span> os.path.splitext(i)[-<span class="number">1</span>] <span class="keyword">in</span> supported]</span><br><span class="line">        <span class="comment"># 排序，保证各平台顺序一致</span></span><br><span class="line">        images.sort()</span><br><span class="line">        <span class="comment"># 获取该类别对应的索引</span></span><br><span class="line">        image_class = class_indices[cla]</span><br><span class="line">        <span class="comment"># 记录该类别的样本数量</span></span><br><span class="line">        every_class_num.append(<span class="built_in">len</span>(images))</span><br><span class="line">        <span class="comment"># 按比例随机采样验证样本</span></span><br><span class="line">        val_path = random.sample(images, k=<span class="built_in">int</span>(<span class="built_in">len</span>(images) * val_rate))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> img_path <span class="keyword">in</span> images:</span><br><span class="line">            <span class="keyword">if</span> img_path <span class="keyword">in</span> val_path:  <span class="comment"># 如果该路径在采样的验证集样本中则存入验证集</span></span><br><span class="line">                val_images_path.append(img_path)</span><br><span class="line">                val_images_label.append(image_class)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则存入训练集</span></span><br><span class="line">                train_images_path.append(img_path)</span><br><span class="line">                train_images_label.append(image_class)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; images were found in the dataset.&quot;</span>.<span class="built_in">format</span>(<span class="built_in">sum</span>(every_class_num)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; images for training.&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_images_path)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(val_images_path)))</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(train_images_path) &gt; <span class="number">0</span>, <span class="string">&quot;number of training images must greater than 0.&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(val_images_path) &gt; <span class="number">0</span>, <span class="string">&quot;number of validation images must greater than 0.&quot;</span></span><br><span class="line"></span><br><span class="line">    plot_image = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> plot_image:</span><br><span class="line">        <span class="comment"># 绘制每种类别个数柱状图</span></span><br><span class="line">        plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(flower_class)), every_class_num, align=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        <span class="comment"># 将横坐标0,1,2,3,4替换为相应的类别名称</span></span><br><span class="line">        plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(flower_class)), flower_class)</span><br><span class="line">        <span class="comment"># 在柱状图上添加数值标签</span></span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(every_class_num):</span><br><span class="line">            plt.text(x=i, y=v + <span class="number">5</span>, s=<span class="built_in">str</span>(v), ha=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        <span class="comment"># 设置x坐标</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;image class&#x27;</span>)</span><br><span class="line">        <span class="comment"># 设置y坐标</span></span><br><span class="line">        plt.ylabel(<span class="string">&#x27;number of images&#x27;</span>)</span><br><span class="line">        <span class="comment"># 设置柱状图的标题</span></span><br><span class="line">        plt.title(<span class="string">&#x27;flower class distribution&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_images_path, train_images_label, val_images_path, val_images_label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_data_loader_image</span>(<span class="params">data_loader</span>):</span><br><span class="line">    batch_size = data_loader.batch_size</span><br><span class="line">    plot_num = <span class="built_in">min</span>(batch_size, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), json_path + <span class="string">&quot; does not exist.&quot;</span></span><br><span class="line">    json_file = <span class="built_in">open</span>(json_path, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    class_indices = json.load(json_file)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> data_loader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(plot_num):</span><br><span class="line">            <span class="comment"># [C, H, W] -&gt; [H, W, C]</span></span><br><span class="line">            img = images[i].numpy().transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 反Normalize操作</span></span><br><span class="line">            img = (img * [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>] + [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]) * <span class="number">255</span></span><br><span class="line">            label = labels[i].item()</span><br><span class="line">            plt.subplot(<span class="number">1</span>, plot_num, i+<span class="number">1</span>)</span><br><span class="line">            plt.xlabel(class_indices[<span class="built_in">str</span>(label)])</span><br><span class="line">            plt.xticks([])  <span class="comment"># 去掉x轴的刻度</span></span><br><span class="line">            plt.yticks([])  <span class="comment"># 去掉y轴的刻度</span></span><br><span class="line">            plt.imshow(img.astype(<span class="string">&#x27;uint8&#x27;</span>))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_pickle</span>(<span class="params">list_info: <span class="built_in">list</span>, file_name: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(list_info, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_pickle</span>(<span class="params">file_name: <span class="built_in">str</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        info_list = pickle.load(f)</span><br><span class="line">        <span class="keyword">return</span> info_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model, optimizer, data_loader, device, epoch</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    loss_function = torch.nn.CrossEntropyLoss()</span><br><span class="line">    accu_loss = torch.zeros(<span class="number">1</span>).to(device)  <span class="comment"># 累计损失</span></span><br><span class="line">    accu_num = torch.zeros(<span class="number">1</span>).to(device)   <span class="comment"># 累计预测正确的样本数</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    sample_num = <span class="number">0</span></span><br><span class="line">    data_loader = tqdm(data_loader, file=sys.stdout)</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        images, labels = data</span><br><span class="line">        sample_num += images.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        pred = model(images.to(device))</span><br><span class="line">        pred_classes = torch.<span class="built_in">max</span>(pred, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        accu_num += torch.eq(pred_classes, labels.to(device)).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        loss = loss_function(pred, labels.to(device))</span><br><span class="line">        loss.backward()</span><br><span class="line">        accu_loss += loss.detach()</span><br><span class="line"></span><br><span class="line">        data_loader.desc = <span class="string">&quot;[train epoch &#123;&#125;] loss: &#123;:.3f&#125;, acc: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,</span><br><span class="line">                                                                               accu_loss.item() / (step + <span class="number">1</span>),</span><br><span class="line">                                                                               accu_num.item() / sample_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> torch.isfinite(loss):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;WARNING: non-finite loss, ending training &#x27;</span>, loss)</span><br><span class="line">            sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> accu_loss.item() / (step + <span class="number">1</span>), accu_num.item() / sample_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, data_loader, device, epoch</span>):</span><br><span class="line">    loss_function = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    accu_num = torch.zeros(<span class="number">1</span>).to(device)   <span class="comment"># 累计预测正确的样本数</span></span><br><span class="line">    accu_loss = torch.zeros(<span class="number">1</span>).to(device)  <span class="comment"># 累计损失</span></span><br><span class="line"></span><br><span class="line">    sample_num = <span class="number">0</span></span><br><span class="line">    data_loader = tqdm(data_loader, file=sys.stdout)</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        images, labels = data</span><br><span class="line">        sample_num += images.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        pred = model(images.to(device))</span><br><span class="line">        pred_classes = torch.<span class="built_in">max</span>(pred, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        accu_num += torch.eq(pred_classes, labels.to(device)).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        loss = loss_function(pred, labels.to(device))</span><br><span class="line">        accu_loss += loss</span><br><span class="line"></span><br><span class="line">        data_loader.desc = <span class="string">&quot;[valid epoch &#123;&#125;] loss: &#123;:.3f&#125;, acc: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,</span><br><span class="line">                                                                               accu_loss.item() / (step + <span class="number">1</span>),</span><br><span class="line">                                                                               accu_num.item() / sample_num)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> accu_loss.item() / (step + <span class="number">1</span>), accu_num.item() / sample_num</span><br></pre></td></tr></table></figure>
<h1>my_dataset.py</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义数据集&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, images_path: <span class="built_in">list</span>, images_class: <span class="built_in">list</span>, transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.images_path = images_path</span><br><span class="line">        self.images_class = images_class</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.images_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        img = Image.<span class="built_in">open</span>(self.images_path[item])</span><br><span class="line">        <span class="comment"># RGB为彩色图片，L为灰度图片</span></span><br><span class="line">        <span class="keyword">if</span> img.mode != <span class="string">&#x27;RGB&#x27;</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;image: &#123;&#125; isn&#x27;t RGB mode.&quot;</span>.<span class="built_in">format</span>(self.images_path[item]))</span><br><span class="line">        label = self.images_class[item]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        <span class="comment"># 官方实现的default_collate可以参考</span></span><br><span class="line">        <span class="comment"># https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py</span></span><br><span class="line">        images, labels = <span class="built_in">tuple</span>(<span class="built_in">zip</span>(*batch))</span><br><span class="line"></span><br><span class="line">        images = torch.stack(images, dim=<span class="number">0</span>)</span><br><span class="line">        labels = torch.as_tensor(labels)</span><br><span class="line">        <span class="keyword">return</span> images, labels</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
              <a href="/tags/Pytorch%E6%90%AD%E5%BB%BACNN/" rel="tag"><i class="fa fa-tag"></i> Pytorch搭建CNN</a>
              <a href="/tags/Vision-Transformer/" rel="tag"><i class="fa fa-tag"></i> Vision Transformer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/05/31/Transformer-vit%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" rel="prev" title="深度学习模型之CNN（二十一）Vision Transformer(vit)网络详解">
                  <i class="fa fa-chevron-left"></i> 深度学习模型之CNN（二十一）Vision Transformer(vit)网络详解
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/03/Swin-Transformer%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3/" rel="next" title="深度学习模型之CNN（二十三）Swin Transformer网络结构详解">
                  深度学习模型之CNN（二十三）Swin Transformer网络结构详解 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="valine-comments"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Linvil Yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("04/21/2023 22:22:22");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "已运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","app_key":"iNfyeaWVmA11Duj7fzr0B1r1","server_url":"https://16p3s6fl.lc-cn-n1-shared.com","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




<script class="next-config" data-name="valine" type="application/json">{"enable":true,"appId":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","appKey":"iNfyeaWVmA11Duj7fzr0B1r1","serverURLs":"https://16p3s6fl.lc-cn-n1-shared.com","placeholder":"请写下您的评论","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"lang":null,"visitor":false,"comment_count":true,"recordIP":true,"enableQQ":true,"requiredFields":[],"el":"#valine-comments","path":"/2023/06/01/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVisionTransformer-vit%E6%A8%A1%E5%9E%8B/"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.valine.el)
    .then(() => NexT.utils.getScript(
      'https://fastly.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
      { condition: window.Valine }
    ))
    .then(() => {
      new Valine(CONFIG.valine);
    });
});
</script>
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '32px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

</body>
</html>
