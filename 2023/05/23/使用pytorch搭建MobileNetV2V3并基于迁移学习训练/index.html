<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logosc/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logosc/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"linvilyao.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":14,"offset":10},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.2》，作为随堂笔记。使用pytorch搭建mobilenetv2和mobilenetv3网络，并基于迁移学习方式训练">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练">
<meta property="og:url" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/index.html">
<meta property="og:site_name" content="Linvil&#39;s Blog">
<meta property="og:description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.2》，作为随堂笔记。使用pytorch搭建mobilenetv2和mobilenetv3网络，并基于迁移学习方式训练">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenetv2%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Inverted%20residual%E5%8F%82%E6%95%B0%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenet%20v3%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/SE%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9D%97.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv2%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv3%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv2%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv3%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png">
<meta property="article:published_time" content="2023-05-23T11:06:13.000Z">
<meta property="article:modified_time" content="2023-06-14T04:07:06.280Z">
<meta property="article:author" content="Linvil Yao">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="Pytorch搭建CNN">
<meta property="article:tag" content="MobileNetV2V3">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenetv2%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0.png">


<link rel="canonical" href="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/","path":"2023/05/23/使用pytorch搭建MobileNetV2V3并基于迁移学习训练/","title":"深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练 | Linvil's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linvil's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">这是一个用来记录的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">23</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">40</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">MobileNet v2v3网络搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#model-v2-py"><span class="nav-number">1.1.</span> <span class="nav-text">model_v2.py</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.1.</span> <span class="nav-text">网络结构参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ConvBNReLU%E7%B1%BB"><span class="nav-number">1.1.2.</span> <span class="nav-text">ConvBNReLU类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inverted-residual%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.3.</span> <span class="nav-text">Inverted residual结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#make-divisible%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">_make_divisible函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89MNv2%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.5.</span> <span class="nav-text">定义MNv2网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-v3-py"><span class="nav-number">1.2.</span> <span class="nav-text">model_v3.py</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#make-divisible%E5%87%BD%E6%95%B0-2"><span class="nav-number">1.2.1.</span> <span class="nav-text">_make_divisible函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ConvBNActivation%E7%B1%BB"><span class="nav-number">1.2.2.</span> <span class="nav-text">ConvBNActivation类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SqueezeExcitation%E6%A8%A1%E5%9D%97"><span class="nav-number">1.2.3.</span> <span class="nav-text">SqueezeExcitation模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#InvertedResidualConfig%E7%B1%BB"><span class="nav-number">1.2.4.</span> <span class="nav-text">InvertedResidualConfig类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#InvertedResidual%E7%B1%BB"><span class="nav-number">1.2.5.</span> <span class="nav-text">InvertedResidual类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89MNv3%E7%B1%BB"><span class="nav-number">1.2.6.</span> <span class="nav-text">定义MNv3类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89mobilenet-v3-large"><span class="nav-number">1.2.7.</span> <span class="nav-text">定义mobilenet_v3_large</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89mobilenet-v3-small"><span class="nav-number">1.2.8.</span> <span class="nav-text">定义mobilenet_v3_small</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#train-py"><span class="nav-number">1.3.</span> <span class="nav-text">train.py</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MNv2%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="nav-number">1.3.1.</span> <span class="nav-text">MNv2训练结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MNv3-Large%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="nav-number">1.3.2.</span> <span class="nav-text">MNv3-Large训练结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#predict-py"><span class="nav-number">1.4.</span> <span class="nav-text">predict.py</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MNv2%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">1.4.1.</span> <span class="nav-text">MNv2预测结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MNv3-Large%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">1.4.2.</span> <span class="nav-text">MNv3-Large预测结果</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Linvil Yao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Linvil Yao</p>
  <div class="site-description" itemprop="description">Welcome to Linvil's Blog!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


        </div>
      </div>


  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/15/%E8%A1%A5%E5%85%85%E4%B9%8B%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0%E8%A7%A3%E9%87%8A/" title="2023&#x2F;06&#x2F;15&#x2F;补充之一些函数解释&#x2F;">补充之一些函数解释（更新ing）</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/14/%E8%A1%A5%E5%85%85%E4%B9%8BTensor%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2/" title="2023&#x2F;06&#x2F;14&#x2F;补充之Tensor的维度变换&#x2F;">Tensor的维度变换</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/13/Transformer%E4%B8%ADSelf-Attention%E4%BB%A5%E5%8F%8AMulti-Head-Attention%E8%AF%A6%E8%A7%A3/" title="2023&#x2F;06&#x2F;13&#x2F;Transformer中Self-Attention以及Multi-Head-Attention详解&#x2F;">Transformer中Self-Attention以及Multi-Head Attention详解</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/10/MobileViT%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E9%80%9A%E8%BF%87Pytorch%E6%90%AD%E5%BB%BAMobileViT%E7%BD%91%E7%BB%9C/" title="2023&#x2F;06&#x2F;10&#x2F;MobileViT网络讲解及通过Pytorch搭建MobileViT网络&#x2F;">深度学习模型之CNN（二十六）MobileViT网络讲解及通过Pytorch搭建</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/05/ConvNeXt%E7%BD%91%E7%BB%9C%E8%AE%B2%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA/" title="2023&#x2F;06&#x2F;05&#x2F;ConvNeXt网络讲解及使用Pytorch搭建&#x2F;">深度学习模型之CNN（二十五）ConvNeXt网络讲解及使用Pytorch搭建</a>
        </li>
    </ul>
  </div>
    </div>


    



  </aside>






    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://linvilyao.github.io/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Linvil Yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linvil's Blog">
      <meta itemprop="description" content="Welcome to Linvil's Blog!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练 | Linvil's Blog">
      <meta itemprop="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.2》，作为随堂笔记。使用pytorch搭建mobilenetv2和mobilenetv3网络，并基于迁移学习方式训练">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-05-23 19:06:13" itemprop="dateCreated datePublished" datetime="2023-05-23T19:06:13+08:00">2023-05-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-14 12:07:06" itemprop="dateModified" datetime="2023-06-14T12:07:06+08:00">2023-06-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span id="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Valine：</span>
  
    <a title="valine" href="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>29k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

            <div class="post-description">本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.2》，作为随堂笔记。使用pytorch搭建mobilenetv2和mobilenetv3网络，并基于迁移学习方式训练</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1>MobileNet v2v3网络搭建</h1>
<p><strong>工程目录</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── Test6_mobilenet</span><br><span class="line">	├── model_v2.py（MobileNet v2模型文件）  </span><br><span class="line">	├── model_v3.py（MobileNet v3模型文件）  </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,MobileNetV2/V3.pth文件）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	├── mobilenet_v2.pth（用于迁移学习时，提前下载好官方的MobileNet v2权重脚本）</span><br><span class="line">	└── mobilenet_v3_large.pth（用于迁移学习时，提前下载好官方的mobilenet_v3_large权重脚本）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<h2 id="model-v2-py">model_v2.py</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNReLU</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, expand_ratio</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, alpha=<span class="number">1.0</span>, round_nearest=<span class="number">8</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h3 id="网络结构参数"><strong>网络结构参数</strong></h3>
<blockquote>
<p>先通过普通卷积，之后进行一个Inverted residual结构（n：倒残差结构重复几遍），一直重复倒残差结构，再通过1x1的普通卷积操作，紧接着平均池化下采样，最后是通过1x1卷积得到最终输出（作用同全连接层）</p>
</blockquote>
<p>搭建该网络重点在于<strong>搭建好Inverted residual结构</strong></p>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenetv2网络结构参数.png" alt="Mobilenetv2网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="ConvBNReLU类">ConvBNReLU类</h3>
<p>ConvBNReLU类是包含conv、BN和ReLU6激活函数的组合层。在MNv2网络中，所有卷积层以及上节课所讲的DW卷积操作，基本上都是由卷积+BN+ReLU6激活函数组成的。</p>
<p><strong>唯一不同的是在Inverted residual结构中的第三层，也就是使用1x1的卷积对特征矩阵进行降维处理，这里使用线性激活函数</strong>，其他地方就都是通过卷积+BN+ReLU6激活函数组成。</p>
<p>因此创建ConvBNReLU类，继承来自<strong>nn.Sequential父类</strong>（nn.Sequential不需要写forward函数，此处参照pytorch的官方实现）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNReLU</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span></span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNReLU, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, groups=groups, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Inverted-residual结构">Inverted residual结构</h3>
<p>类似于ResNet中的残差结构，只不过普通的ResNet的残差结构是两头粗中间细的结构，但倒残差结构是两头细中间粗的结构。</p>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Inverted residual参数结构.png" alt="Inverted residual结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<blockquote>
<p>第一层：普通卷积层，卷积核大小为1x1，激活函数是ReLU6，采用卷积核个数为tk（t是倍率因子，用来扩大特征矩阵深度）；</p>
<p>第二层：DW卷积，卷积核大小为3x3，步距stride = s（传入参数），激活函数是ReLU6，卷积核个数 = 输入特征矩阵深度 = 输出特征矩阵深度，缩减高宽；</p>
<p>第三层：普通1x1卷积层，采用线性激活函数，卷积核个数为k‘（人为设定）</p>
</blockquote>
<p><strong>__init__函数</strong></p>
<ul>
<li>初始化函数传入参数<code>expand_ratio</code>：拓展因子，即上图表中的t</li>
<li>定义隐层<code>hidden_channel</code>：第一层卷积核的个数，即上图表中的tk</li>
<li><code>self.use_shortcut</code>：是一个bool变量，判断在正向传播过程中是否采用shortcut（只有当stride = 1且输入特征矩阵的shape与输出特征矩阵的shape保持一致市才能使用sortcut）</li>
<li>定义层列表<code>layers</code>：判断expand_ratio是否等于1（在v2网络结构参数表中的第一个bottleneck的第一层中，n = 1，s = 1，因此特征矩阵的shape都未发生变化，因此在搭建过程中进行跳过），当expand_ratio = 1时，跳过该处1x1卷积层。</li>
</ul>
<blockquote>
<p>假设此处expand_ratio！= 1，那么就需要在layers列表中添加一个卷积层，调用ConvBNReLU层结构（输入特征矩阵深度，hidden_channel，由上图（右表）得知第一层卷积核大小为1x1）。</p>
</blockquote>
<ul>
<li>调用<code>layers.extend</code>函数：添加一系列层结构，实际上通过extend和append函数添加层结构是一样的，只不过extend函数能够实现一次性批量插入多个元素。</li>
</ul>
<blockquote>
<p>对于Inverted residual结构，第二层是DW卷积，因此此处调用定义好的ConvBNReLU层结构，输入特征矩阵深度为上一层的输出特征矩阵深度hidden_channel，输出深度同样为hidden_channel（因为DW卷积层的输入特征矩阵深度和输出特征矩阵深度是一样的）</p>
<p>stride是传入的参数，group默认为1时是普通卷积，当传入与输入特征矩阵深度相同的个数的话，就是DW卷积。而此处输入特征矩阵的深度是hidden_channel，因此此处groups=hidden_channel。</p>
<p>第三层是1x1的普通卷积，所采用的的激活函数是线性激活函数（y = x），不能使用定义好的ConvBNReLU，因此使用原始的Conv2d，输入特征矩阵深度为上一层输出特征矩阵深度，输出特征矩阵深度为传入参数out_channel，卷积核大小为1</p>
<p>最后使用Batch Normalization标准化处理。</p>
<p>注意：<strong>因为第三层使用线性激活函数y = x，等于不对输入做任何处理，因此不需要再额外定义一个激活层函数，因为不添加激活层函数，就等于linear激活函数</strong></p>
</blockquote>
<ul>
<li>最后通过nn.Sequential类将*layers参数传入进去（*用于将层列表解包为位置参数，这使我们能够像船体单个参数一样将层列表传递给Sequential模块</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, expand_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        hidden_channel = in_channel * expand_ratio</span><br><span class="line">        self.use_shortcut = stride == <span class="number">1</span> <span class="keyword">and</span> in_channel == out_channel</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">if</span> expand_ratio != <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 1x1 pointwise conv</span></span><br><span class="line">            layers.append(ConvBNReLU(in_channel, hidden_channel, kernel_size=<span class="number">1</span>))</span><br><span class="line">        layers.extend([</span><br><span class="line">            <span class="comment"># 3x3 depthwise conv</span></span><br><span class="line">            ConvBNReLU(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel),</span><br><span class="line">            <span class="comment"># 1x1 pointwise conv(linear)</span></span><br><span class="line">            nn.Conv2d(hidden_channel, out_channel, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p><strong>forward函数</strong></p>
<p>x为输入特征矩阵</p>
<p>首先进行判断，是否使用shortcut，如果使用即返回特征矩阵与shortcut相加之后的输出特征矩阵；如果不使用shortcut，则直接返回主分支上的输出特征矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">	<span class="keyword">if</span> self.use_shortcut:</span><br><span class="line">		<span class="keyword">return</span> x + self.conv(x)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure>
<h3 id="make-divisible函数">_make_divisible函数</h3>
<p><strong>是为了将卷积核个数（输出的通道个数）调整为输入round_nearest参数的整数倍</strong>。搭建中采用round_nearest=8，也就是要将卷积核的个数设置为8的整数倍。</p>
<p>目的：为了更好的调用硬件设备，比如多GPU变形运算，或者多机器分布式运算</p>
<ul>
<li><code>ch</code>：传入的卷积核个数（输出特征矩阵的channel）</li>
<li><code>divisor</code>：传入round_nearest基数，即将卷积核个数ch调整为divisor的整数倍</li>
<li><code>min_ch</code>：最小通道数，如果为None，就将min_ch设置为divisor</li>
<li><code>new_ch</code>：即将卷积核个数调整为离它最近的8的倍数的值</li>
<li>之后进行判断new_ch是否小于传入ch的0.9倍，如果小于，则加上一个divisor（为了确保new_ch向下取整的时候，不会减少超过10%）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function is taken from the original tf repo.</span></span><br><span class="line"><span class="string">    It ensures that all layers have a channel number that is divisible by 8</span></span><br><span class="line"><span class="string">    It can be seen here:</span></span><br><span class="line"><span class="string">    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> min_ch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        min_ch = divisor</span><br><span class="line">    new_ch = <span class="built_in">max</span>(min_ch, <span class="built_in">int</span>(ch + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_ch &lt; <span class="number">0.9</span> * ch:</span><br><span class="line">        new_ch += divisor</span><br><span class="line">    <span class="keyword">return</span> new_ch</span><br></pre></td></tr></table></figure>
<h3 id="定义MNv2网络">定义MNv2网络</h3>
<ul>
<li><code>block</code>：将前面定义的InvertedResidual类传给block</li>
<li><code>input_channel</code>：是第一层卷积层所采用的卷积核的个数，也就是下一层输出特征矩阵的深度，为8的倍数</li>
<li><code>last_channel</code>：指的是参数表中1x1的卷积核，输出特征矩阵深度为1280</li>
<li><code>inverted_residual_setting</code>：根据参数表创建list列表，列表中的元素对应着参数表中每一行的参数t（拓展因子）、c（输出channel）、n（倒残差结构重复次数）、s（每一个block第一层卷积层的步距）</li>
<li><code>features</code>：在其中先添加第一个卷积层，输入channel即彩色图片</li>
</ul>
<blockquote>
<p>通过循环定义一系列的Inverted residual结构。将每一层output_channel通过_make_divisible函数进行调整，再进行n次的Inverted residual（在循环中除了列表中的stride = s，其他都是 = 1）</p>
<p>在经过一系列Inverted residual处理后，接下来的是一个1x1的卷积层，直接使用ConvBNReLU</p>
</blockquote>
<ul>
<li>
<p><code>self.features</code>：通常将以上一系列层结构统称为特征提取层，所以Sequential将刚刚定义好的一系列层结构通过位置参数的形式传入进去，打包成一个整体。</p>
</li>
<li>
<p>定义由<code>self.avgpool</code>和<code>self.classifier</code>组成的分类器</p>
<blockquote>
<p>首先定义平均池化下采样层（采用自适应的平均池化下采样操作，给定输出特征矩阵的高宽为1）</p>
<p>再通过nn.Sequential将Dropout层和全连接层组合在一起</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, alpha=<span class="number">1.0</span>, round_nearest=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV2, self).__init__()</span><br><span class="line">        block = InvertedResidual</span><br><span class="line">        input_channel = _make_divisible(<span class="number">32</span> * alpha, round_nearest)</span><br><span class="line">        last_channel = _make_divisible(<span class="number">1280</span> * alpha, round_nearest)</span><br><span class="line"></span><br><span class="line">        inverted_residual_setting = [</span><br><span class="line">            <span class="comment"># t, c, n, s</span></span><br><span class="line">            [<span class="number">1</span>, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">24</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">96</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">160</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">320</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        features = []</span><br><span class="line">        <span class="comment"># conv1 layer</span></span><br><span class="line">        features.append(ConvBNReLU(<span class="number">3</span>, input_channel, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># building inverted residual residual blockes</span></span><br><span class="line">        <span class="keyword">for</span> t, c, n, s <span class="keyword">in</span> inverted_residual_setting:</span><br><span class="line">            output_channel = _make_divisible(c * alpha, round_nearest)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                stride = s <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                features.append(block(input_channel, output_channel, stride, expand_ratio=t))</span><br><span class="line">                input_channel = output_channel</span><br><span class="line">        <span class="comment"># building last several layers</span></span><br><span class="line">        features.append(ConvBNReLU(input_channel, last_channel, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># combine feature layers</span></span><br><span class="line">        self.features = nn.Sequential(*features)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building classifier</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(last_channel, num_classes)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><strong>权重初始化</strong></p>
<ul>
<li>遍历每一个子模块，如果是卷积层，就对权重进行初始化，如果存在偏置，则将偏置设置为0；</li>
<li>如果子模块是一个BN层，则将方差设置为1，均值设置为0；</li>
<li>如果子模块是一个全连接层的话，对权重进行初始化，normal_函数为一个正态分布函数，将权重调整为均值为0.0，方差为0.01的正态分布；将偏置设置为0</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># weight initialization</span></span><br><span class="line">	<span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">			nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>)</span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				nn.init.zeros_(m.bias)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">			nn.init.ones_(m.weight)</span><br><span class="line">			nn.init.zeros_(m.bias)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">			nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">			nn.init.zeros_(m.bias)</span><br></pre></td></tr></table></figure>
<p><strong>正向传播过程</strong></p>
<p>首先将输入特征矩阵输入进特征提取部分，通过平均池化下采样得到输出，再对输出进行展平处理，最后通过分类器得到最终输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">	x = self.features(x)</span><br><span class="line">	x = self.avgpool(x)</span><br><span class="line">	x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">	x = self.classifier(x)</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="model-v3-py">model_v3.py</h2>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenet v3网络结构参数.png" alt="Mobilenet v3网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="comment">#同MobileNet v2中的_make_divisible作用一致</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 out_planes: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 kernel_size: <span class="built_in">int</span> = <span class="number">3</span>, </span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span> = <span class="number">1</span>, </span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeExcitation</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_c: <span class="built_in">int</span>, squeeze_factor: <span class="built_in">int</span> = <span class="number">4</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualConfig</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_c: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 kernel: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 expanded_c: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 out_c: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 use_se: <span class="built_in">bool</span>, </span></span><br><span class="line"><span class="params">                 activation: <span class="built_in">str</span>, </span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjust_channels</span>(<span class="params">channels: <span class="built_in">int</span>, width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="keyword">return</span> _make_divisible(channels * width_multi, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cnf: InvertedResidualConfig, norm_layer: <span class="type">Callable</span>[..., nn.Module]</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 inverted_residual_setting: <span class="type">List</span>[InvertedResidualConfig], </span></span><br><span class="line"><span class="params">                 last_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 block: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_large</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_small</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h3 id="make-divisible函数-2">_make_divisible函数</h3>
<p>同MobileNet v2中的_make_divisible作用一致，<strong>是为了将卷积核个数（输出的通道个数）调整为输入round_nearest参数的整数倍</strong>。搭建中采用round_nearest=8，也就是要将卷积核的个数设置为8的整数倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function is taken from the original tf repo.</span></span><br><span class="line"><span class="string">    It ensures that all layers have a channel number that is divisible by 8</span></span><br><span class="line"><span class="string">    It can be seen here:</span></span><br><span class="line"><span class="string">    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> min_ch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        min_ch = divisor</span><br><span class="line">    new_ch = <span class="built_in">max</span>(min_ch, <span class="built_in">int</span>(ch + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_ch &lt; <span class="number">0.9</span> * ch:</span><br><span class="line">        new_ch += divisor</span><br><span class="line">    <span class="keyword">return</span> new_ch</span><br></pre></td></tr></table></figure>
<h3 id="ConvBNActivation类">ConvBNActivation类</h3>
<p>ConvBNActivation类是包含conv、BN和激活函数的组合层。</p>
<ul>
<li><code>in_planes</code>：输入特征矩阵的深度</li>
<li><code>out_planes</code>：输出特征矩阵的深度，对应卷积核的个数</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层</li>
<li><code>activation_layer</code>：对应激活函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel_size: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> activation_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            activation_layer = nn.ReLU6</span><br><span class="line">        <span class="built_in">super</span>(ConvBNActivation, self).__init__(nn.Conv2d(in_channels=in_planes,</span><br><span class="line">                                                         out_channels=out_planes,</span><br><span class="line">                                                         kernel_size=kernel_size,</span><br><span class="line">                                                         stride=stride,</span><br><span class="line">                                                         padding=padding,</span><br><span class="line">                                                         groups=groups,</span><br><span class="line">                                                         bias=<span class="literal">False</span>),</span><br><span class="line">                                               norm_layer(out_planes),</span><br><span class="line">                                               activation_layer(inplace=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<h3 id="SqueezeExcitation模块">SqueezeExcitation模块</h3>
<p><strong>初始化函数</strong></p>
<p>相当于两个全连接层。</p>
<ul>
<li><strong>对于第一个全连接层，此处的节点个数 = 该处输入的特征矩阵channel的1/4</strong>（在v3原论文中作者有给出）</li>
<li>第二层全连接层的节点个数 = 该处输入的特征矩阵channel</li>
<li>第一个全连接层的激活函数是ReLU，第二个全连接层的激活函数是hard-sigmoid</li>
</ul>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/SE注意力机制模块.png" alt="SE注意力机制模块" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<ul>
<li><code>squeeze_factor</code>：因为第一个全连接层的节点个数是输入的特征矩阵channel的1/4，所以这里存放的是分母4；</li>
<li><code>squeeze_c</code>：调用_make_divisible方法调整到离该数最近的8的整数倍的数字；</li>
<li><code>self.fc1</code>：使用卷积来作为全连接层，作用与全连接层一样</li>
<li><code>self.fc2</code>：输入channel是上一层的输出channel，所以是squeeze_c，又因为SE机制最终输出需要和输入特征矩阵的channel保持一致，所以输出channel为input_c</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeExcitation</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_c: <span class="built_in">int</span>, squeeze_factor: <span class="built_in">int</span> = <span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SqueezeExcitation, self).__init__()</span><br><span class="line">        squeeze_c = _make_divisible(input_c // squeeze_factor, <span class="number">8</span>)</span><br><span class="line">        self.fc1 = nn.Conv2d(input_c, squeeze_c, <span class="number">1</span>)</span><br><span class="line">        self.fc2 = nn.Conv2d(squeeze_c, input_c, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>正向传播函数</strong></p>
<p>如上图（下）第三个特征矩阵需要进行池化操作，因此进行自适应池化下采样操作，并且返回特征矩阵的高宽是1x1。再将拿到的输出通过全连接层，relu激活函数，第二层全连接层，和一个h-sigmoid激活函数，得到输出。<strong>这里的输出就是第二层全连接层的输出，也就是得到了不同channel对应的权重</strong>。</p>
<p>接下来需要将输出与原特征矩阵相乘，得到通过SE模块之后的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor: <span class="comment"># 返回Tensor结构</span></span><br><span class="line">       scale = F.adaptive_avg_pool2d(x, output_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">       scale = self.fc1(scale)</span><br><span class="line">       scale = F.relu(scale, inplace=<span class="literal">True</span>)</span><br><span class="line">       scale = self.fc2(scale)</span><br><span class="line">       scale = F.hardsigmoid(scale, inplace=<span class="literal">True</span>)</span><br><span class="line">       <span class="keyword">return</span> scale * x</span><br></pre></td></tr></table></figure>
<h3 id="InvertedResidualConfig类">InvertedResidualConfig类</h3>
<p>InvertedResidualConfig对应的是MobileNetV3中的每一个bneck结构的参数配置，其中有</p>
<ul>
<li><code>input_c</code>：输入特征矩阵的大小（主要指channel）；</li>
<li><code>kernel</code>：每一层使用的kernel_size（即DW卷积中的卷积核大小）；</li>
<li><code>expanded_c</code>：第一层卷积层所采用的的卷积核的个数；</li>
<li><code>out_c</code>：最后一层1x1的卷积层输出得到特征矩阵的channel；</li>
<li><code>use_se</code>：是否使用SE注意力机制；</li>
<li><code>activation</code>：采用的激活函数，RE表示采用ReLU激活函数，HS表示采用H-Swish激活函数；</li>
<li><code>stride</code>：指的是DW卷积所对应的步距；</li>
<li><code>width_multi</code>：就是MNv2中提到的$\alpha$参数，用来调节每一个卷积层所使用channel的倍率因子。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualConfig</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 input_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 expanded_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 use_se: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">        self.input_c = self.adjust_channels(input_c, width_multi)</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.expanded_c = self.adjust_channels(expanded_c, width_multi)</span><br><span class="line">        self.out_c = self.adjust_channels(out_c, width_multi)</span><br><span class="line">        self.use_se = use_se</span><br><span class="line">        self.use_hs = activation == <span class="string">&quot;HS&quot;</span>  <span class="comment"># whether using h-swish activation</span></span><br><span class="line">        self.stride = stride</span><br></pre></td></tr></table></figure>
<p><strong>adjust_channels函数</strong></p>
<p>是一个静态方法，其实也是直接调用了_make_divisible方法，传入参数为channel和$\alpha$倍率因子，最终得到channel和$\alpha$的乘积离8最近的整数倍的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adjust_channels</span>(<span class="params">channels: <span class="built_in">int</span>, width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">	<span class="keyword">return</span> _make_divisible(channels * width_multi, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h3 id="InvertedResidual类">InvertedResidual类</h3>
<p><strong>初始化函数</strong></p>
<ul>
<li><code>cnf</code>：前文提到的InvertedResidualConfig配置文件；</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层</li>
<li><code>cnf.stride</code>：判断步距是否为1或2，因为在网络参数表中，步距只有1和2两种情况，当出现第三种情况时，就是不合法的步距情况；再判断</li>
<li><code>self.use_res_connect</code>：是否使用shortcut连接，shortcut只有在stride == 1且input_c == output_c时才有；</li>
<li><code>activation_layer</code>：判断使用ReLU或者H-Swish激活函数（官方是在1.7及以上版本中才有官方实现的H-Swish和H-Sigmoid激活函数，如果需要使用MNv3网络的话，得把pytorch版本更新至1.7及以上）</li>
<li>expand区域指在InvertedResidual结构中的第一个1x1卷积层进行升维处理，因为第一个kneck存在输入特征矩阵的channel和输出特征矩阵的channel相等，因此可以跳过，所以会进行判断cnf.expanded_c != cnf.input_c；</li>
<li>depthwise区域为dw卷积区域</li>
</ul>
<blockquote>
<p><code>groups</code>：由于DW卷积是针对每一个channel都单独使用一个channel为1的卷积核来进行卷及处理，所以groups和channel的个数是保持一致的，所以groups=cnf.expanded_c</p>
</blockquote>
<ul>
<li>project区域是InvertedResidual结构中1x1卷积中的降维部分，activation_layer=nn.Identity中的Identity其实就是线性y = x，没有做任何处理；</li>
<li></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 cnf: InvertedResidualConfig,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Callable</span>[..., nn.Module]</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cnf.stride <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;illegal stride value.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.use_res_connect = (cnf.stride == <span class="number">1</span> <span class="keyword">and</span> cnf.input_c == cnf.out_c)</span><br><span class="line"></span><br><span class="line">        layers: <span class="type">List</span>[nn.Module] = []</span><br><span class="line">        activation_layer = nn.Hardswish <span class="keyword">if</span> cnf.use_hs <span class="keyword">else</span> nn.ReLU</span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand</span></span><br><span class="line">        <span class="keyword">if</span> cnf.expanded_c != cnf.input_c:</span><br><span class="line">            layers.append(ConvBNActivation(cnf.input_c,</span><br><span class="line">                                           cnf.expanded_c,</span><br><span class="line">                                           kernel_size=<span class="number">1</span>,</span><br><span class="line">                                           norm_layer=norm_layer,</span><br><span class="line">                                           activation_layer=activation_layer))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># depthwise</span></span><br><span class="line">        layers.append(ConvBNActivation(cnf.expanded_c,</span><br><span class="line">                                       cnf.expanded_c,</span><br><span class="line">                                       kernel_size=cnf.kernel,</span><br><span class="line">                                       stride=cnf.stride,</span><br><span class="line">                                       groups=cnf.expanded_c,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=activation_layer))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cnf.use_se:</span><br><span class="line">            layers.append(SqueezeExcitation(cnf.expanded_c))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project</span></span><br><span class="line">        layers.append(ConvBNActivation(cnf.expanded_c,</span><br><span class="line">                                       cnf.out_c,</span><br><span class="line">                                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=nn.Identity))</span><br><span class="line"></span><br><span class="line">        self.block = nn.Sequential(*layers)</span><br><span class="line">        self.out_channels = cnf.out_c</span><br></pre></td></tr></table></figure>
<p><strong>正向传播函数</strong></p>
<p>将特征矩阵传入block方法得到主分支输出特征矩阵，再经过判断是否有shortcut，如果有则相加，无则直接输出主分支特征矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">	result = self.block(x)</span><br><span class="line">	<span class="keyword">if</span> self.use_res_connect:</span><br><span class="line">		result += x</span><br><span class="line">	<span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="定义MNv3类">定义MNv3类</h3>
<p><strong>初始化函数</strong></p>
<ul>
<li><code>inverted_residual_setting</code>：对应一系列bneck结构参数的列表；</li>
<li><code>last_channel</code>：对应是MNv3网络参数表中倒数第二个全连接层的输出节点的个数；</li>
<li><code>block</code>：就是之前定义的InvertedResidual模块；</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层；</li>
</ul>
<p>当inverted_residual_setting为空或者不是一个list的话都会报错。</p>
<p><strong>building first layer</strong>创建第一层conv2d</p>
<blockquote>
<p>firstconv_output_c获取第一个bneck结构的输入特征矩阵的channel，所对应的是第一个卷积层输出的channel；</p>
<p>ConvBNActivation对应的是第一个conv2d，无论是v3-Large还是v3-Small，第一个都是3x3的卷积层。所以先创建了一个3x3的卷积层。</p>
</blockquote>
<p><strong>building inverted residual blocks</strong>创建block模块</p>
<blockquote>
<p>遍历每一个bneck结构，将每一层的配置文件和norm_layer都传给block，将创建好的每一个block结构，也就是InvertedResidual模块给填进layers当中。</p>
</blockquote>
<p><strong>building last several layers</strong>创建平均池化下采样层和几个卷积层</p>
<blockquote>
<p><code>lastconv_input_c</code>：最后一个bneck模块的输出特征矩阵channel；</p>
<p><code>lastconv_output_c</code>：无论是v3-Large还是v3-Small，lastconv_output_c都是lastconv_input_c的6倍</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 inverted_residual_setting: <span class="type">List</span>[InvertedResidualConfig],</span></span><br><span class="line"><span class="params">                 last_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 block: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV3, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inverted_residual_setting:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;The inverted_residual_setting should not be empty.&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> (<span class="built_in">isinstance</span>(inverted_residual_setting, <span class="type">List</span>) <span class="keyword">and</span></span><br><span class="line">                  <span class="built_in">all</span>([<span class="built_in">isinstance</span>(s, InvertedResidualConfig) <span class="keyword">for</span> s <span class="keyword">in</span> inverted_residual_setting])):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;The inverted_residual_setting should be List[InvertedResidualConfig]&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            block = InvertedResidual</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = partial(nn.BatchNorm2d, eps=<span class="number">0.001</span>, momentum=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">        layers: <span class="type">List</span>[nn.Module] = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building first layer</span></span><br><span class="line">        firstconv_output_c = inverted_residual_setting[<span class="number">0</span>].input_c</span><br><span class="line">        layers.append(ConvBNActivation(<span class="number">3</span>,</span><br><span class="line">                                       firstconv_output_c,</span><br><span class="line">                                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                                       stride=<span class="number">2</span>,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=nn.Hardswish))</span><br><span class="line">        <span class="comment"># building inverted residual blocks</span></span><br><span class="line">        <span class="keyword">for</span> cnf <span class="keyword">in</span> inverted_residual_setting:</span><br><span class="line">            layers.append(block(cnf, norm_layer))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building last several layers</span></span><br><span class="line">        lastconv_input_c = inverted_residual_setting[-<span class="number">1</span>].out_c</span><br><span class="line">        lastconv_output_c = <span class="number">6</span> * lastconv_input_c</span><br><span class="line">        layers.append(ConvBNActivation(lastconv_input_c,</span><br><span class="line">                                       lastconv_output_c,</span><br><span class="line">                                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=nn.Hardswish))</span><br><span class="line">        self.features = nn.Sequential(*layers)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.classifier = nn.Sequential(nn.Linear(lastconv_output_c, last_channel),</span><br><span class="line">                                        nn.Hardswish(inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Dropout(p=<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Linear(last_channel, num_classes))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initial weights</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.ones_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br></pre></td></tr></table></figure>
<p><strong>正向传播函数</strong></p>
<p>将输入特征矩阵依次通过特征提取、平均池化、展平和classifier处理得到输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">return</span> self._forward_impl(x)</span><br></pre></td></tr></table></figure>
<h3 id="定义mobilenet-v3-large">定义mobilenet_v3_large</h3>
<ul>
<li><code>width_multi</code>：$\alpha$超参数，用来调整channel；</li>
<li><code>bneck_conf</code>：同样使用partial方法，给InvertedResidualConfig方法传入默认参数width_multi，即$\alpha$超参数；</li>
<li><code>adjust_channels</code>：即InvertedResidualConfig类中的adjust_channels方法，使用partial方法，给InvertedResidualConfig.adjust_channels方法传入默认参数width_multi，即$\alpha$超参数；</li>
<li><code>reduce_divider</code>：对应最后三层bneck结构，对卷积的channel进行了调整，这里默认为False，指不做调整，如果设置为True，可以减少一些参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_large</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Constructs a large MobileNetV3 architecture from</span></span><br><span class="line"><span class="string">    &quot;Searching for MobileNetV3&quot; &lt;https://arxiv.org/abs/1905.02244&gt;.</span></span><br><span class="line"><span class="string">    weights_link:</span></span><br><span class="line"><span class="string">    https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes</span></span><br><span class="line"><span class="string">        reduced_tail (bool): If True, reduces the channel counts of all feature layers</span></span><br><span class="line"><span class="string">            between C4 and C5 by 2. It is used to reduce the channel redundancy in the</span></span><br><span class="line"><span class="string">            backbone for Detection and Segmentation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    width_multi = <span class="number">1.0</span></span><br><span class="line">    bneck_conf = partial(InvertedResidualConfig, width_multi=width_multi)</span><br><span class="line">    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_multi=width_multi)</span><br><span class="line"></span><br><span class="line">    reduce_divider = <span class="number">2</span> <span class="keyword">if</span> reduced_tail <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    inverted_residual_setting = [</span><br><span class="line">        <span class="comment"># input_c, kernel, expanded_c, out_c, use_se, activation, stride</span></span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C1</span></span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">3</span>, <span class="number">72</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">5</span>, <span class="number">72</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C2</span></span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C3</span></span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">200</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">184</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">184</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">480</span>, <span class="number">112</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">112</span>, <span class="number">3</span>, <span class="number">672</span>, <span class="number">112</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">112</span>, <span class="number">5</span>, <span class="number">672</span>, <span class="number">160</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C4</span></span><br><span class="line">        bneck_conf(<span class="number">160</span> // reduce_divider, <span class="number">5</span>, <span class="number">960</span> // reduce_divider, <span class="number">160</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">160</span> // reduce_divider, <span class="number">5</span>, <span class="number">960</span> // reduce_divider, <span class="number">160</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">    ]</span><br><span class="line">    last_channel = adjust_channels(<span class="number">1280</span> // reduce_divider)  <span class="comment"># C5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> MobileNetV3(inverted_residual_setting=inverted_residual_setting,</span><br><span class="line">                       last_channel=last_channel,</span><br><span class="line">                       num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<h3 id="定义mobilenet-v3-small">定义mobilenet_v3_small</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_small</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Constructs a large MobileNetV3 architecture from</span></span><br><span class="line"><span class="string">    &quot;Searching for MobileNetV3&quot; &lt;https://arxiv.org/abs/1905.02244&gt;.</span></span><br><span class="line"><span class="string">    weights_link:</span></span><br><span class="line"><span class="string">    https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes</span></span><br><span class="line"><span class="string">        reduced_tail (bool): If True, reduces the channel counts of all feature layers</span></span><br><span class="line"><span class="string">            between C4 and C5 by 2. It is used to reduce the channel redundancy in the</span></span><br><span class="line"><span class="string">            backbone for Detection and Segmentation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    width_multi = <span class="number">1.0</span></span><br><span class="line">    bneck_conf = partial(InvertedResidualConfig, width_multi=width_multi)</span><br><span class="line">    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_multi=width_multi)</span><br><span class="line"></span><br><span class="line">    reduce_divider = <span class="number">2</span> <span class="keyword">if</span> reduced_tail <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    inverted_residual_setting = [</span><br><span class="line">        <span class="comment"># input_c, kernel, expanded_c, out_c, use_se, activation, stride</span></span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C1</span></span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">72</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C2</span></span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">3</span>, <span class="number">88</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">5</span>, <span class="number">96</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C3</span></span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">240</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">240</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">48</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">48</span>, <span class="number">5</span>, <span class="number">144</span>, <span class="number">48</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">48</span>, <span class="number">5</span>, <span class="number">288</span>, <span class="number">96</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C4</span></span><br><span class="line">        bneck_conf(<span class="number">96</span> // reduce_divider, <span class="number">5</span>, <span class="number">576</span> // reduce_divider, <span class="number">96</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">96</span> // reduce_divider, <span class="number">5</span>, <span class="number">576</span> // reduce_divider, <span class="number">96</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>)</span><br><span class="line">    ]</span><br><span class="line">    last_channel = adjust_channels(<span class="number">1024</span> // reduce_divider)  <span class="comment"># C5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> MobileNetV3(inverted_residual_setting=inverted_residual_setting,</span><br><span class="line">                       last_channel=last_channel,</span><br><span class="line">                       num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<h2 id="train-py"><a target="_blank" rel="noopener" href="http://train.py">train.py</a></h2>
<p>下载官方权重文件：输入import torchvision.model.mobilenet，ctrl+左键进入函数当中，会有显示下载链接</p>
<p>该训练脚本与前期使用的AlexNet、VGG、GooglNet以及ResNet所使用的训练脚本基本一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># 训练MobileNetv2</span></span><br><span class="line"><span class="keyword">from</span> model_v2 <span class="keyword">import</span> MobileNetV2</span><br><span class="line"><span class="comment"># 训练MobileNetv3-Large</span></span><br><span class="line"><span class="keyword">from</span> model_v3 <span class="keyword">import</span> mobilenet_v3_large</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">16</span></span><br><span class="line">    epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                   transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])&#125;</span><br><span class="line"></span><br><span class="line">    data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))  <span class="comment"># get data root path</span></span><br><span class="line">    image_path = os.path.join(data_root, <span class="string">&quot;data_set&quot;</span>, <span class="string">&quot;flower_data&quot;</span>)  <span class="comment"># flower data set path</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;train&quot;</span>),</span><br><span class="line">                                         transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">    train_num = <span class="built_in">len</span>(train_dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line">    flower_list = train_dataset.class_to_idx</span><br><span class="line">    cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line">    <span class="comment"># write dict into json file</span></span><br><span class="line">    json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">    nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line"></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                               batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                               num_workers=nw)</span><br><span class="line"></span><br><span class="line">    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>),</span><br><span class="line">                                            transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">    val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">    validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                                  batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                  num_workers=nw)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(train_num,</span><br><span class="line">                                                                           val_num))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    <span class="comment"># 训练MobileNetv2</span></span><br><span class="line">    net = MobileNetV2(num_classes=<span class="number">5</span>)</span><br><span class="line">	<span class="comment"># 训练MobileNetv3-Large</span></span><br><span class="line">    net = mobilenet_v3_large(num_classes=<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># load pretrain weights</span></span><br><span class="line">    <span class="comment"># download url: https://download.pytorch.org/models/mobilenet_v2-b0353104.pth</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./mobilenet_v2.pth&quot;</span> <span class="comment">#&quot;./mobilenet_v3_large.pth&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; dose not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line">    pre_weights = torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># delete classifier weights</span></span><br><span class="line">    pre_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pre_weights.items() <span class="keyword">if</span> net.state_dict()[k].numel() == v.numel()&#125;</span><br><span class="line">    missing_keys, unexpected_keys = net.load_state_dict(pre_dict, strict=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># freeze features weights</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.features.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    net.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define loss function</span></span><br><span class="line">    loss_function = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># construct an optimizer</span></span><br><span class="line">    params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    optimizer = optim.Adam(params, lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line">    save_path = <span class="string">&#x27;./MobileNetV2.pth&#x27;</span> <span class="comment"># save_path = &#x27;./MobileNetV3.pth&#x27;</span></span><br><span class="line">    train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># train</span></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        train_bar = tqdm(train_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">            images, labels = data</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            logits = net(images.to(device))</span><br><span class="line">            loss = loss_function(logits, labels.to(device))</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print statistics</span></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                     epochs,</span><br><span class="line">                                                                     loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">            <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">                val_images, val_labels = val_data</span><br><span class="line">                outputs = net(val_images.to(device))</span><br><span class="line">                <span class="comment"># loss = loss_function(outputs, test_labels)</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">                val_bar.desc = <span class="string">&quot;valid epoch[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                           epochs)</span><br><span class="line">        val_accurate = acc / val_num</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">            best_acc = val_accurate</span><br><span class="line">            torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>**需要注意的不同点：**模型权重加载部分</p>
<p>首先实例化模型，将类别个数设置为5；</p>
<p>通过torch.load函数载入预训练参数，载入进之后是一个字典类型。因为官方是在ImageNet数据集上进行预训练，所以最后一层全连接层的节点个数 = 1000，而我们这最后一层节点个数 = 5，所以最后一层不能用。</p>
<p>因此首先遍历权重字典，查找权重名称中是否含有classifier参数，如果有这个参数，说明是最后一层全连接层的参数。如果没有classifier，则直接保存进pre_dict字典变量当中。</p>
<p>再通过net.load_state_dict函数将不包含classifier全连接层的权重字典全部载入进去。</p>
<p>之后冻结特征提取部分的所有权重，通过遍历net.features下的所有参数，将参数的requires_grad全部设置为False，这样就不会对其进行求导及参数更新</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create model</span></span><br><span class="line"> net = MobileNetV2(num_classes=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># load pretrain weights</span></span><br><span class="line"> <span class="comment"># download url: https://download.pytorch.org/models/mobilenet_v2-b0353104.pth</span></span><br><span class="line"> model_weight_path = <span class="string">&quot;./mobilenet_v2.pth&quot;</span></span><br><span class="line"> <span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; dose not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line"> pre_weights = torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># delete classifier weights</span></span><br><span class="line"> pre_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pre_weights.items() <span class="keyword">if</span> <span class="string">&quot;classifier&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> k&#125;</span><br><span class="line"> missing_keys, unexpected_keys = net.load_state_dict(pre_dict, strict=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line"> <span class="comment"># freeze features weights</span></span><br><span class="line"> <span class="keyword">for</span> param <span class="keyword">in</span> net.features.parameters():</span><br><span class="line">     param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="MNv2训练结果"><strong>MNv2训练结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv2%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="MNv2训练结果"></p>
<h3 id="MNv3-Large训练结果"><strong>MNv3-Large训练结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv3%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="MNv3训练结果"></p>
<h2 id="predict-py"><a target="_blank" rel="noopener" href="http://predict.py">predict.py</a></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># MNv2</span></span><br><span class="line"><span class="keyword">from</span> model_v2 <span class="keyword">import</span> MobileNetV2</span><br><span class="line"><span class="comment"># MNv3-Large</span></span><br><span class="line"><span class="keyword">from</span> model_v3 <span class="keyword">import</span> mobilenet_v3_large</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(<span class="number">256</span>),</span><br><span class="line">         transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    img_path = <span class="string">&quot;tulip.jpg&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    img = data_transform(img)</span><br><span class="line">    <span class="comment"># expand batch dimension</span></span><br><span class="line">    img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    <span class="comment"># MNv2</span></span><br><span class="line">    model = MobileNetV2(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line">    <span class="comment"># MNv3-Large</span></span><br><span class="line">    model = mobilenet_v3_large(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./MobileNetV2.pth&quot;</span> <span class="comment"># model_weight_path = &quot;./MobileNetV3.pth&quot;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_weight_path, map_location=device))</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># predict class</span></span><br><span class="line">        output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">        predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">        predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">    print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                                 predict[predict_cla].numpy())</span><br><span class="line">    plt.title(print_res)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                                  predict[i].numpy()))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="MNv2预测结果"><strong>MNv2预测结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv2%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="MNv2预测结果"></p>
<h3 id="MNv3-Large预测结果"><strong>MNv3-Large预测结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv3%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="MNv3预测结果"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
              <a href="/tags/Pytorch%E6%90%AD%E5%BB%BACNN/" rel="tag"><i class="fa fa-tag"></i> Pytorch搭建CNN</a>
              <a href="/tags/MobileNetV2V3/" rel="tag"><i class="fa fa-tag"></i> MobileNetV2V3</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" rel="prev" title="深度学习模型之CNN（十三）MobileNetv1v2v3网络详解">
                  <i class="fa fa-chevron-left"></i> 深度学习模型之CNN（十三）MobileNetv1v2v3网络详解
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/24/ShuffleNetv1v2%E7%90%86%E8%AE%BA%E8%AE%B2%E8%A7%A3/" rel="next" title="深度学习模型之CNN（十五）ShuffleNetv1v2理论讲解">
                  深度学习模型之CNN（十五）ShuffleNetv1v2理论讲解 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="valine-comments"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Linvil Yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("04/21/2023 22:22:22");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "已运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","app_key":"iNfyeaWVmA11Duj7fzr0B1r1","server_url":"https://16p3s6fl.lc-cn-n1-shared.com","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '32px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>


<script class="next-config" data-name="valine" type="application/json">{"enable":true,"appId":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","appKey":"iNfyeaWVmA11Duj7fzr0B1r1","serverURLs":"https://16p3s6fl.lc-cn-n1-shared.com","placeholder":"请写下您的评论","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"lang":null,"visitor":false,"comment_count":true,"recordIP":true,"enableQQ":true,"requiredFields":[],"el":"#valine-comments","path":"/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.valine.el)
    .then(() => NexT.utils.getScript(
      'https://fastly.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
      { condition: window.Valine }
    ))
    .then(() => {
      new Valine(CONFIG.valine);
    });
});
</script>

</body>
</html>
