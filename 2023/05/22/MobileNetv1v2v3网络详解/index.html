<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logosc/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logosc/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"linvilyao.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":14,"offset":10},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.1》，作为随堂笔记。本节课学习MobileNet v1、MobileNet v2和MobileNet v3论文中一些亮点，包括Depthwide Convolution、Pointwise Convolution以及Inverted Residuals等结构。并对MobileNet v2和MobileNet v3网络结构进行学习。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习模型之CNN（十三）MobileNetv1v2v3网络详解">
<meta property="og:url" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="Linvil&#39;s Blog">
<meta property="og:description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.1》，作为随堂笔记。本节课学习MobileNet v1、MobileNet v2和MobileNet v3论文中一些亮点，包括Depthwide Convolution、Pointwise Convolution以及Inverted Residuals等结构。并对MobileNet v2和MobileNet v3网络结构进行学习。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/DW%E5%8D%B7%E7%A7%AF.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/PW%E5%8D%B7%E7%A7%AF.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E5%8D%B7%E7%A7%AF%E5%92%8C%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8F%82%E6%95%B0%E5%8C%BA%E5%88%AB.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv1%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inverted%20Residual%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/ReLU6%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Linear%20Bottleneck.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inverted%20residual%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%9B%BE.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet%20v2%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv3L%E5%AF%B9%E6%AF%94v3S%E5%AF%B9%E6%AF%94v2.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv3%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3%E6%9B%B4%E6%96%B0Block%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3%E4%B8%A4%E5%A4%84%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3%E6%9B%B4%E6%96%B0Block%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1%E8%80%97%E6%97%B6%E5%B1%82%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet%20v3%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet%20v3-Small%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0.png">
<meta property="article:published_time" content="2023-05-22T15:54:01.000Z">
<meta property="article:modified_time" content="2023-05-23T13:32:04.227Z">
<meta property="article:author" content="Linvil Yao">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="CNN网络详解">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/DW%E5%8D%B7%E7%A7%AF.png">


<link rel="canonical" href="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/","path":"2023/05/22/MobileNetv1v2v3网络详解/","title":"深度学习模型之CNN（十三）MobileNetv1v2v3网络详解"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习模型之CNN（十三）MobileNetv1v2v3网络详解 | Linvil's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linvil's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">这是一个用来记录的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">5</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">28</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">MobileNet详解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNet-v1"><span class="nav-number">1.1.</span> <span class="nav-text">MobileNet v1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DW%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.1.1.</span> <span class="nav-text">DW卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Depthwise-Separable-Conv"><span class="nav-number">1.1.2.</span> <span class="nav-text">Depthwise Separable Conv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DW-PW%E5%8D%B7%E7%A7%AF%E5%92%8C%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.1.3.</span> <span class="nav-text">DW&amp;PW卷积和普通卷积的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNet-v1%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">MobileNet v1网络结构参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNet-v2"><span class="nav-number">1.2.</span> <span class="nav-text">MobileNet v2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inverted-Residuals"><span class="nav-number">1.2.1.</span> <span class="nav-text">Inverted Residuals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Bottlenecks"><span class="nav-number">1.2.2.</span> <span class="nav-text">Linear Bottlenecks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNet-v2%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">MobileNet v2网络结构参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="nav-number">1.2.4.</span> <span class="nav-text">性能对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNet-v3"><span class="nav-number">1.3.</span> <span class="nav-text">MobileNet v3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0Block"><span class="nav-number">1.3.1.</span> <span class="nav-text">更新Block</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1%E8%80%97%E6%97%B6%E5%B1%82%E7%BB%93%E6%9E%84"><span class="nav-number">1.3.2.</span> <span class="nav-text">重新设计耗时层结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.3.</span> <span class="nav-text">重新设计激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V3-Large%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.4.</span> <span class="nav-text">V3-Large网络结构参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V3-Small%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.5.</span> <span class="nav-text">V3-Small网络结构参数</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Linvil Yao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Linvil Yao</p>
  <div class="site-description" itemprop="description">Welcome to Linvil's Blog!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


        </div>
      </div>


  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/26/EfficientNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" title="2023&#x2F;05&#x2F;26&#x2F;EfficientNet网络详解&#x2F;">深度学习模型之CNN（十七）EfficientNet网络详解</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/25/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAShuffleNetv2/" title="2023&#x2F;05&#x2F;25&#x2F;使用Pytorch搭建ShuffleNetv2&#x2F;">深度学习模型之CNN（十六）使用Pytorch搭建ShuffleNetv2</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/24/ShuffleNetv1v2%E7%90%86%E8%AE%BA%E8%AE%B2%E8%A7%A3/" title="2023&#x2F;05&#x2F;24&#x2F;ShuffleNetv1v2理论讲解&#x2F;">深度学习模型之CNN（十五）ShuffleNetv1v2理论讲解</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/" title="2023&#x2F;05&#x2F;23&#x2F;使用pytorch搭建MobileNetV2V3并基于迁移学习训练&#x2F;">深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" title="2023&#x2F;05&#x2F;22&#x2F;MobileNetv1v2v3网络详解&#x2F;">深度学习模型之CNN（十三）MobileNetv1v2v3网络详解</a>
        </li>
    </ul>
  </div>
    </div>


    



  </aside>






    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://linvilyao.github.io/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Linvil Yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linvil's Blog">
      <meta itemprop="description" content="Welcome to Linvil's Blog!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习模型之CNN（十三）MobileNetv1v2v3网络详解 | Linvil's Blog">
      <meta itemprop="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.1》，作为随堂笔记。本节课学习MobileNet v1、MobileNet v2和MobileNet v3论文中一些亮点，包括Depthwide Convolution、Pointwise Convolution以及Inverted Residuals等结构。并对MobileNet v2和MobileNet v3网络结构进行学习。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习模型之CNN（十三）MobileNetv1v2v3网络详解
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-05-22 23:54:01" itemprop="dateCreated datePublished" datetime="2023-05-22T23:54:01+08:00">2023-05-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-23 21:32:04" itemprop="dateModified" datetime="2023-05-23T21:32:04+08:00">2023-05-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span id="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习模型之CNN（十三）MobileNetv1v2v3网络详解" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Valine：</span>
  
    <a title="valine" href="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

            <div class="post-description">本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础7.1》，作为随堂笔记。本节课学习MobileNet v1、MobileNet v2和MobileNet v3论文中一些亮点，包括Depthwide Convolution、Pointwise Convolution以及Inverted Residuals等结构。并对MobileNet v2和MobileNet v3网络结构进行学习。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1>MobileNet详解</h1>
<p>在传统的卷积神经网络中，内存需求大、运算量大，导致无法在移动设备以及嵌入式设备上运行。在之前学习的网络结构中，例如VGG16的权重大小有400+M、ResNet的152层模型权重大小能到达600+M。如此大的模型参数是不可能在移动设备以及嵌入式设备上运行的。</p>
<p>MobileNet网络是由由google团队在2017年提出的，专注于移动端或者嵌入式设备中的轻量级CNN网络。相比传统卷积神经网络，在准确率小幅降低的前提下大大减少模型参数与运算量。(相比VGG16准确率减少了0.9%，但模型参数只有VGG的1/32)。</p>
<h2 id="MobileNet-v1">MobileNet v1</h2>
<p><strong>MobileNetv1版本</strong>原论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04861">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p>
<p>论文网络中的亮点：</p>
<ul>
<li><strong>Depthwise Convolution</strong>（简称DW卷积，大大减少运算量和参数数量）</li>
<li>增加超参数$\alpha$、$\beta$（人为设定，$\alpha$控制卷积层卷积核的个数、$\beta$控制输入图像大小）</li>
</ul>
<h3 id="DW卷积">DW卷积</h3>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/DW卷积.png" alt="DW卷积" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>传统卷积过程</strong></p>
<blockquote>
<p>输入一个深度为3的特征矩阵，经过4个卷积核进行卷积，每个卷积核的深度与输入特征矩阵的深度是相同的，所以每个卷积核深度也为3。又因输出特征矩阵的深度是由卷积核的个数决定的，因此最终输出特征矩阵的深度为4。</p>
<p><strong>总结：</strong></p>
<ul>
<li>卷积核channel = 输入特征矩阵channel</li>
<li>输出特征矩阵channel = 卷积核个数</li>
</ul>
</blockquote>
<p><strong>DW卷积过程</strong></p>
<blockquote>
<p>输入一个深度为3的特征矩阵，经过3个卷积核进行卷积，且卷积核深度为1，也就是说一个卷积核对应输入特征矩阵的一个channel，再得到相应的输出特征矩阵的一个channel。既然一个卷积核负责输入特征矩阵的一个深度，那么卷积核的个数应该和输入特征矩阵的深度相同。又因为每一个卷积核与输入特征矩阵的一个channel进行卷积之后得到一个输出特征矩阵的channel，那么输出特征矩阵的深度与卷积核的个数相同，也和输入特征矩阵的深度相同。</p>
<p><strong>总结：</strong></p>
<ul>
<li>卷积核channel = 1</li>
<li>输入特征矩阵channel = 卷积核个数 = 输出特征矩阵channel</li>
</ul>
</blockquote>
<h3 id="Depthwise-Separable-Conv"><strong>Depthwise Separable Conv</strong></h3>
<blockquote>
<p>也叫做深度可分的卷积操作，由两部分组成，第一部分由DW卷积，第二部分为PW卷积。PW卷积实际上相当于普通卷积，只不过卷积核的大小为1。由下图所示，卷积层中卷积核的深度 = 1。</p>
<p>通常情况下DW卷积和PW卷积是一起使用的。</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/PW卷积.png" alt="PW卷积" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="DW-PW卷积和普通卷积的区别">DW&amp;PW卷积和普通卷积的区别</h3>
<p>下图（上左）为普通卷积操作，下图（下左）为DW卷积操作，下图（下右）为PW卷积操作，二者结合为深度可分卷积操作。</p>
<p>在普通卷积中，通过卷积操作之后，得到的是channel = 4的特征矩阵；在DW&amp;PW卷积操作之后，得到的同样是channel = 4的特征矩阵。</p>
<blockquote>
<p>参数代表含义：$D_F$：输入特征矩阵的宽高；$D_K$：卷积核的大小；M：输入特征矩阵的深度；N：输出特征矩阵的深度</p>
<p><strong>普通卷积计算量</strong>：卷积核的高x卷积核的宽x输入特征矩阵的深度x输出特征矩阵的深度x输入特征矩阵的高x输入特征矩阵的宽（默认stride = 1）</p>
<p><strong>DW&amp;PW卷积计算量</strong>：</p>
<ul>
<li>DW：卷积核的大小x输入特征矩阵的深度x输入特征矩阵的高宽</li>
<li>PW（相当于普通的卷积）：卷积核的高和宽（都是1）x输入特征矩阵的深度x输出特征矩阵的深度x输入特征矩阵的高宽（默认stride = 1）</li>
</ul>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/深度可分卷积和普通卷积的参数区别.png" alt="深度可分卷积和普通卷积的参数区别" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>理论上，普通卷积计算量是DW&amp;PW卷积的8~9倍</strong></p>
<h3 id="MobileNet-v1网络结构参数">MobileNet v1网络结构参数</h3>
<p>Filter Shape：3 x 3 x 3 x 32分别指的是卷积核的高宽、深度、个数</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv1网络参数.png" alt="MobileNetv1网络参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>超参数$\alpha$、$\beta$</strong></p>
<ul>
<li>$\alpha$：卷积核个数的倍率，用来控制卷积过程中所采用卷积核的个数</li>
<li>$\beta$：分辨率的参数，用来控制输入图像的大小</li>
</ul>
<p>大多数人发现，DW卷积在训练完之后的部分卷积核容易废掉，即卷积核参数大部分为0，也就是说DW卷积核没有起到作用。针对这个问题，在MobileNet v2版本中有一定的改善。</p>
<h2 id="MobileNet-v2">MobileNet v2</h2>
<p>MobileNet v2网络是由google团队在2018年提出的，相比MobileNet v1网络，准确率更高，模型更小。原论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.04381">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>
<p>网络中的亮点：</p>
<ul>
<li>Inverted Residuals（倒残差结构）</li>
<li>Linear Bottlenecks</li>
</ul>
<h3 id="Inverted-Residuals">Inverted Residuals</h3>
<p><strong>Residual block</strong></p>
<blockquote>
<p>对输入特征矩阵采用1x1卷积核进行降维处理，之后经过3x3的卷积核及逆行卷积处理，再通过1x1的卷积核进行升维，之后再输出特征矩阵。<strong>这就形成两边大中间小的瓶颈结构。</strong></p>
<p>注意：采用ReLU激活函数</p>
</blockquote>
<p><strong>Inverted residual block</strong></p>
<blockquote>
<p>对输入特征矩阵采用1x1卷积核进行升维处理，之后经过3x3的卷积核进行DW卷积，再通过1x1的卷积核进行降维处理，之后在输出特征矩阵。<strong>这就形成两边小中间大的橄榄球结构。</strong></p>
<p>注意：采用ReLU6激活函数</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inverted Residual结构.png" alt="Inverted Residual结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>ReLU6激活函数</strong></p>
<blockquote>
<p>在普通的ReLU激活函数中，当输入值&lt;0时，默认将输出置0；当输入值&gt;0时，即不进行处理。在ReLU6激活函数中，在输入值&lt;0，以及 [ 0, 6 ] 的区间时处理都一致，但在输入值&gt;6时，将会将输出值一直为6。</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/ReLU6激活函数.png" alt="ReLU6激活函数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); zoom: 50%;">
<h3 id="Linear-Bottlenecks">Linear Bottlenecks</h3>
<p>在论文中是针对Inverted residual结构的最后一个1x1的卷积层，使用线性激活函数而不是之前的ReLU激活函数。在下图中是作者在原文中描述为什么在最后一个1x1卷积层不使用ReLU激活函数原因的实验证明。</p>
<blockquote>
<p>首先输入的是一个2维矩阵，channel = 1；之后采用不同维度的matrix T 将输入进行变化，变化到更高的维度，再通过ReLU激活函数得到输出值。之后再使用T矩阵的逆矩阵$T^{-1}$，将输出的矩阵还原回2维的矩阵。当matrix T的维度是2，3的时候（即还原回去之后），所对应的即是图中的Output/dim = 2，Output/dim = 3</p>
<p>由图中可以看到，当被还原之后，图像中已经丢失了很多信息，随着matrix T维度不断的加深，丢失的信息就越来越少。因此<strong>ReLU激活函数对低维特征信息造成大量损失，而对于高维特征信息造成的损失很小</strong></p>
</blockquote>
<p>而在Inverted residual结构当中，是两边细、中间粗的结构，因此在输出时是一个低维的特征向量，需要一个线性的激活函数来替代ReLU激活函数，来避免特征信息的损失。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Linear Bottleneck.png" alt="Linear Bottleneck" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>Inverted residual网络结构图</strong></p>
<blockquote>
<p>Inverted residual网络结构如下图（左），首先通过1x1的卷积层进行卷积，通过ReLU6激活函数进行激活，之后通过DW卷积，卷积核大小为3x3，同样通过ReLU6激活函数激活，最后经过卷积核大小为1x1的卷积处理，使用线性激活函数。</p>
<p>过程信息为下图（右）表格，首先通过1x1卷积层进行升维处理（输出深度为tk，也就是说这里采用的卷积核的个数是tk个）；在第二层中，输入特征矩阵的深度为上一层的输出，即tk，这里采用卷积核大小为3x3的DW卷积，stride = s，由前文可知此处深度不变，依旧是tk，但高宽缩减s倍；最后一层通过1x1卷积核进行降维处理，卷积核个数为k‘。</p>
</blockquote>
<p>注意：**在MobileNet v2版本中，并不是每一个Inverted residual结构中都有shortcut。**在论文中，表述当stride = 1时，是有shortcut，当stride = 2时，是没有shortcut。但通过搭建网络结构之后，会发现论文此处表述有误，<strong>正确表述应该为：当stride = 1且输入特征矩阵与输出特征矩阵shape相同时才有shortcut连接。</strong></p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inverted residual网络结构图.png" alt="Inverted residual网络结构图" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="MobileNet-v2网络结构参数"><strong>MobileNet v2网络结构参数</strong></h3>
<p><strong>相关参数介绍：</strong></p>
<ul>
<li>t：扩展因子，对应着第一层1x1的卷积层所采用的卷积核个数的拓展倍率</li>
<li>c：时输出特征矩阵的深度channel，即是上图中提到的k’，控制着输出特征矩阵的深度</li>
<li>n：bottleneck的重复次数，此处的bottleneck指的是论文中的<strong>Inverted residual结构</strong></li>
<li>s：步距，<strong>仅仅针对每一个block所对应的第一层bottleneck的步距（一个block由一系列bottleneck组成），其他都为1</strong>。假设当n = 2时，即bottleneck需要重复2遍，对于这个结构而言，第一层的步距是2，第二层s为1。</li>
</ul>
<p>注意：在第一个bottleneck中，t = 1，即该处的扩展因子为1，也就是说在第一层卷积层是没有对特征矩阵的深度做调整的。<strong>因此在搭建网络时，因为这一层卷积层没有起到升维或者降维作用，所以会被跳过，直接进入3x3的卷积层中处理。</strong></p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet v2网络结构参数.png" alt="Mobilenet v2网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>如何理解有无shortcut？</strong></p>
<blockquote>
<p>以输入为$14^2$x64这一行的block来举例，这一行的block采用了3个bottleneck结构，stride = 1，即此处block的第一层卷积层的stride = 1，如果按照官方标注而言，应该需要由shortcut。</p>
<p>但实际上这里不可能有shortcut，因此此处的输入特征矩阵的深度为64，但是输出特征矩阵的深度是96。也就是说，如果有shortcut的话，那么输出特征矩阵的深度应该是64，但从表中可知经过一系列操作后输出的特征矩阵深度是96，所以两个特征矩阵的深度是不相同的，无法相加。</p>
<p>在第二层的时候，s = 1，这里的输入特征矩阵深度是上一层的输出特征矩阵深度，即96，对于第二层bottleneck而言，输入的特征矩阵深度十96，其输出特征矩阵的深度也是96，有因为stride = 1，特征矩阵的高宽不会发生变化，因此满足输入特征矩阵和输出特征矩阵的shape保持一致的条件，此时才能使用shortcut。</p>
<p>因此，<strong>只有当stride = 1且输入特征矩阵与输出特征矩阵shape相同时才有shortcut连接。</strong></p>
</blockquote>
<p><strong>conv2d 1x1</strong></p>
<p>网络结构最后一层是卷积层，输入特征矩阵为1x1x1280，实际上这一层就是一个全连接层，功能上二者一模一样。k表示的是分类的类别个数，若针对ImageNet而言，这里的k = 1000。</p>
<h3 id="性能对比">性能对比</h3>
<p>表头对应为：准确率、模型参数、运算量、运算时间。</p>
<ul>
<li>下图（上）是针对在分类任务中的性能对比，MobileNetV2（1.4）指的是$\alpha$ = 1.4（倍率因子）；</li>
<li>下图（下）是针对在目标检测中的效果对比，将MNet V2与SSDLite联合使用，将MNet v2作为前置网络，SSDLite将其中的一些卷积层换为深度可分卷积，也就是DW卷积+PW卷积。</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/性能对比.png" alt="性能对比" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h2 id="MobileNet-v3">MobileNet v3</h2>
<p>原论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.02244">Searching for MobileNetV3</a></p>
<p>论文中的亮点：</p>
<ul>
<li>更新Block（bneck）（即对Inverted residual结构基础上，进行简单改动）</li>
<li>使用NAS搜索参数（Neural Architecture Search）</li>
<li>重新设计耗时层结构（NAS搜索之后得到的网络，之后对网络的每一层的推理时间进行分析，针对某些耗时的层结构做进一步的优化）</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv3L对比v3S对比v2.png" alt="MobileNetv3L对比v3S对比v2" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>性能对比</strong></p>
<p>在原论文的摘要中，作者提到，v3版本的Large对比v2而言在图像分类任务当中准确率上升有3.2%，延迟有降低20%。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv3性能对比.png" alt="MobileNetv3性能对比" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="更新Block">更新Block</h3>
<p>在v3当中，做出了Block的更新：</p>
<ul>
<li>加入SE模块，即通道的注意力机制模块</li>
<li>更新了激活函数</li>
</ul>
<p><strong>Mobilenet V2：bottleneck with residual</strong></p>
<blockquote>
<p>在v2当中，会首先通过1x1的卷积核对输入特征矩阵进行升维处理，之后通过BN，ReLU6激活函数，之后是通过3x3的DW卷积，之后进行BN，ReLU激活函数激活，最后通过1x1卷积层进行降维处理，之后只跟了一个BN结构，并没有像前面几层一样还有一个ReLU6激活函数。最后有一个shortcut相加处理。</p>
<p>注意：<strong>当stride = 1且input_c = output_c才有shortcut连接</strong></p>
</blockquote>
<p><strong>Mobilenet V3 block</strong></p>
<p><strong>加入SE模块</strong></p>
<blockquote>
<p>大部分框架与v2保持一致，但是在第二层卷积层中加入了SE注意力机制。也就是说针对得到的特征矩阵，对每个channel进行池化处理，那么特征矩阵的channel是多少，得到的1维向量就有多少元素，之后再通过两个全连接层得到输出向量。</p>
<p>注意：</p>
<ul>
<li><strong>对于第一个全连接层，此处的节点个数 = 该处卷积层特征矩阵channel的1/4</strong>（在v3原论文中作者有给出）</li>
<li>第二层全连接层的节点个数 = 该处卷积层特征矩阵channel</li>
<li>对于输出的向量，可以理解为是对该层卷积层特征矩阵的每一个channel分析出了一个权重关系，觉得比较重要的channel会赋予更大的权重，觉得不是那么重要的channel的维度上就对应较小的权重。</li>
</ul>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3更新Block结构.png" alt="更新Block结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>理解两处的全连接层</strong></p>
<blockquote>
<p>假设此处特征矩阵的channel = 2，首先通过平均池化下采样层操作对每一个channel取一个均值，由于有两个channel，所以得到一个元素个数为2的向量。之后再依次通过两个全连接层，得到输出。</p>
<p>针对FC1，节点个数= 输入特征矩阵channel的1/4，实际情况中channel会很多，不会出现小于4的情况。之后跟着一个ReLU激活函数。对于FC2而言，节点个数 =  输入特征矩阵channel = 2，之后跟着H-sig激活函数（之后讲解）。</p>
<p>之后会得到有2个元素的向量，每一个元素对应的就是每一个channel所对应的权重。比如预测第一个channel的权重 = 0.5，那么就将0.5与第一个channel的当中的所有元素相乘得到一个新的channel数据。</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3两处全连接层.png" alt="两处全连接层理解" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>更新激活函数</strong></p>
<p>在v3的Block中所使用的激活函数标注的都是NL激活函数，即非线性激活函数的意思。因为在不同的层之间所使用的激活函数都不一样，因此在下图中并未给出明确的激活函数名称。</p>
<p>注意：在最后一个标注的1x1降维的卷积层中，是没有使用激活函数的，也可以说使用了一个线性激活函数y = x。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3更新Block结构.png" alt="更新Block结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="重新设计耗时层结构">重新设计耗时层结构</h3>
<ul>
<li>
<p>减少第一个卷积层的卷积核个数（32-&gt;16）</p>
<blockquote>
<p>​		在原论文当中，作者提到当卷积核个数变化了之后，准确率是没有变化的，那么就达成了在维持准确率不降的前提下减少计算量的目的，这里大概减少2ms的时间</p>
</blockquote>
</li>
<li>
<p>精简Last Stage</p>
<blockquote>
<p>​		在通过NAS搜索出来的网络结构的最后一个部分叫做Original Last Stage，作者在原论文中发现，这一个流程是比较耗时的，因此作者针对这个结构做出精简化，于是提出了Efficient Last Stage。</p>
<p>​		在精简之后发现，第一个卷积层是没有发生变化的，紧接着直接进行平均池化操作，然后跟着两个卷积核输出。和下图（上）的结构对比而言，减少了许多层结构。调整之后，作者发现准确率无多少变化，但执行时间上减少了7ms，这7ms占据整个推理时间的11%。</p>
</blockquote>
</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/重新设计耗时层结构.png" alt="重新设计耗时层结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="重新设计激活函数">重新设计激活函数</h3>
<p>在v2网络结构中，基本使用的激活函数都是ReLU6，目前来说比较常用的激活函数叫做swish激活函数，也就是输入值*sigmoid函数。使用swish激活函数确实会提高网络的准确率，但是在计算、求导上非常复杂，对量化过程也不友好（将模型部署到硬件设备上）。</p>
<p>于是作者提出h-swish激活函数，也就是输入值*h-sigmoid函数，其中h-sigmoid = ReLU6（x+3）/6，也就是在ReLU函数的输入值基础上+3的值除以6。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/重新设计激活函数.png" alt="重新设计激活函数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="V3-Large网络结构参数">V3-Large网络结构参数</h3>
<ul>
<li><code>input</code>：输入特征矩阵的长宽深；</li>
<li><code>Operator</code>：对应操作，例如第一层进行conv2d操作；bneck指进行的block操作，其中3x3指的是下图（左）中的DW卷积中卷积核的大小</li>
<li><code>exp size</code>：代表第一个升维的1x1卷积需要将维度升到多少</li>
<li><code>#out</code>：输出特征矩阵的channel；</li>
<li><code>SE</code>：是否使用注意力机制</li>
<li><code>NL</code>：使用的激活函数，HS指H-swish激活函数，RE指ReLU6激活函数</li>
<li><code>s</code>：表示DW卷积中的步距stride</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet v3网络结构参数.png" alt="Mobilenet v3-Large网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p>最后两层有写到NBN结构，指的是不去使用，和全连接层的作用是差不多的，这里直接使用卷积结构。</p>
<p>注意：在第一个bneck结构的第一层卷积层之中，因为i输入的特征矩阵channel = 16且输出的特征矩阵channel = 16，因此没有进行升维和降维，且没有SE结构，直接进行1x1的卷积层输出就没有了，因此在搭建网络中，会省略这一步。</p>
<h3 id="V3-Small网络结构参数">V3-Small网络结构参数</h3>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet v3-Small网络结构参数.png" alt="Mobilenet v3-Small网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
              <a href="/tags/CNN%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" rel="tag"><i class="fa fa-tag"></i> CNN网络详解</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/05/21/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNeXt%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/" rel="prev" title="深度学习模型之CNN（十二）使用pytorch搭建ResNeXt并基于迁移学习训练">
                  <i class="fa fa-chevron-left"></i> 深度学习模型之CNN（十二）使用pytorch搭建ResNeXt并基于迁移学习训练
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/" rel="next" title="深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练">
                  深度学习模型之CNN（十四）使用pytorch搭建MobileNetV2V3并基于迁移学习训练 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="valine-comments"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Linvil Yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("04/21/2023 22:22:22");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "已运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","app_key":"iNfyeaWVmA11Duj7fzr0B1r1","server_url":"https://16p3s6fl.lc-cn-n1-shared.com","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '32px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>


<script class="next-config" data-name="valine" type="application/json">{"enable":true,"appId":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","appKey":"iNfyeaWVmA11Duj7fzr0B1r1","serverURLs":"https://16p3s6fl.lc-cn-n1-shared.com","placeholder":"请写下您的评论","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"lang":null,"visitor":false,"comment_count":true,"recordIP":true,"enableQQ":true,"requiredFields":[],"el":"#valine-comments","path":"/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.valine.el)
    .then(() => NexT.utils.getScript(
      'https://fastly.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
      { condition: window.Valine }
    ))
    .then(() => {
      new Valine(CONFIG.valine);
    });
});
</script>

</body>
</html>
