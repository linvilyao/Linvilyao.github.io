<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logosc/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logosc/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"linvilyao.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":14,"offset":10},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础9.2》，作为随堂笔记，使用pytorch搭建EfficientNet网络。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络">
<meta property="og:url" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Linvil&#39;s Blog">
<meta property="og:description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础9.2》，作为随堂笔记，使用pytorch搭建EfficientNet网络。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/EfficientNet-B0%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/MBConv%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/SE%E6%A8%A1%E5%9D%97.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png">
<meta property="article:published_time" content="2023-05-27T13:05:42.000Z">
<meta property="article:modified_time" content="2023-05-28T15:28:06.633Z">
<meta property="article:author" content="Linvil Yao">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="Pytorch搭建CNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/EfficientNet-B0%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png">


<link rel="canonical" href="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/","path":"2023/05/27/使用Pytorch搭建EfficientNet网络/","title":"深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络 | Linvil's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linvil's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">这是一个用来记录的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">5</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">30</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#EfficientNet-B0%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text">EfficientNet-B0网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MBConv%E7%BB%93%E6%9E%84"><span class="nav-number">1.2.</span> <span class="nav-text">MBConv结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SE%E6%A8%A1%E5%9D%97"><span class="nav-number">1.3.</span> <span class="nav-text">SE模块</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">工程目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">搭建网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6"><span class="nav-number">3.1.</span> <span class="nav-text">代码框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#make-divisible%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">_make_divisible函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ConvBNActivation%E7%B1%BB"><span class="nav-number">3.3.</span> <span class="nav-text">ConvBNActivation类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SE%E6%A8%A1%E5%9D%97-2"><span class="nav-number">3.4.</span> <span class="nav-text">SE模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InvertedResidualConfig%E7%B1%BB"><span class="nav-number">3.5.</span> <span class="nav-text">InvertedResidualConfig类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InvertedResidual%E7%B1%BB%EF%BC%88MBConv%E6%A8%A1%E5%9D%97%EF%BC%89"><span class="nav-number">3.6.</span> <span class="nav-text">InvertedResidual类（MBConv模块）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EfficientNet%E7%B1%BB"><span class="nav-number">3.7.</span> <span class="nav-text">EfficientNet类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%E5%8C%96EfficientNet-B0-B7"><span class="nav-number">3.8.</span> <span class="nav-text">实例化EfficientNet-B0~B7</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">训练结果</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">预测结果</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Linvil Yao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Linvil Yao</p>
  <div class="site-description" itemprop="description">Welcome to Linvil's Blog!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


        </div>
      </div>


  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/29/EfficientNetV2%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" title="2023&#x2F;05&#x2F;29&#x2F;EfficientNetV2网络详解&#x2F;">EfficientNetV2网络详解</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/" title="2023&#x2F;05&#x2F;27&#x2F;使用Pytorch搭建EfficientNet网络&#x2F;">深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/26/EfficientNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" title="2023&#x2F;05&#x2F;26&#x2F;EfficientNet网络详解&#x2F;">深度学习模型之CNN（十七）EfficientNet网络详解</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/25/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAShuffleNetv2/" title="2023&#x2F;05&#x2F;25&#x2F;使用Pytorch搭建ShuffleNetv2&#x2F;">深度学习模型之CNN（十六）使用Pytorch搭建ShuffleNetv2</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/24/ShuffleNetv1v2%E7%90%86%E8%AE%BA%E8%AE%B2%E8%A7%A3/" title="2023&#x2F;05&#x2F;24&#x2F;ShuffleNetv1v2理论讲解&#x2F;">深度学习模型之CNN（十五）ShuffleNetv1v2理论讲解</a>
        </li>
    </ul>
  </div>
    </div>


    



  </aside>






    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://linvilyao.github.io/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Linvil Yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linvil's Blog">
      <meta itemprop="description" content="Welcome to Linvil's Blog!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络 | Linvil's Blog">
      <meta itemprop="description" content="本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础9.2》，作为随堂笔记，使用pytorch搭建EfficientNet网络。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-05-27 21:05:42" itemprop="dateCreated datePublished" datetime="2023-05-27T21:05:42+08:00">2023-05-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-28 23:28:06" itemprop="dateModified" datetime="2023-05-28T23:28:06+08:00">2023-05-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span id="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习模型之CNN（十八）使用Pytorch搭建EfficientNet网络" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Valine：</span>
  
    <a title="valine" href="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>22k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

            <div class="post-description">本笔记跟随字母站UP主霹雳吧啦Wz《卷积神经网络基础9.2》，作为随堂笔记，使用pytorch搭建EfficientNet网络。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1>回顾</h1>
<h2 id="EfficientNet-B0网络结构">EfficientNet-B0网络结构</h2>
<p>EfficientNet网络结构基本上分为9个Stage，第一个Stage是一个卷积核大小为3x3步距为2的普通卷积层（包含BN和激活函数Swish）；第二至第八的Stage 都是使用MBConv，也就是在MobileNetv3所使用到的InvertedResidualBlock结构。</p>
<p>每个MBConv后会跟一个数字1或6，这里的1或6就是倍率因子n即MBConv中第一个1x1的卷积层会将输入特征矩阵的channels扩充为n倍，其中k3x3或k5x5表示MBConv中Depthwise Conv所采用的卷积核大小。</p>
<img src="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/EfficientNet-B0网络结构.png" alt="EfficientNet-B0网络结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224);  ">
<h2 id="MBConv结构">MBConv结构</h2>
<p>MBConv结构主要由一个1x1的普通卷积（升维作用，包含BN和Swish），一个kxk的Depthwise Conv卷积（包含BN和Swish）<strong>k的具体值可看EfficientNet-B0的网络框架主要有3x3和5x5两种情况</strong>，一个SE模块，一个1x1的普通卷积（降维作用，包含BN），一个Droupout层构成。</p>
<img src="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/MBConv结构.png" alt="MBConv结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224);  ">
<h2 id="SE模块">SE模块</h2>
<img src="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/SE模块.png" alt="SE模块" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224);  ">
<h1>工程目录</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── Test9_efficientnet</span><br><span class="line">	├── model.py（模型文件）  </span><br><span class="line">	├── my_dataset.py（数据处理文件）  </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,EfficientNet.pth文件）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── utils.py（工具文件，用得上就对了）</span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	└── efficientnet-b0.pth（用于迁移学习时，提前下载好官方的efficientnet-b0权重脚本）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<h1>搭建网络结构</h1>
<h2 id="代码框架">代码框架</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Callable</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel_size: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeExcitation</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 input_c: <span class="built_in">int</span>,   <span class="comment"># block input channel</span></span></span><br><span class="line"><span class="params">                 expand_c: <span class="built_in">int</span>,  <span class="comment"># block expand channel</span></span></span><br><span class="line"><span class="params">                 squeeze_factor: <span class="built_in">int</span> = <span class="number">4</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualConfig</span>:</span><br><span class="line">    <span class="comment"># kernel_size, in_channel, out_channel, exp_ratio, strides, use_SE, drop_connect_rate</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 kernel: <span class="built_in">int</span>,          <span class="comment"># 3 or 5</span></span></span><br><span class="line"><span class="params">                 input_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 expanded_ratio: <span class="built_in">int</span>,  <span class="comment"># 1 or 6</span></span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span>,          <span class="comment"># 1 or 2</span></span></span><br><span class="line"><span class="params">                 use_se: <span class="built_in">bool</span>,         <span class="comment"># True</span></span></span><br><span class="line"><span class="params">                 drop_rate: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">                 index: <span class="built_in">str</span>,           <span class="comment"># 1a, 2a, 2b, ...</span></span></span><br><span class="line"><span class="params">                 width_coefficient: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjust_channels</span>(<span class="params">channels: <span class="built_in">int</span>, width_coefficient: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 cnf: InvertedResidualConfig,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Callable</span>[..., nn.Module]</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EfficientNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 width_coefficient: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">                 depth_coefficient: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">                 num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate: <span class="built_in">float</span> = <span class="number">0.2</span>,</span></span><br><span class="line"><span class="params">                 drop_connect_rate: <span class="built_in">float</span> = <span class="number">0.2</span>,</span></span><br><span class="line"><span class="params">                 block: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">round_repeats</span>(<span class="params">repeats</span>):</span><br><span class="line">            <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b0</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b1</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b2</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b3</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b4</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b5</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b6</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b7</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h2 id="make-divisible函数">_make_divisible函数</h2>
<p><strong>是为了将卷积核个数（输出的通道个数ch）调整为输入divisor参数的整数倍</strong>。搭建中采用divisor=8，也就是要将卷积核的个数设置为8的整数倍。</p>
<p>目的：为了更好的调用硬件设备，比如多GPU变形运算，或者多机器分布式运算</p>
<ul>
<li><code>ch</code>：传入的卷积核个数（输出特征矩阵的channel）</li>
<li><code>divisor</code>：传入round_nearest基数，即将卷积核个数ch调整为divisor的整数倍</li>
<li><code>min_ch</code>：最小通道数，如果为None，就将min_ch设置为divisor</li>
<li><code>new_ch</code>：即将卷积核个数调整为离它最近的8的倍数的值</li>
<li>之后进行判断new_ch是否小于传入ch的0.9倍，如果小于，则加上一个divisor（为了确保new_ch向下取整的时候，不会减少超过10%）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function is taken from the original tf repo.</span></span><br><span class="line"><span class="string">    It ensures that all layers have a channel number that is divisible by 8</span></span><br><span class="line"><span class="string">    It can be seen here:</span></span><br><span class="line"><span class="string">    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> min_ch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        min_ch = divisor</span><br><span class="line">    new_ch = <span class="built_in">max</span>(min_ch, <span class="built_in">int</span>(ch + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_ch &lt; <span class="number">0.9</span> * ch:</span><br><span class="line">        new_ch += divisor</span><br><span class="line">    <span class="keyword">return</span> new_ch</span><br></pre></td></tr></table></figure>
<h2 id="ConvBNActivation类">ConvBNActivation类</h2>
<p>在MBConv结构中，基本上的组成都是<strong>卷积+BN+Swish激活函数</strong>（尽管第二次1x1卷积之后没有激活函数）</p>
<ul>
<li><code>group</code>：用来分辨是使用普通卷积还是dw卷积</li>
<li><code>norm_layer</code>：在EfficientNet中相当于BN结构</li>
<li><code>activation_layer</code>：BN结构之后的激活函数，当传入为nn.identity，指不做任何处理的方法</li>
<li><code>nn.SiLU</code>：实际上和Swish函数一样（只有官方torch版本≥1.7的时候，才有该激活函数）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel_size: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> activation_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            activation_layer = nn.SiLU  <span class="comment"># alias Swish  (torch&gt;=1.7)</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(ConvBNActivation, self).__init__(nn.Conv2d(in_channels=in_planes,</span><br><span class="line">                                                         out_channels=out_planes,</span><br><span class="line">                                                         kernel_size=kernel_size,</span><br><span class="line">                                                         stride=stride,</span><br><span class="line">                                                         padding=padding,</span><br><span class="line">                                                         groups=groups,</span><br><span class="line">                                                         bias=<span class="literal">False</span>),</span><br><span class="line">                                               norm_layer(out_planes),</span><br><span class="line">                                               activation_layer())</span><br></pre></td></tr></table></figure>
<h2 id="SE模块-2">SE模块</h2>
<ul>
<li><code>input_c</code>：对应的是MBConv模块输入特征矩阵的channel；</li>
<li><code>expand_c</code>：对应的是MBConv模块中第一个1x1的卷积层升维之后所输出的特征矩阵channel</li>
</ul>
<blockquote>
<p>由于MBConv模块中的dw卷积是不会对特征矩阵channel做变化的，因此dw卷积之后的特征矩阵channel与 = 第一层1x1卷积层之后的特征矩阵channel = expand_c</p>
</blockquote>
<ul>
<li><code>squeeze_factor</code>：第一个全连接层的结点个数 = input_c // squeeze_factor（第一个全连接层<strong>原理上特别注意</strong>：是input_c除以4）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeExcitation</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 input_c: <span class="built_in">int</span>,   <span class="comment"># block input channel</span></span></span><br><span class="line"><span class="params">                 expand_c: <span class="built_in">int</span>,  <span class="comment"># block expand channel</span></span></span><br><span class="line"><span class="params">                 squeeze_factor: <span class="built_in">int</span> = <span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SqueezeExcitation, self).__init__()</span><br><span class="line">        squeeze_c = input_c // squeeze_factor</span><br><span class="line">        <span class="comment"># 此处的Conv2d理论上和使用全连接层效果上是一样的</span></span><br><span class="line">        self.fc1 = nn.Conv2d(expand_c, squeeze_c, <span class="number">1</span>)</span><br><span class="line">        self.ac1 = nn.SiLU()  <span class="comment"># 效果同Swish，只是名字不一样</span></span><br><span class="line">        self.fc2 = nn.Conv2d(squeeze_c, expand_c, <span class="number">1</span>)</span><br><span class="line">        self.ac2 = nn.Sigmoid()</span><br><span class="line">	<span class="comment"># -&gt; Tensor表示最后返回值的类型</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        scale = F.adaptive_avg_pool2d(x, output_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        scale = self.fc1(scale)</span><br><span class="line">        scale = self.ac1(scale)</span><br><span class="line">        scale = self.fc2(scale)</span><br><span class="line">        scale = self.ac2(scale)</span><br><span class="line">        <span class="keyword">return</span> scale * x</span><br></pre></td></tr></table></figure>
<h2 id="InvertedResidualConfig类">InvertedResidualConfig类</h2>
<p>类似于MobileNetv3中的InvertedResidualConfig，在EfficientNet中，对应的是每一个MBConv模块中的配置参数。</p>
<ul>
<li>
<p><code>kernel</code>：每一层MBConv模块使用的kernel_size（即DW卷积中的卷积核大小，3x3 or 5x5）；</p>
</li>
<li>
<p><code>input_c</code>：输入MBConv模块的特征矩阵channel；</p>
</li>
<li>
<p><code>out_c</code>：MBConv模块输出特征矩阵的channel；</p>
</li>
<li>
<p><code>expanded_ratio</code>：对应MBConv模块中1x1卷积层，用来调节每一个卷积层所使用channel的倍率因子；</p>
</li>
<li>
<p><code>stride</code>：指的是DW卷积所对应的步距；</p>
</li>
<li>
<p><code>use_se</code>：是否使用SE注意力机制；</p>
</li>
<li>
<p><code>drop_rate</code>：随机失活比例；</p>
</li>
<li>
<p><code>index</code>：记录当前MBConv模块的名称，用来方便后期分析；</p>
</li>
<li>
<p><code>width_coefficient</code>：关于网络宽度width上的倍率因子（实际上指特征矩阵的channel）。</p>
<p><code>@staticmethod</code>：静态方法（可以不实例化类就直接调用，主要是类显得不重要的时候用）</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualConfig</span>:</span><br><span class="line">    <span class="comment"># kernel_size, in_channel, out_channel, exp_ratio, strides, use_SE, drop_connect_rate</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 kernel: <span class="built_in">int</span>,          <span class="comment"># 3 or 5</span></span></span><br><span class="line"><span class="params">                 input_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 expanded_ratio: <span class="built_in">int</span>,  <span class="comment"># 1 or 6</span></span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span>,          <span class="comment"># 1 or 2</span></span></span><br><span class="line"><span class="params">                 use_se: <span class="built_in">bool</span>,         <span class="comment"># True</span></span></span><br><span class="line"><span class="params">                 drop_rate: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">                 index: <span class="built_in">str</span>,           <span class="comment"># 1a, 2a, 2b, ...</span></span></span><br><span class="line"><span class="params">                 width_coefficient: <span class="built_in">float</span></span>):</span><br><span class="line">        self.input_c = self.adjust_channels(input_c, width_coefficient)</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.expanded_c = self.input_c * expanded_ratio</span><br><span class="line">        self.out_c = self.adjust_channels(out_c, width_coefficient)</span><br><span class="line">        self.use_se = use_se</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.index = index</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjust_channels</span>(<span class="params">channels: <span class="built_in">int</span>, width_coefficient: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="keyword">return</span> _make_divisible(channels * width_coefficient, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h2 id="InvertedResidual类（MBConv模块）">InvertedResidual类（MBConv模块）</h2>
<p>同MobileNetv3一致（从pytorch搭建MobileNetv3复制过来的）</p>
<ul>
<li><code>cnf</code>：前文提到的InvertedResidualConfig配置文件；</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层</li>
<li><code>cnf.stride</code>：判断步距是否为1或2，因为在网络参数表中，步距只有1和2两种情况，当出现第三种情况时，就是不合法的步距情况；再判断</li>
<li><code>self.use_res_connect</code>：是否使用shortcut连接，shortcut只有在stride == 1且input_c == output_c时才有；</li>
<li><code>activation_layer</code>：判断使用ReLU或者H-Swish激活函数（官方是在1.7及以上版本中才有官方实现的H-Swish和H-Sigmoid激活函数，如果需要使用MNv3网络的话，得把pytorch版本更新至1.7及以上）</li>
<li>expand区域指在InvertedResidual结构中的第一个1x1卷积层进行升维处理，因为第一个block存在输入特征矩阵的channel和输出特征矩阵的channel相等，因此可以跳过，所以会进行判断cnf.expanded_c != cnf.input_c；</li>
<li>depthwise区域为dw卷积区域</li>
</ul>
<blockquote>
<p><code>groups</code>：由于DW卷积是针对每一个channel都单独使用一个channel为1的卷积核来进行卷及处理，所以groups和channel的个数是保持一致的，所以groups=cnf.expanded_c</p>
</blockquote>
<ul>
<li>project区域是InvertedResidual结构中1x1卷积中的降维部分，activation_layer=nn.Identity中的Identity其实就是线性y = x，没有做任何处理；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 cnf: InvertedResidualConfig,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Callable</span>[..., nn.Module]</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cnf.stride <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;illegal stride value.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.use_res_connect = (cnf.stride == <span class="number">1</span> <span class="keyword">and</span> cnf.input_c == cnf.out_c)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义有序字典layers</span></span><br><span class="line">        layers = OrderedDict()</span><br><span class="line">        activation_layer = nn.SiLU  <span class="comment"># alias Swish</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand</span></span><br><span class="line">        <span class="keyword">if</span> cnf.expanded_c != cnf.input_c:</span><br><span class="line">            layers.update(&#123;<span class="string">&quot;expand_conv&quot;</span>: ConvBNActivation(cnf.input_c,</span><br><span class="line">                                                           cnf.expanded_c,</span><br><span class="line">                                                           kernel_size=<span class="number">1</span>,</span><br><span class="line">                                                           norm_layer=norm_layer,</span><br><span class="line">                                                           activation_layer=activation_layer)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># depthwise</span></span><br><span class="line">        layers.update(&#123;<span class="string">&quot;dwconv&quot;</span>: ConvBNActivation(cnf.expanded_c,</span><br><span class="line">                                                  cnf.expanded_c,</span><br><span class="line">                                                  kernel_size=cnf.kernel,</span><br><span class="line">                                                  stride=cnf.stride,</span><br><span class="line">                                                  groups=cnf.expanded_c,</span><br><span class="line">                                                  norm_layer=norm_layer,</span><br><span class="line">                                                  activation_layer=activation_layer)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cnf.use_se:</span><br><span class="line">            layers.update(&#123;<span class="string">&quot;se&quot;</span>: SqueezeExcitation(cnf.input_c,</span><br><span class="line">                                                   cnf.expanded_c)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project</span></span><br><span class="line">        layers.update(&#123;<span class="string">&quot;project_conv&quot;</span>: ConvBNActivation(cnf.expanded_c,</span><br><span class="line">                                                        cnf.out_c,</span><br><span class="line">                                                        kernel_size=<span class="number">1</span>,</span><br><span class="line">                                                        norm_layer=norm_layer,</span><br><span class="line">                                                        activation_layer=nn.Identity)&#125;)</span><br><span class="line"></span><br><span class="line">        self.block = nn.Sequential(layers)</span><br><span class="line">        self.out_channels = cnf.out_c</span><br><span class="line">        self.is_strided = cnf.stride &gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只有在使用shortcut且drop_rate大于0时才使用dropout层</span></span><br><span class="line">        <span class="keyword">if</span> self.use_res_connect <span class="keyword">and</span> cnf.drop_rate &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = DropPath(cnf.drop_rate)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        result = self.block(x)</span><br><span class="line">        result = self.dropout(result)</span><br><span class="line">        <span class="keyword">if</span> self.use_res_connect:</span><br><span class="line">            result += x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h2 id="EfficientNet类">EfficientNet类</h2>
<ul>
<li><code>width coeficient</code>：代表channel维度上的倍率因子，比如在 EfcientNetB0中Stagel的3x3卷积层所使用的卷积核个数是32，那么在B6中就是 32 X 18=57.6接着取整到离它最近的8的整数倍即56，其它Stage同理。</li>
<li><code>depth coeficient</code>：代表depth维度上的倍率因子（仅针对Stage2到Stage8），比如在EcientNetB0中Stage7的L=4，那么在B6中就是 4 X 2.6=10.4，接着向上取整即11。</li>
<li><code>drop_connect_rate</code>：对应MBConv模块Dropout层的随机失活比例（并不是所有层的Dropout都是0.2，而是渐渐从0增长至0.2）；</li>
<li><code>dropout_rate</code>：MBConv模块中最后一个全连接层前面的Dropout层的随机失活比例，对应EfficientNet网络中Stage9当中<code>FC</code>全连接层前面的一个Dropout层的随机失活比例。</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">input_size</th>
<th style="text-align:center">width_coefficient</th>
<th style="text-align:center">depth_coefficient</th>
<th style="text-align:center">drop_connect_rate</th>
<th style="text-align:center">dropout_rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">EfficientNetB0</td>
<td style="text-align:center">224x224</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.2</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB1</td>
<td style="text-align:center">240x240</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">1.1</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.2</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB2</td>
<td style="text-align:center">260x260</td>
<td style="text-align:center">1.1</td>
<td style="text-align:center">1.2</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.3</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB3</td>
<td style="text-align:center">300x300</td>
<td style="text-align:center">1.2</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.3</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB4</td>
<td style="text-align:center">380x380</td>
<td style="text-align:center">1.4</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB5</td>
<td style="text-align:center">456x456</td>
<td style="text-align:center">1.6</td>
<td style="text-align:center">2.2</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB6</td>
<td style="text-align:center">528x528</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">2.6</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.5</td>
</tr>
<tr>
<td style="text-align:center">EfficientNetB7</td>
<td style="text-align:center">600x600</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.1</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.5</td>
</tr>
</tbody>
</table>
<p><code>default_cnf</code>：存储网络中Stage2~Stage8之间的默认配置文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EfficientNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 width_coefficient: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">                 depth_coefficient: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">                 num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate: <span class="built_in">float</span> = <span class="number">0.2</span>,</span></span><br><span class="line"><span class="params">                 drop_connect_rate: <span class="built_in">float</span> = <span class="number">0.2</span>,</span></span><br><span class="line"><span class="params">                 block: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>(EfficientNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># kernel_size, in_channel, out_channel, exp_ratio, strides, use_SE, drop_connect_rate, repeats</span></span><br><span class="line">        default_cnf = [[<span class="number">3</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">16</span>, <span class="number">24</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">5</span>, <span class="number">24</span>, <span class="number">40</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">40</span>, <span class="number">80</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">5</span>, <span class="number">80</span>, <span class="number">112</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">5</span>, <span class="number">112</span>, <span class="number">192</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">4</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">192</span>, <span class="number">320</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="literal">True</span>, drop_connect_rate, <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">round_repeats</span>(<span class="params">repeats</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;Round number of repeats based on depth multiplier.&quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">int</span>(math.ceil(depth_coefficient * repeats))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            block = InvertedResidual</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = partial(nn.BatchNorm2d, eps=<span class="number">1e-3</span>, momentum=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">        adjust_channels = partial(InvertedResidualConfig.adjust_channels,</span><br><span class="line">                                  width_coefficient=width_coefficient)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build inverted_residual_setting</span></span><br><span class="line">        bneck_conf = partial(InvertedResidualConfig,</span><br><span class="line">                             width_coefficient=width_coefficient)</span><br><span class="line"></span><br><span class="line">        b = <span class="number">0</span></span><br><span class="line">        num_blocks = <span class="built_in">float</span>(<span class="built_in">sum</span>(round_repeats(i[-<span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> default_cnf))</span><br><span class="line">        inverted_residual_setting = []</span><br><span class="line">        <span class="comment"># 遍历每一个Stage</span></span><br><span class="line">        <span class="keyword">for</span> stage, args <span class="keyword">in</span> <span class="built_in">enumerate</span>(default_cnf):</span><br><span class="line">            cnf = copy.copy(args)</span><br><span class="line">            <span class="comment"># 遍历每一个Stage中的MBConv模块</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(round_repeats(cnf.pop(-<span class="number">1</span>))):</span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># strides equal 1 except first cnf</span></span><br><span class="line">                    cnf[-<span class="number">3</span>] = <span class="number">1</span>  <span class="comment"># strides</span></span><br><span class="line">                    cnf[<span class="number">1</span>] = cnf[<span class="number">2</span>]  <span class="comment"># input_channel equal output_channel</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 最后的以及被pop出去，现在cnf[-1]指的是drop_connect_rate</span></span><br><span class="line">                cnf[-<span class="number">1</span>] = args[-<span class="number">2</span>] * b / num_blocks  <span class="comment"># update dropout ratio</span></span><br><span class="line">                <span class="comment"># 通过该方法能记录当前MBConv结构是属于第几个Stage中的第几个MBConv结构</span></span><br><span class="line">                index = <span class="built_in">str</span>(stage + <span class="number">1</span>) + <span class="built_in">chr</span>(i + <span class="number">97</span>)  <span class="comment"># 1a, 2a, 2b, ...</span></span><br><span class="line">                inverted_residual_setting.append(bneck_conf(*cnf, index))</span><br><span class="line">                b += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create layers</span></span><br><span class="line">        layers = OrderedDict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># first conv </span></span><br><span class="line">        <span class="comment"># Stage1</span></span><br><span class="line">        layers.update(&#123;<span class="string">&quot;stem_conv&quot;</span>: ConvBNActivation(in_planes=<span class="number">3</span>,</span><br><span class="line">                                                     out_planes=adjust_channels(<span class="number">32</span>),</span><br><span class="line">                                                     kernel_size=<span class="number">3</span>,</span><br><span class="line">                                                     stride=<span class="number">2</span>,</span><br><span class="line">                                                     norm_layer=norm_layer)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building inverted residual blocks </span></span><br><span class="line">        <span class="comment"># Stage2~Stage8</span></span><br><span class="line">        <span class="keyword">for</span> cnf <span class="keyword">in</span> inverted_residual_setting:</span><br><span class="line">            layers.update(&#123;cnf.index: block(cnf, norm_layer)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build top</span></span><br><span class="line">        <span class="comment"># Stage9</span></span><br><span class="line">        last_conv_input_c = inverted_residual_setting[-<span class="number">1</span>].out_c</span><br><span class="line">        last_conv_output_c = adjust_channels(<span class="number">1280</span>)</span><br><span class="line">        layers.update(&#123;<span class="string">&quot;top&quot;</span>: ConvBNActivation(in_planes=last_conv_input_c,</span><br><span class="line">                                               out_planes=last_conv_output_c,</span><br><span class="line">                                               kernel_size=<span class="number">1</span>,</span><br><span class="line">                                               norm_layer=norm_layer)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Stage~Stage8+Stagr9的Conv1，特征提取部分</span></span><br><span class="line">        self.features = nn.Sequential(layers)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        classifier = []</span><br><span class="line">        <span class="keyword">if</span> dropout_rate &gt; <span class="number">0</span>:</span><br><span class="line">            classifier.append(nn.Dropout(p=dropout_rate, inplace=<span class="literal">True</span>))</span><br><span class="line">        classifier.append(nn.Linear(last_conv_output_c, num_classes))</span><br><span class="line">        self.classifier = nn.Sequential(*classifier)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initial weights</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.ones_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">return</span> self._forward_impl(x)</span><br></pre></td></tr></table></figure>
<h2 id="实例化EfficientNet-B0-B7">实例化EfficientNet-B0~B7</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b0</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 224x224</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.0</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">1.0</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.2</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b1</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 240x240</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.0</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">1.1</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.2</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b2</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 260x260</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.1</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">1.2</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.3</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b3</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 300x300</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.2</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">1.4</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.3</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b4</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 380x380</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.4</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">1.8</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.4</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b5</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 456x456</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.6</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">2.2</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.4</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b6</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 528x528</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">1.8</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">2.6</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.5</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficientnet_b7</span>(<span class="params">num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># input image size 600x600</span></span><br><span class="line">    <span class="keyword">return</span> EfficientNet(width_coefficient=<span class="number">2.0</span>,</span><br><span class="line">                        depth_coefficient=<span class="number">3.1</span>,</span><br><span class="line">                        dropout_rate=<span class="number">0.5</span>,</span><br><span class="line">                        num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<h1>训练结果</h1>
<p>注意：<strong><code>from model import efficientnet_b0 as create_model</code>中的<code>efficientnet_b0</code>需要和`num_model = &quot;B0&quot;保持一致</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.optim.lr_scheduler <span class="keyword">as</span> lr_scheduler</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> efficientnet_b0 <span class="keyword">as</span> create_model</span><br><span class="line"><span class="keyword">from</span> my_dataset <span class="keyword">import</span> MyDataSet</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> read_split_data, train_one_epoch, evaluate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    device = torch.device(args.device <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Start Tensorboard with &quot;tensorboard --logdir=runs&quot;, view at http://localhost:6006/&#x27;</span>)</span><br><span class="line">    tb_writer = SummaryWriter()</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(<span class="string">&quot;./weights&quot;</span>) <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">        os.makedirs(<span class="string">&quot;./weights&quot;</span>)</span><br><span class="line"></span><br><span class="line">    train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args.data_path)</span><br><span class="line"></span><br><span class="line">    img_size = &#123;<span class="string">&quot;B0&quot;</span>: <span class="number">224</span>,</span><br><span class="line">                <span class="string">&quot;B1&quot;</span>: <span class="number">240</span>,</span><br><span class="line">                <span class="string">&quot;B2&quot;</span>: <span class="number">260</span>,</span><br><span class="line">                <span class="string">&quot;B3&quot;</span>: <span class="number">300</span>,</span><br><span class="line">                <span class="string">&quot;B4&quot;</span>: <span class="number">380</span>,</span><br><span class="line">                <span class="string">&quot;B5&quot;</span>: <span class="number">456</span>,</span><br><span class="line">                <span class="string">&quot;B6&quot;</span>: <span class="number">528</span>,</span><br><span class="line">                <span class="string">&quot;B7&quot;</span>: <span class="number">600</span>&#125;</span><br><span class="line">    num_model = <span class="string">&quot;B0&quot;</span></span><br><span class="line"></span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(img_size[num_model]),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(img_size[num_model]),</span><br><span class="line">                                   transforms.CenterCrop(img_size[num_model]),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化训练数据集</span></span><br><span class="line">    train_dataset = MyDataSet(images_path=train_images_path,</span><br><span class="line">                              images_class=train_images_label,</span><br><span class="line">                              transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化验证数据集</span></span><br><span class="line">    val_dataset = MyDataSet(images_path=val_images_path,</span><br><span class="line">                            images_class=val_images_label,</span><br><span class="line">                            transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line"></span><br><span class="line">    batch_size = args.batch_size</span><br><span class="line">    nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                               batch_size=batch_size,</span><br><span class="line">                                               shuffle=<span class="literal">True</span>,</span><br><span class="line">                                               pin_memory=<span class="literal">True</span>,</span><br><span class="line">                                               num_workers=nw,</span><br><span class="line">                                               collate_fn=train_dataset.collate_fn)</span><br><span class="line"></span><br><span class="line">    val_loader = torch.utils.data.DataLoader(val_dataset,</span><br><span class="line">                                             batch_size=batch_size,</span><br><span class="line">                                             shuffle=<span class="literal">False</span>,</span><br><span class="line">                                             pin_memory=<span class="literal">True</span>,</span><br><span class="line">                                             num_workers=nw,</span><br><span class="line">                                             collate_fn=val_dataset.collate_fn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果存在预训练权重则载入</span></span><br><span class="line">    model = create_model(num_classes=args.num_classes).to(device)</span><br><span class="line">    <span class="keyword">if</span> args.weights != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(args.weights):</span><br><span class="line">            weights_dict = torch.load(args.weights, map_location=device)</span><br><span class="line">            load_weights_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> weights_dict.items()</span><br><span class="line">                                 <span class="keyword">if</span> model.state_dict()[k].numel() == v.numel()&#125;</span><br><span class="line">            <span class="built_in">print</span>(model.load_state_dict(load_weights_dict, strict=<span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> FileNotFoundError(<span class="string">&quot;not found weights file: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(args.weights))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 是否冻结权重</span></span><br><span class="line">    <span class="keyword">if</span> args.freeze_layers:</span><br><span class="line">        <span class="keyword">for</span> name, para <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="comment"># 除最后一个卷积层和全连接层外，其他权重全部冻结</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;features.top&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name) <span class="keyword">and</span> (<span class="string">&quot;classifier&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name):</span><br><span class="line">                para.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;training &#123;&#125;&quot;</span>.<span class="built_in">format</span>(name))</span><br><span class="line"></span><br><span class="line">    pg = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    optimizer = optim.SGD(pg, lr=args.lr, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1E-4</span>)</span><br><span class="line">    <span class="comment"># Scheduler https://arxiv.org/pdf/1812.01187.pdf</span></span><br><span class="line">    lf = <span class="keyword">lambda</span> x: ((<span class="number">1</span> + math.cos(x * math.pi / args.epochs)) / <span class="number">2</span>) * (<span class="number">1</span> - args.lrf) + args.lrf  <span class="comment"># cosine</span></span><br><span class="line">    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        <span class="comment"># train</span></span><br><span class="line">        mean_loss = train_one_epoch(model=model,</span><br><span class="line">                                    optimizer=optimizer,</span><br><span class="line">                                    data_loader=train_loader,</span><br><span class="line">                                    device=device,</span><br><span class="line">                                    epoch=epoch)</span><br><span class="line"></span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        acc = evaluate(model=model,</span><br><span class="line">                       data_loader=val_loader,</span><br><span class="line">                       device=device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[epoch &#123;&#125;] accuracy: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, <span class="built_in">round</span>(acc, <span class="number">3</span>)))</span><br><span class="line">        tags = [<span class="string">&quot;loss&quot;</span>, <span class="string">&quot;accuracy&quot;</span>, <span class="string">&quot;learning_rate&quot;</span>]</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">0</span>], mean_loss, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">1</span>], acc, epoch)</span><br><span class="line">        tb_writer.add_scalar(tags[<span class="number">2</span>], optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>], epoch)</span><br><span class="line"></span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;./weights/model-&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_classes&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">30</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lrf&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集所在根目录</span></span><br><span class="line">    <span class="comment"># https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--data-path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        default=<span class="string">&quot;D:/python_test/deep-learning-for-image-processing/data_set/flower_data/flower_photos&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># download model weights</span></span><br><span class="line">    <span class="comment"># 链接: https://pan.baidu.com/s/1ouX0UmjCsmSx3ZrqXbowjw  密码: 090i</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weights&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;./efficientnetb0.pth&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;initial weights path&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--freeze-layers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">False</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--device&#x27;</span>, default=<span class="string">&#x27;cuda:0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;device id (i.e. 0 or 0,1 or cpu)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    opt = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(opt)</span><br></pre></td></tr></table></figure>
<p><img src="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="训练结果"></p>
<h1>预测结果</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> efficientnet_b0 <span class="keyword">as</span> create_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    img_size = &#123;<span class="string">&quot;B0&quot;</span>: <span class="number">224</span>,</span><br><span class="line">                <span class="string">&quot;B1&quot;</span>: <span class="number">240</span>,</span><br><span class="line">                <span class="string">&quot;B2&quot;</span>: <span class="number">260</span>,</span><br><span class="line">                <span class="string">&quot;B3&quot;</span>: <span class="number">300</span>,</span><br><span class="line">                <span class="string">&quot;B4&quot;</span>: <span class="number">380</span>,</span><br><span class="line">                <span class="string">&quot;B5&quot;</span>: <span class="number">456</span>,</span><br><span class="line">                <span class="string">&quot;B6&quot;</span>: <span class="number">528</span>,</span><br><span class="line">                <span class="string">&quot;B7&quot;</span>: <span class="number">600</span>&#125;</span><br><span class="line">    num_model = <span class="string">&quot;B0&quot;</span></span><br><span class="line"></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(img_size[num_model]),</span><br><span class="line">         transforms.CenterCrop(img_size[num_model]),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    img_path = <span class="string">&quot;tulip.jpg&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    img = data_transform(img)</span><br><span class="line">    <span class="comment"># expand batch dimension</span></span><br><span class="line">    img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    model = create_model(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./weights/model-2.pth&quot;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_weight_path, map_location=device))</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># predict class</span></span><br><span class="line">        output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">        predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">        predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">    print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                                 predict[predict_cla].numpy())</span><br><span class="line">    plt.title(print_res)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                                  predict[i].numpy()))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p><img src="/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="预测结果"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
              <a href="/tags/Pytorch%E6%90%AD%E5%BB%BACNN/" rel="tag"><i class="fa fa-tag"></i> Pytorch搭建CNN</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/05/26/EfficientNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" rel="prev" title="深度学习模型之CNN（十七）EfficientNet网络详解">
                  <i class="fa fa-chevron-left"></i> 深度学习模型之CNN（十七）EfficientNet网络详解
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/29/EfficientNetV2%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" rel="next" title="EfficientNetV2网络详解">
                  EfficientNetV2网络详解 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="valine-comments"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Linvil Yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("04/21/2023 22:22:22");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "已运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","app_key":"iNfyeaWVmA11Duj7fzr0B1r1","server_url":"https://16p3s6fl.lc-cn-n1-shared.com","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




<script class="next-config" data-name="valine" type="application/json">{"enable":true,"appId":"16p3s6fLzeTRVQeTGaUl2ZaN-gzGzoHsz","appKey":"iNfyeaWVmA11Duj7fzr0B1r1","serverURLs":"https://16p3s6fl.lc-cn-n1-shared.com","placeholder":"请写下您的评论","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"lang":null,"visitor":false,"comment_count":true,"recordIP":true,"enableQQ":true,"requiredFields":[],"el":"#valine-comments","path":"/2023/05/27/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAEfficientNet%E7%BD%91%E7%BB%9C/"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.valine.el)
    .then(() => NexT.utils.getScript(
      'https://fastly.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
      { condition: window.Valine }
    ))
    .then(() => {
      new Valine(CONFIG.valine);
    });
});
</script>
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '32px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

</body>
</html>
