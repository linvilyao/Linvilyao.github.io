<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习模型之CNN（三）AlexNet网络结构及数据集下载</title>
    <url>/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/</url>
    <content><![CDATA[<h1 id="AlexNet详解"><a href="#AlexNet详解" class="headerlink" title="AlexNet详解"></a>AlexNet详解</h1><p>AlexNet时2012年ILSVRC 2012（ImageNet Large Scale Visual Recognition Challenge）竞赛的冠军网络，分类准确率由传统的70%+提升到80%+。它是由Hinton和他的学生Alex Krizhevsky设计的。也是从那年后，深度学习开始迅速发展。</p>
<p>ILSVRC 2012</p>
<ul>
<li>训练集：1,281,167张已标注图片</li>
<li>验证集：50,000张已标注图片</li>
<li>测试集：100,000张未标注图片</li>
</ul>
<p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/AlexNet网络架构.png" alt="AlexNet网络架构"></p>
<h2 id="网络的亮点"><a href="#网络的亮点" class="headerlink" title="网络的亮点"></a>网络的亮点</h2><ol>
<li>首次使用GPU进行网络加速训练</li>
<li>使用ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数</li>
<li>使用LRN局部响应归一化</li>
<li>在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合</li>
</ol>
<ul>
<li>高端GPU的提速比可以达到CPU的20-50倍的速度差距</li>
<li>sigmoid激活函数的两个缺点：<strong>1、求导的过程比较麻烦；2、当网络比较深的时候会出现梯度消失的现象</strong>。ReLU能够解决以上问题</li>
<li>dropout操作可以减少过拟合现象</li>
</ul>
<h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>过拟合：根本原因是特征维度过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多，导致拟合的函数完美的预测训练集，但对新数据的测试集预测结果差。过度的拟合了训练数据，而没有考虑到泛化能力</p>
<p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/过拟合.png" alt="过拟合"></p>
<ul>
<li>第一幅图是网络的一个初始状态，随机地划分了一条边界对于样本进行分类</li>
<li>通过不断的训练过程中，网络会慢慢学习出一条分类的边界如第二幅图所示，得到了一个比较好的分类的结果</li>
<li>第三幅图虽然能够将训练样本进行完全正确的分类，但是图中出现了过拟合现象</li>
<li><strong>过拟合的函数能够完美地预测训练集，但是对新数据的测试机预测效果较差，过度的拟合了训练数据，而没有考虑到泛化能力</strong></li>
</ul>
<h2 id="使用dropout减少过拟合现象"><a href="#使用dropout减少过拟合现象" class="headerlink" title="使用dropout减少过拟合现象"></a>使用dropout减少过拟合现象</h2><p>使用Dropout的方式在网络正向传播过程中随机失活一部分神经元</p>
<p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/Dropout.png" alt="Dropout"></p>
<ul>
<li>左图是一个正常的全连接的正向传播过程，每一个节点都与下层的节点进行全连接</li>
<li>使用了dropout之后，会在每一层随机地失活一部分神经元，变相地减少了网络中训练的参数，从而达到了减少过拟合现象的作用</li>
</ul>
<h1 id="AlexNet网络结构"><a href="#AlexNet网络结构" class="headerlink" title="AlexNet网络结构"></a>AlexNet网络结构</h1><p>经卷积后的矩阵尺寸大小计算公式为：</p>
<p>\begin{flalign}<br>        N = (W-F+2P)/S + 1<br>        \end{flalign}</p>
<ul>
<li>输入图片大小：W×W</li>
<li>Filter大小：F×F</li>
<li>步长：S</li>
<li>padding的像素数：P</li>
</ul>
<p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/AlexNet网络架构.png" alt="AlexNet网络架构"></p>
<ul>
<li>这个图可以看成上下两部分：<strong>作者使用了两块GPU进行了并行运算</strong></li>
<li>上下两部分都是一样的，只用看其中一部分即可</li>
</ul>
<h2 id="Conv1"><a href="#Conv1" class="headerlink" title="Conv1"></a><strong>Conv1</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/第一层（Conv1）.png" alt="Conv1"></p>
<ul>
<li>原始图像是一个224*224的channel为3的彩色图像</li>
<li>卷积核大小是11*11</li>
<li>步长为4</li>
<li>卷积核的大小为11</li>
<li>一共有48*2=96个卷积核</li>
<li>可以推理出padding的大小是1和2：表示在特征矩阵的左边加上一列0，右边加上两列0，上面加上一列0，下面加上两列0<strong>（注意代表padding的2p值的是两边padding的像素之和，并不一定要求两边像素一定要一样）</strong></li>
</ul>
<h2 id="Maxpooling1"><a href="#Maxpooling1" class="headerlink" title="Maxpooling1"></a><strong>Maxpooling1</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/Maxpooling1.png" alt="Maxpooling1"></p>
<ul>
<li>最大池化下采样操作</li>
<li>池化核大小等于3</li>
<li>padding为0</li>
<li>步长为2</li>
<li>这一层的输入是第一层卷积层的输出</li>
<li><strong>池化操作只会改变输出矩阵的高度和宽度，不会改变特征矩阵的深度</strong></li>
</ul>
<h2 id="Conv2"><a href="#Conv2" class="headerlink" title="Conv2"></a><strong>Conv2</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/第二层（Conv2）.png" alt="Conv2"></p>
<ul>
<li>卷积核的个数为128*2=256</li>
<li>卷积核的大小为5</li>
<li>padding为 [ 2，2 ]</li>
<li>步长为1</li>
</ul>
<h2 id="Maxpooling2"><a href="#Maxpooling2" class="headerlink" title="Maxpooling2"></a><strong>Maxpooling2</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/Maxpooling2.png" alt="Maxpooling2"></p>
<ul>
<li>池化核大小为3</li>
<li>padding为0</li>
<li>步长等于2</li>
</ul>
<h2 id="Conv3"><a href="#Conv3" class="headerlink" title="Conv3"></a><strong>Conv3</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/第三层（Conv3）.png" alt="Conv3"></p>
<ul>
<li>卷积核的个数为192*2=384</li>
<li>卷积核的大小为3</li>
<li>padding为 [ 1，1 ]</li>
<li>步长为1</li>
</ul>
<h2 id="Conv4"><a href="#Conv4" class="headerlink" title="Conv4"></a><strong>Conv4</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/第四层（Conv4）.png" alt="Conv4"></p>
<ul>
<li>卷积核的个数为192*2=384</li>
<li>卷积核的大小为3</li>
<li>padding为 [ 1，1 ]</li>
<li>步长为1</li>
</ul>
<h2 id="Conv5"><a href="#Conv5" class="headerlink" title="Conv5"></a><strong>Conv5</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/第五层（Conv5）.png" alt="Conv5"></p>
<ul>
<li>卷积核的个数为128*2=256</li>
<li>卷积核的大小为3</li>
<li>padding为 [ 1，1 ]</li>
<li>步长为1</li>
</ul>
<h2 id="Maxpooling3"><a href="#Maxpooling3" class="headerlink" title="Maxpooling3"></a><strong>Maxpooling3</strong></h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/Maxpooling3.png" alt="Maxpooling3"></p>
<ul>
<li>池化核的大小为3</li>
<li>padding等于0</li>
<li>步长为2</li>
<li>输出的特征矩阵展平之后和三个全连接层进行连接<strong>（注意最后一个全连接层只有1000个节点，对应数据集的1000个类别，如果要将这个网络应用到自己的数据集的话，只需要将最后一层全连接层的节点个数改成和自己数据集的类别数一致即可）</strong></li>
</ul>
<h2 id="层级参数总结"><a href="#层级参数总结" class="headerlink" title="层级参数总结"></a>层级参数总结</h2><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/AlexNet网络架构过程总结.png" alt="AlexNet网络架构过程总结"></p>
<h1 id="下载花分类数据集"><a href="#下载花分类数据集" class="headerlink" title="下载花分类数据集"></a><strong>下载花分类数据集</strong></h1><p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/下载花分类数据集.png" alt="下载花分类数据集"></p>
<p>步骤如下：</p>
<ul>
<li>在data_set文件夹下创建新文件夹”flower_data”</li>
<li>点击链接下载花分类数据集 <a href="https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz">https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz</a></li>
<li>解压数据集到flower_data文件夹下</li>
<li>执行”split_data.py”脚本(<a href="https://pan.baidu.com/s/1bijzd-8zBjKLlXi5uG9pFA?pwd=1234">网盘下载</a>)自动将数据集划分成训练集train和验证集val（训练集：测试集 = 9 ：1）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── data_set </span><br><span class="line">	├── split_data.py</span><br><span class="line">	└── flower_data   </span><br><span class="line">		├── flower_photos（解压的数据集文件夹，3670个样本）  </span><br><span class="line">		├── train（生成的训练集，3306个样本）  </span><br><span class="line">		└── val（生成的验证集，364个样本） </span><br></pre></td></tr></table></figure>
<p>附split_data.py代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copy, rmtree</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mk_file</span>(<span class="params">file_path: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(file_path):</span><br><span class="line">        <span class="comment"># 如果文件夹存在，则先删除原文件夹在重新创建</span></span><br><span class="line">        rmtree(file_path)</span><br><span class="line">    os.makedirs(file_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 保证随机可复现</span></span><br><span class="line">    random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集中10%的数据划分到验证集中</span></span><br><span class="line">    split_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指向你解压后的flower_photos文件夹</span></span><br><span class="line">    cwd = os.getcwd()</span><br><span class="line">    data_root = os.path.join(cwd, <span class="string">&quot;flower_data&quot;</span>)</span><br><span class="line">    origin_flower_path = os.path.join(data_root, <span class="string">&quot;flower_photos&quot;</span>)</span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(origin_flower_path), <span class="string">&quot;path &#x27;&#123;&#125;&#x27; does not exist.&quot;</span>.<span class="built_in">format</span>(origin_flower_path)</span><br><span class="line"></span><br><span class="line">    flower_class = [cla <span class="keyword">for</span> cla <span class="keyword">in</span> os.listdir(origin_flower_path)</span><br><span class="line">                    <span class="keyword">if</span> os.path.isdir(os.path.join(origin_flower_path, cla))]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 建立保存训练集的文件夹</span></span><br><span class="line">    train_root = os.path.join(data_root, <span class="string">&quot;train&quot;</span>)</span><br><span class="line">    mk_file(train_root)</span><br><span class="line">    <span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">        <span class="comment"># 建立每个类别对应的文件夹</span></span><br><span class="line">        mk_file(os.path.join(train_root, cla))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 建立保存验证集的文件夹</span></span><br><span class="line">    val_root = os.path.join(data_root, <span class="string">&quot;val&quot;</span>)</span><br><span class="line">    mk_file(val_root)</span><br><span class="line">    <span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">        <span class="comment"># 建立每个类别对应的文件夹</span></span><br><span class="line">        mk_file(os.path.join(val_root, cla))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">        cla_path = os.path.join(origin_flower_path, cla)</span><br><span class="line">        images = os.listdir(cla_path)</span><br><span class="line">        num = <span class="built_in">len</span>(images)</span><br><span class="line">        <span class="comment"># 随机采样验证集的索引</span></span><br><span class="line">        eval_index = random.sample(images, k=<span class="built_in">int</span>(num*split_rate))</span><br><span class="line">        <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">            <span class="keyword">if</span> image <span class="keyword">in</span> eval_index:</span><br><span class="line">                <span class="comment"># 将分配至验证集中的文件复制到相应目录</span></span><br><span class="line">                image_path = os.path.join(cla_path, image)</span><br><span class="line">                new_path = os.path.join(val_root, cla)</span><br><span class="line">                copy(image_path, new_path)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 将分配至训练集中的文件复制到相应目录</span></span><br><span class="line">                image_path = os.path.join(cla_path, image)</span><br><span class="line">                new_path = os.path.join(train_root, cla)</span><br><span class="line">                copy(image_path, new_path)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\r[&#123;&#125;] processing [&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(cla, index+<span class="number">1</span>, num), end=<span class="string">&quot;&quot;</span>)  <span class="comment"># processing bar</span></span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;processing done!&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>运行split_data.py之后，结果如图所示</p>
<p><img src="/2023/05/02/AlexNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD/split_data结果.png" alt="split_data结果"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本节课主要讲AlexNet的网络结构，以及需要使用到的数据集，下节课将会使用Pytorch来搭建AlexNet，并用下载好的花分类数据集进行训练。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（七）GoogLeNet网络详解</title>
    <url>/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="GoogLeNet详解"><a href="#GoogLeNet详解" class="headerlink" title="GoogLeNet详解"></a>GoogLeNet详解</h1><p>GoogLeNet在2014年由Google团队提出，斩获当年ImageNet竞赛中Classification Task (分类任务) 第一名。原论文地址：<a href="https://arxiv.org/abs/1409.4842">Going deeper with convolutions</a></p>
<p>GoogLeNet 的创新点：</p>
<ul>
<li>引入了<strong>Inception结构</strong>（融合不同尺度的特征信息）</li>
<li>使用1x1的卷积核进行降维以及映射处理（虽然VGG网络中也有，但该论文介绍的更详细）</li>
<li>添加两个辅助分类器帮助训练（<strong>AlexNet和VGG都只有一个输出层，GoogLeNet有三个输出层（其中两个辅助分类层）</strong>）</li>
<li>丢弃全连接层，使用平均池化层（大大减少模型参数，除去两个辅助分类器，网络大小只有VGG的1/20）</li>
</ul>
<h2 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h2><p>传统的CNN结构如AlexNet、VggNet（下图）都是<strong>串联的结构</strong>，即将一系列的卷积层和池化层进行串联得到的结构。</p>
<p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/VGG网络架构图.png" alt="VGG网络架构图"></p>
<h3 id="Inception原始结构"><a href="#Inception原始结构" class="headerlink" title="Inception原始结构"></a>Inception原始结构</h3><p>GoogLeNet 提出了一种并联结构，下图是论文中提出的Inception原始结构，将特征矩阵<strong>同时输入到多个分支</strong>进行处理，并将输出的特征矩阵<strong>按深度进行拼接</strong>，得到最终输出。</p>
<p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inception结构初始版本.png" alt="Inception结构初始版本"></p>
<p>在之前所讲的网络中，例如AlexNet和VGG，网络都是串行结构，将一系列的卷积层和最大池化下采样层进行串联得到一个网络结构。而GoogLeNet中的Inception结构是一个并行的结构。</p>
<p>也就是说，在上一层输入之后，将得到的特征矩阵同时输入到四个分支当中进行处理，处理之后将得到的四个分支的特征矩阵按深度进行拼接，得到输出特征矩阵。</p>
<p>在特征分支中，分别为大小是1*1、3*3、5*5的卷积核和大小为3*3的池化核，通过这四个分支能得到不同尺度的特征矩阵。<strong>注意：每个分支所得的特征矩阵高和宽必须相同</strong>，否则无法延深度方向进行拼接。</p>
<h3 id="Inception结构（降维功能）"><a href="#Inception结构（降维功能）" class="headerlink" title="Inception结构（降维功能）"></a>Inception结构（降维功能）</h3><p>在 Inception 的基础上，还可以加上降维功能的结构，如下图所示，在原始 Inception 结构的基础上，在分支2，3，4上加入了<strong>卷积核大小为1x1的卷积层</strong>，目的是为了降维（减小深度），减少模型训练参数，减少计算量。</p>
<p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/加上降维功能的Inception结构.png" alt="加上降维功能的Inception结构"></p>
<h3 id="1-1卷积核的降维功能"><a href="#1-1卷积核的降维功能" class="headerlink" title="1*1卷积核的降维功能"></a>1*1卷积核的降维功能</h3><p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/降维图示.png" alt="降维图示"></p>
<p>假设有一个深度为512的特征矩阵</p>
<p>当不使用1*1的卷积核降维，直接使用64个5*5的卷积核进行卷积的话，需要用到的参数个数：5*5*512*64 = 819200</p>
<p>当先使用24个卷积核大小1*1的卷积核进行卷积时，得出的特征矩阵深度会由原先的512变为24（<strong>由输入的卷积核的个数决定的</strong>），之后再使用64个卷积核大小为5*5的卷积核进行卷积，需要用到的参数个数：1*1*512*24 + 5*5*24*64 = 12288+38400 = 50688。</p>
<p>因此，在添加了3个卷积核大小为1*1的卷积核进行卷积之后，能对特征矩阵进行降维，减少特征矩阵的深度，从而减少所需的卷积参数，也就减少了计算量。</p>
<p>注：<strong>CNN参数个数 = 卷积核尺寸×卷积核深度 × 卷积核组数 = 卷积核尺寸 × 输入特征矩阵深度 × 输出特征矩阵深度</strong></p>
<h2 id="辅助分类器（Auxiliary-Classifier）"><a href="#辅助分类器（Auxiliary-Classifier）" class="headerlink" title="辅助分类器（Auxiliary Classifier）"></a>辅助分类器（Auxiliary Classifier）</h2><p>AlexNet 和 VGG 都只有1个输出层，GoogLeNet 有3个输出层，其中的两个是辅助分类层。如下图所示，网络主干右边的两个分支 就是辅助分类器，其结构一模一样。</p>
<p>网络中的两个辅助分类器分别来自于4a和4d，对照参数表中的数据，4a的特征矩阵输出尺寸为14*14*512，4d输出的特征矩阵的尺寸为14*14*528，这两个特征矩阵高度和宽度都一样，只有深度不同。</p>
<p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/辅助分类器.png" alt="辅助分类器"></p>
<p>在训练模型时，将两个辅助分类器的损失乘以权重（论文中是0.3）加到网络的整体损失上，再进行反向传播。</p>
<p>辅助分类器的两个分支有什么用呢？</p>
<ul>
<li>可以把他看做Inception网络中的一个小细节，它确保了即便是隐藏单元和中间层也参与了特征计算，他们也能预测图片的类别，他在Inception网络中起到一种调整的效果，并且能防止网络发生过拟合。</li>
<li>给定深度相对较大的网络，有效传播梯度反向通过所有层的能力是一个问题。通过将辅助分类器添加到这些中间层，可以期望较低阶段分类器的判别力。在训练期间，它们的损失以折扣权重（辅助分类器损失的权重是0.3）加到网络的整个损失上。</li>
</ul>
<p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/GoogLeNet网络架构.png" alt="GoogLeNet网络架构"></p>
<h2 id="GoogLeNet-网络参数"><a href="#GoogLeNet-网络参数" class="headerlink" title="GoogLeNet 网络参数"></a>GoogLeNet 网络参数</h2><p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/GoogLeNet网络参数.png" alt="GoogLeNet网络参数"></p>
<p>对于Inception模块，所需要使用到参数有<code>#1x1</code>, <code>#3x3reduce</code>, <code>#3x3</code>, <code>#5x5reduce</code>, <code>#5x5</code>, <code>poolproj</code>，这6个参数，分别对应着所使用的卷积核个数。</p>
<p><img src="/2023/05/09/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/inception卷积核个数.png" alt="Inception卷积核个数"></p>
<ul>
<li><code>#1x1</code>对应着分支1上1x1的卷积核个数</li>
<li><code>#3x3reduce</code>对应着分支2上1x1的卷积核个数</li>
<li><code>#3x3</code>对应着分支2上3x3的卷积核个数</li>
<li><code>#5x5reduce</code>对应着分支3上1x1的卷积核个数</li>
<li><code>#5x5</code>对应着分支3上5x5的卷积核个数</li>
<li><code>poolproj</code>对应着分支4上1x1的卷积核个数。</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（十一）ResNeXt网络结构</title>
    <url>/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<p>本堂课讲述根据ResNet网络之后再次升级的ResNeXt网络，对原先ResNet的一些地方做了优化处理，原论文：<a href="https://arxiv.org/abs/1611.05431">img Transformations for Deep Neural Networks</a></p>
<h1 id="ResNet和ResNeXt的比较"><a href="#ResNet和ResNeXt的比较" class="headerlink" title="ResNet和ResNeXt的比较"></a>ResNet和ResNeXt的比较</h1><p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/50层以上的ResNet和50层以上的ResNeXt残差比较.png" alt="50层以上的ResNet和50层以上的ResNeXt残差比较 " style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<p>上图（左）是在ResNet网络中提到的<strong>实线型残差结构</strong>，在主分支中先对channel为256的特征矩阵<strong>降维</strong>到64，之后用3x3的卷积层进行<strong>卷积处理</strong>，再通过1x1的卷积核<strong>升维</strong>到256，以此来减少参数，最后将得到的特征矩阵与shortcut的特征矩阵相加得到输出特征矩阵</p>
<p>在ResNeXt网络中，使用上图（右）的结构替代上图（左）的结构，经下图展示，替换之后的ResNeXt网络结构确实比原先的ResNet在各性能上更优秀</p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/ResNet和ResNeXt性能对比.png" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<p>下图（左）是ResNet-50与ResNeXt-50在ImageNet上top-1的错误率比较，在对焦验证集中ResNet-50val（蓝色）和ResNeXt-50val（橙色）两条实线的错误率比较，明显的是在计算量相同的情况下，RexNeXt-50比ResNet-50的错误率更低。</p>
<p>下图（右）是ResNet-101与ResNeXt-101在ImageNet上top-1的错误率比较，与50层类似，在计算量相同的情况下，RexNeXt-101比ResNet-101的错误率更低。</p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/ResNet和ResNeXt的对比.png" alt="ResNet和ResNeXt的对比" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<p><strong>Group Convolution对比Convolution</strong></p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/GroupConvolution解释.png" alt="Group Convolution解释" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<p><strong>Convolution</strong></p>
<blockquote>
<p>假设输入特征矩阵size = k x k，channel = 4，那么此时对于每一个卷积核的channel要与输入特征矩阵的channel保持一致，所以卷积核的channel = 4。假设输出特征矩阵channel = n，表示需要输入n个卷积核来进行卷积处理。</p>
<p>综上，需要使用到的参数Parameters = k x k x $C_{in}$ x n（$C_{in}$表示输入特征矩阵的channel，n表示使用卷积核的个数）</p>
</blockquote>
<p><strong>Group Convolution</strong></p>
<blockquote>
<p>假设输入特征矩阵size = k x k，channel = 4，将输入特征矩阵的channel划分为两个组，之后对每一个组分别进行卷积操作。假设对划分的一个组而言，所采用的卷积核的channel要与该组的channel保持一致，因此卷积核channel = 4 / 2 = 2。假设输出特征矩阵channel = n，则经过分成两组之后，其中一组的channel = n / 2，表示需要输入n / 2个卷积核来进行卷积处理。</p>
<p>综上，需要使用到的参数Parameters = ( k x k x $C_{in}$/g x n/g ) / g = k x k x $C_{in}$ x n x 1/g（$C_{in}$表示输入特征矩阵的channel，n表示使用卷积核的个数，g表示分了多少个组）</p>
</blockquote>
<p>注意：<strong>当g = $C_{in}$，n = $C_{in}$，此时就是DW Conv（深度卷积depthwise conv），相当于对输入特征矩阵的每个channel分配了一个channel为1的卷积核进行卷积</strong></p>
<h1 id="ResNeXt的创新点"><a href="#ResNeXt的创新点" class="headerlink" title="ResNeXt的创新点"></a>ResNeXt的创新点</h1><h2 id="提出Block模块"><a href="#提出Block模块" class="headerlink" title="提出Block模块"></a>提出Block模块</h2><p>在原论文中，作者提出以下a、b、c三种block模块，它们在数学计算上完全等价</p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/Block模块.png" alt="Block模块" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<p><strong>（c）block模块</strong></p>
<blockquote>
<p>首先通过1 x 1的卷积层进行降维处理，在经过Group Conv进行处理，且卷积核size = 3 x 3，group = 32，之后再通过1 x 1的卷积核进行升维处理，最后将输出与shortcut的输出相加，得到最终输出特征矩阵</p>
</blockquote>
<p><strong>（b）block模块</strong></p>
<blockquote>
<p>通过32个分支进行处理（total 32 paths），对于每个分支都首先采用1 x 1的卷积核进行降维处理，每一个分支都将特征矩阵从channel = 256降维到channel = 4，因分支 = 32，所以<strong>实际上通过这一层的降维处理之后，总体channel = 4 x 32 = 128，同（c）的第一步处理一致</strong>；</p>
<p>之后对每个分支的特征矩阵进行3 x 3的卷积核进行卷积处理，处理之后再通过concatenate拼接，再通过1 x 1的卷积核进行升维处理，最后将输出与shortcut的输出相加，得到最终输出特征矩阵</p>
</blockquote>
<p><strong>（a）block模块</strong></p>
<blockquote>
<p>通过32个分支进行处理（total 32 paths），对于每个分支都首先采用1 x 1的卷积核进行降维处理，每一个分支都将特征矩阵从channel = 256降维到channel = 4；之后对每个分支的特征矩阵进行3 x 3的卷积核进行卷积处理；</p>
<p>然后通过1 x 1的卷积核将特征矩阵升维从channel = 4到channel = 256，之后进行相加，<strong>原理上同（b）的concatenate拼接之后得到的特征矩阵一致</strong>；</p>
<p>最后将输出与shortcut的输出相加，得到最终输出特征矩阵</p>
</blockquote>
<p> <strong>（a）block模块的升维和相加步骤</strong></p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/（a）虚线框图像解释.png" alt="（a）虚线框图像解释" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<blockquote>
<p>假设pants = 2，pants中采用数量为1的1 x 1的卷积核进行卷积。对这两个pants分别进行卷积之后会得到对应的feature map，再将二者的feature map进行相加，最终得到结果的feature map。</p>
</blockquote>
<p><strong>（b）block模块的concatenate步骤</strong></p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/（b）虚线框图像解释.png" alt="（b）虚线框图像解释" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<blockquote>
<p>将按照pants = 2的feature map进行拼接，得到上图最左边的feature map，此处采用的kernel同样是1 x 1大小，且将对应的4个卷积核参数进行拼接，最终得到的feature map和前面（a）升维且相加之后的feature map的结果一致。</p>
</blockquote>
<h2 id="Block模块仅适用层结构≥3"><a href="#Block模块仅适用层结构≥3" class="headerlink" title="Block模块仅适用层结构≥3"></a>Block模块仅适用层结构≥3</h2><p>对于浅层的18、34层，其Inception的层级仅只有两层，搭建出来的block已经没有意义了。如果进行拆分之后再进行卷积，再进行相加的话，其实并未产生实质的变化，只不过卷积的个数变多了。</p>
<p><img src="/2023/05/17/ResNeXt%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/Block模块仅适用≥3层的层结构.png" alt="Block模块仅适用≥3层的层结构" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224); "></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本次论文最大创新点就是提出Block模块，再次提高了整体网络的性能。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（五）VGG网络详解及感受野的计算</title>
    <url>/2023/05/04/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8A%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>VGG在2014年由牛津大学著名研究组VGG(Visual GeometryGroup)提出，斩获该年ImageNet竞赛中 Localization Task(定位任务)第一名和 classification Task(分类任务)第二名。</p>
<p>VGG原论文：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</p>
<h1 id="VGG知识介绍"><a href="#VGG知识介绍" class="headerlink" title="VGG知识介绍"></a>VGG知识介绍</h1><p>下图中显示了6个VGG网络的配置，6个配置中作者尝试了不同深度、是否使用LRN，以及卷积核大小为1为3的效果。实际使用过程中，我们常常使用D配置：一共有16层，其中包括13个卷积层，3个全连接层。</p>
<p><img src="/2023/05/04/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8A%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E8%AE%A1%E7%AE%97/VGG网络架构表.png" style="zoom:80%;"></p>
<h2 id="网络的亮点"><a href="#网络的亮点" class="headerlink" title="网络的亮点"></a>网络的亮点</h2><ul>
<li>通过堆叠多个3×3的卷积核来替代大尺度卷积核（为了减少所需的参数）</li>
</ul>
<p>论文内提到，可以通过堆叠两个3×3的卷积核替代一个5×5的卷积核，堆叠三个3×3的卷积核替代一个7×7的卷积核（拥有相同的感受野）</p>
<h2 id="什么是感受野"><a href="#什么是感受野" class="headerlink" title="什么是感受野"></a>什么是感受野</h2><p>在卷积神经网络中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野（receptive filed）。通俗解释是：输出feature map上的一个单元对应输入层上的区域大小 </p>
<p><img src="/2023/05/04/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8A%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E8%AE%A1%E7%AE%97/感受野.png" style="zoom: 50%;"></p>
<p>例子如图所示，最下面是一个9 × 9 × 1的特征矩阵，首先通过卷积层Conv1（卷积核大小3 × 3，步距为2），通过下方的计算公式：（ 9 - 3 + 0 ）/ 2 + 1 = 4，因此得到第二层的特征矩阵大小为4 × 4 × 1；再通过最大池化下采样层MaxPool1（池化核大小2 × 2，步距为2），通过下方计算公式：（ 4 - 3 + 1 ）/ 2 + 1 = 2，因此得到第三层的特征矩阵大小为2 × 2 × 1。</p>
<p>\begin{flalign}<br>        out_{size} = (in_{size} - F_{size} + 2P)/S + 1<br>        \end{flalign}</p>
<p>那么，在第三层特征层当中的一个单元，在第二层中对应的一个感受野是2 × 2的区域，在第三层图中显示是一个5 × 5的感受野大小。</p>
<h2 id="如何计算感受野？"><a href="#如何计算感受野？" class="headerlink" title="如何计算感受野？"></a>如何计算感受野？</h2><ul>
<li>F（ i ）：第 i 层感受野；</li>
<li>Stride：第 i 层的步距；</li>
<li>Ksize：卷积核或采样核尺寸</li>
</ul>
<p><img src="/2023/05/04/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8A%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E8%AE%A1%E7%AE%97/感受野的计算.png" style="zoom:80%;"></p>
<p>论文内提到，可以通过堆叠两个3×3的卷积核替代一个5×5的卷积核，堆叠三个3×3的卷积核替代一个7×7的卷积核</p>
<p>注意：在VGG网络中，卷积层的步距默认为1</p>
<p>假设：一个特征矩阵通过3 × 3 × 3的卷积层后，所得到的Feature map，Feature map上的一个单位对应上一层的感受野为（ 1 - 1 ）× 1 + 3 = 3；如果再计算上上一层的感受野为（ 3 - 1 ）× 1 + 3 = 5；如果再计算上上上层的感受野为（ 5 - 1 ）× 1 + 3 = 7</p>
<p>也就是说，<strong>我们通过3 × 3 × 3的卷积核卷积之后所得到的的特征矩阵上的一个单位在原图上对应的感受野相当于是一个7 × 7的大小，那么也就与采用一个7 × 7的卷积核的大小所得到的的感受野是相同的</strong></p>
<p>因此，<strong>当我们使用多个小的卷积核去堆叠就可以去替代一个大的卷积核，目的是为了节省网络训练参数的个数</strong></p>
<h2 id="7-×-7卷积核和3-×-3-×-3所需参数对比"><a href="#7-×-7卷积核和3-×-3-×-3所需参数对比" class="headerlink" title="7 × 7卷积核和3 × 3 × 3所需参数对比"></a>7 × 7卷积核和3 × 3 × 3所需参数对比</h2><p>假设输入输出channel为C，下方两个C分别表示卷积核的深度和组数</p>
<p>7 × 7卷积核所需参数 ：7 × 7 × C × C = 49CC</p>
<p>3 × 3 × 3卷积核所需参数 ：3 × 3 × C × C <strong>+</strong> 3 × 3 × C × C <strong>+</strong> 3 × 3 × C × C = 27CC</p>
<h1 id="VGG网络架构"><a href="#VGG网络架构" class="headerlink" title="VGG网络架构"></a>VGG网络架构</h1><ul>
<li>input：224*224的RGB图片</li>
<li>通过2层3 × 3的卷积层、1层最大下采样池化层</li>
<li>再通过2层3 × 3的卷积层、1层最大下采样池化层</li>
<li>再通过3层3 × 3的卷积层、1层最大下采样池化层</li>
<li>再通过3层3 × 3的卷积层、1层最大下采样池化层</li>
<li>再通过3层3 × 3的卷积层、1层最大下采样池化层</li>
<li>连接3个全连接层</li>
<li>经过soft-max处理得到概率分布</li>
</ul>
<p><img src="/2023/05/04/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E5%8F%8A%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E8%AE%A1%E7%AE%97/VGG网络架构图.png" style="zoom:80%;"></p>
<p>补充参数：</p>
<ul>
<li>表中的卷积核：默认stride为1、padding为1<strong>（这样通过一个3 × 3的卷积核进行卷积之后，其输入输出的特征矩阵的高度和宽度不变）</strong>。公式计算得证：（ 3 - 3 + 2 ）/ 1 + 1 = 3</li>
<li>最大下采样池化核：默认size为2、stride为2<strong>（相当于将特征矩阵的高宽缩小为原来的一半）</strong></li>
</ul>
<p>对照左边的图，进行更直观的理解</p>
<p>左边的图形，是根据右表中D模型进行绘制。图中黑色框代表：卷积层+激活函数；红色框代表：最大下采样操作；蓝色框代表：全连接层+激活函数；橙色框代表：softmax处理</p>
<ol>
<li>首先，输入224×224×3的RGB彩色图像；</li>
<li>通过两层3×3的卷积核之后，得到224×224×64特征矩阵大小（stride为1、padding为1——长宽不变；表中标注conv3-64——采用卷积核个数为64，因此输出深度也为64）；</li>
<li>紧接着进入到最大下采样操作， 将特征矩阵缩减为原来的一半，得到112×112×64特征矩阵大小；</li>
<li>通过两层3×3的卷积核之后（采用卷积核个数为128），得到112×112×128特征矩阵大小；</li>
<li>进入到最大下采样操作， 将特征矩阵缩减为原来的一半，得到56×56×128特征矩阵大小；</li>
<li>通过三层3×3的卷积核之后（采用卷积核个数为256），得到56×56×256特征矩阵大小；</li>
<li>进入到最大下采样操作， 将特征矩阵缩减为原来的一半，得到28×28×256特征矩阵大小；</li>
<li>通过三层3×3的卷积核之后（采用卷积核个数为512），得到28×28×512特征矩阵大小；</li>
<li>进入到最大下采样操作， 将特征矩阵缩减为原来的一半，得到14×14×512特征矩阵大小；</li>
<li>通过三层3×3的卷积核之后（采用卷积核个数为512），得到14×14×512特征矩阵大小；</li>
<li>进入到最大下采样操作， 将特征矩阵缩减为原来的一半，得到7×7×512特征矩阵大小；</li>
<li>进入全连接层，全连接层1和全连接层2所采用的节点个数都是4096，全连接层3有1000各节点，因为ImageNet的分类任务有1000个类别<strong>（注意：最后一个全连接层不需要加ReLU激活函数，因为最后需要用到softmax函数来激活）</strong>。</li>
<li>最后是一个softmax激活函数，将预测结果转化为概率分布</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本节课主要讲解CGG16的网络结构，以及拓展感受野的知识点，下节课将会使用Pytorch去搭建VGG16网络。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/04/21/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hello World</category>
      </categories>
      <tags>
        <tag>Hello</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（九）ResNet网络结构、BN以及迁移学习详解</title>
    <url>/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="ResNet详解"><a href="#ResNet详解" class="headerlink" title="ResNet详解"></a>ResNet详解</h1><p>ResNet在2015年由微软实验室提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名（超级NB）。原论文：<a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/ResNet-34层网络结构图.png" alt="ResNet-34层网络结构图"></p>
<h2 id="网络中的创新点"><a href="#网络中的创新点" class="headerlink" title="网络中的创新点"></a>网络中的创新点</h2><ul>
<li>超深的网络结构（突破1000层）</li>
<li>提出residual模块（残差模块，因为有了残差模块，才能够搭建层数很深的网络）</li>
<li>使用Batch Normalization加速训练（丢弃dropout）</li>
</ul>
<p>卷积层和池化层不断叠加是否都会导致准确率不断提高？</p>
<p>下图（左）通过简单将卷积层和池化层进行堆叠所搭建的网络结构，橙黄色曲线表示20层的网络结构，最终的训练错误率在[1%, 2%]，红色区县表示56层网络结构，最终的训练错误率在[7%, 8%]，很明显，当仅仅通过简单的把卷积层和最大池化下采样层堆叠网络，并不代表层数越深效果越好。</p>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/层数加深导致准确率变化情况.png" alt="层数加深导致准确率变化情况"></p>
<p>那么是什么原因导致更深的网络造成效果越差呢？</p>
<p>在ResNet论文中，作者给出以下两个结论：</p>
<ul>
<li>梯度消失或梯度爆炸</li>
<li>退化问题（degradation problem）</li>
</ul>
<h3 id="梯度消失或梯度爆炸"><a href="#梯度消失或梯度爆炸" class="headerlink" title="梯度消失或梯度爆炸"></a><strong>梯度消失或梯度爆炸</strong></h3><p>随着网络层级越来越深，梯度消失或梯度爆炸的现象会越来越明显。</p>
<blockquote>
<p>假设每一层误差梯度小于1，那么在反向传播过程中，每向前传播一次，都要乘以这个小于1的系数。当网络越来越深的时候，所乘的小于1的系数越多，那么这个数越趋近于0，梯度就会越来越小，这就是所说的梯度消失现象。</p>
<p>反过来，当每一层误差梯度是一个大于1的数，那么在反向传播过程中，每向前传播一次，都要乘以这个大于1的系数。当网络越来越深的时候，梯度就会越来越大，这就是所说的梯度爆炸现象。</p>
</blockquote>
<p><strong>如何解决梯度消失/爆炸</strong>：通常对数据进行标准化处理、权重初始化、以及这堂课将会讲述的<strong>Batch Normalization标准化处理</strong>来解决</p>
<h3 id="退化问题"><a href="#退化问题" class="headerlink" title="退化问题"></a>退化问题</h3><p>在解决了梯度消失或梯度爆炸的问题之后，可能仍然会出现层数深的没有层数小的效果好的问题。也就是网络越深反而识别错误率提高的现象。</p>
<p><strong>那该如何来解决提到的退化问题呢？</strong></p>
<p>在ResNet论文当中，作者提到一个名为<strong>残差的结构</strong>，通过残差结构，即能解决退化问题</p>
<p>上图（右）就是在原论文中所搭建的一系列网络，里面有20、32、44、56、110层的网络。其中实线代表验证集的错误率变化情况，虚线代表测试集的错误率变化情况（主要看验证集中的错误率）</p>
<p>在图中，随着层数的加深，错误率越小，效果越好。对比图左，ResNet确实解决了文中所提到的退化问题，因此，我们即可以使用文中提到的残差结构来不断加深网络，以获得更好的结果</p>
<h2 id="residual模块（残差结构）"><a href="#residual模块（残差结构）" class="headerlink" title="residual模块（残差结构）"></a>residual模块（残差结构）</h2><p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/residual结构.png" alt="residual结构"></p>
<h3 id="图形结构对比"><a href="#图形结构对比" class="headerlink" title="图形结构对比"></a>图形结构对比</h3><p>图中代表着两种不同的残差结构，左边的结构主要针对网络层数较少的结构，对应的是32层的网络结构；右边的结构对应的是50、101、152层的网络结构。</p>
<p>左边结构的主分支，将输入特征矩阵通过2个3×3的卷积层得到结果，右边有一个弧线，直接从输入连接到输出，并在输出点有个+符号。意思是：<strong>在主分支上通过一系列卷积层之后所得到的特征矩阵，再与输入特征矩阵进行相加，相加之后再通过ReLU激活函数</strong>。且在主分支上，第一次卷积层之后通过ReLU激活函数激活，但第二层是在与输入特征矩阵相加之后，再通过ReLU激活函数激活的。</p>
<p><strong>注意：主分支与侧分支（shortcut）的输出特征矩阵中shape必须相同</strong></p>
<p>右边结构的主分支，将输入特征矩阵先通过1×1卷积层，再通过3×3的卷积层，最后通过1×1的卷积层之后，与侧分支的输入特征矩阵进行相加，最终得出结果。</p>
<p>相比左边的结构，是在输入及输出分别加入一个1×1的卷积层，目的是用来降维和升维</p>
<blockquote>
<p>在结构中，输入深度为256的特征矩阵，通过1×1卷积层，高宽不变，深度由256变为64，因此第一个卷积层是起到降维的作用。再通过一个3×3卷积层，之后再经过1×1卷积层，高宽不变，深度由64变为256，因此这一个卷积层是起到升维的作用。</p>
</blockquote>
<h3 id="使用参数对比"><a href="#使用参数对比" class="headerlink" title="使用参数对比"></a>使用参数对比</h3><p>左侧residual结构所需参数：3 × 3 × 256 × 256 + 3 × 3 × 256 × 256 = 1,179,648</p>
<p>右侧residual结构所需参数：1 × 1 × 256 × 64 + 3 × 3 × 64 × 64 + 1 × 1 × 64 × 256 = 69,632</p>
<p>使用的残差结构越多，减少的参数也就越多</p>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/原论文中的参数列表.png" alt="原论文中的参数列表"></p>
<p>上图是原论文给出的不同深度的ResNet网络结构配置，代表着在不同层数下的参数列表，所对应的是18、34、50、101、152层的网络结构。注意表中的残差结构给出了主分支上卷积核的大小与卷积核个数，表中 残差块×N 表示将该残差结构重复N次。</p>
<p>图中显示，这几个网络框架是一致的，同样是通过1个7X7的卷积层，再通过3X3的最大池化下采样层，再分别通过一系列残差结构，最后再接1个平均池化下采样层以及全连接层输出，softmax将输出转化为概率分布。</p>
<h3 id="降维时的shortcut"><a href="#降维时的shortcut" class="headerlink" title="降维时的shortcut"></a>降维时的shortcut</h3><h4 id="34层残差结构"><a href="#34层残差结构" class="headerlink" title="34层残差结构"></a>34层残差结构</h4><p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/residual中实虚线的区别-34层.png" alt="residual中实虚线的区别-34层"></p>
<p>经过对 ResNet34层网络的观察，可以发现有些残差块的shortcut是实线的，而有些则是虚线的。这二者间有什么区别呢？</p>
<p>对于上图左两层3X3卷积层的残差结构来说，输入特征矩阵深度和输出特征矩阵的shape是一致的，所以能够直接进行相加。但是对于上图右虚线的残差结构来说，其输入和输出的shape是不一致的。</p>
<p>上图右对应到参数表中，是ResNet-34层conv3.x第一层，输入特征矩阵shape是[56, 56, 64]，输出特征矩阵shape是[28, 28, 128]</p>
<p><strong>只有通过虚线残差结构，得到输出之后输入到实线对应的残差结构当中，才能够保证输入特征矩阵和输出特征矩阵的shape保持一致。</strong></p>
<h4 id="更高层的残差结构"><a href="#更高层的残差结构" class="headerlink" title="更高层的残差结构"></a>更高层的残差结构</h4><p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/residual中实虚线的区别-更高层.png" alt="residual中实虚线的区别-更高层"></p>
<p>上图右虚线对应的参数，也就是参数表中conv3.x对应的一系列残差结构：50,101,152层。输入特征矩阵的shape是[56, 56, 256],输出特征矩阵的shape是[28, 28, 512]</p>
<blockquote>
<p>对于主分支而言，stride=1，所以第一层1X1卷积层只起到降维的作用，不改变高宽，将深度从256改为128；在第二层stride=2，（ 56 - 3 + 1 ）/ 2 + 1 = 28，所以第二层3X3卷积层将特征矩阵的高宽缩减为原本的一半，深度不变；第三层stride=1，所以第三层1X1卷积层只起到升维的作用，不改变高宽，将深度从128升到512。</p>
<p>对于shortcut分支，仅使用1X1，深度为512，stride = 2的卷积层对输入特征矩阵起到升维且改变高宽的目的。深度1X1，将深度从256改为512；stride = 2，将输入特征矩阵高宽56，改为28。</p>
<script type="math/tex; mode=display">
N = （ W-F+2P ）/S+1</script></blockquote>
<p>原论文中，作者对于残差结构的shortcut有optionA、B、C三个方法，但作者通过一系列对比之后，得出optionB方法效果最好。</p>
<p>由此得知，虚线残差结构有一个额外的作用，<strong>就是将输入特征矩阵的shape与输出特征矩阵的shape保持一致（变化高宽深）</strong>，而在实线中，输入和输出特征矩阵的shape是完全无变化的</p>
<p>所以在参数表中，50层及以上的深层网络结构，<strong>conv3、conv4、conv5所对应的一系列残差结构的第一层都是虚线残差结构</strong>。因为第一层需要将上一层输出的特征矩阵的高宽深度调整为当前层所需要的特征矩阵的高宽深度。<strong>在conv2中，仅仅改变了深度（所以用的还是实线）</strong></p>
<h2 id="Batch-Normalization标准化处理"><a href="#Batch-Normalization标准化处理" class="headerlink" title="Batch Normalization标准化处理"></a>Batch Normalization标准化处理</h2><p>老师博客：<a href="https://blog.csdn.net/qq_37541097/article/details/104434557">Batch Normalization详解以及pytorch实验</a></p>
<p>Batch Normalization是google团队在2015年论文<a href="https://arxiv.org/abs/1502.03167">《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》</a>提出的。通过该方法能够加速网络的收敛并提升准确率。本文主要分为以下几个部分：</p>
<ul>
<li>BN的原理</li>
<li>使用pytorch验证本文的观点</li>
<li>使用BN需要注意的地方（BN没用好就是个坑）</li>
</ul>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/Batch Normalization.png" alt="Batch Normalization"></p>
<h3 id="BN原理"><a href="#BN原理" class="headerlink" title="BN原理"></a>BN原理</h3><p>在之前搭建网络过程中，通常第一步会对图像进行标准化处理，也就是将图像数据调整到满足某分布规律，这样能够加速网络的收敛。如下图所示，对于conv1而言，输入的就是经过预处理之后满足某分布规律的特征矩阵，但对于conv2而言，输入的feature map就不一定满足某分布规律了，而Batch Normalization目的即是使feature map满足均值为0，方差为1的分布规律。</p>
<p><strong>注意：Batch Normalization的目的是使我们的一批（Batch）feature map满足均值为0，方差为1 分布规律，并不指某一个feature map</strong></p>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/BN原理图.png" alt="BN原理图"></p>
<p>原论文中有一句：“对于一个拥有d维的输入x，我们将对它的每一个维度进行标准化处理。” </p>
<blockquote>
<p>假设我们输入的x是RGB三通道的彩色图像，那么这里的d就是输入图像的channels即d=3，$x=(x^{(1)}+x^{(2)}+x^{(3)})$，$x^{(1)}$代表我们的R通道所对应的特征矩阵，依此类推。标准化处理也就是<strong>分别</strong>对R通道，G通道，B通道进行处理。</p>
</blockquote>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/公式.png" alt="原论文公式"></p>
<p>γ主要调整数据方差的大小，β主要调整这批数据均值的大小（是否处于中心），如果不通过γ和β进行调整，那么得到的数据是均值为0方差为1的数据分布规律。<strong>γ和β是通过反向传播去学习得到的，均值和方差是根据一批批的数据统计得到的</strong></p>
<p>γ初始值为1，β初始值为0</p>
<p>示例如下</p>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/BN标准化原理示例.png" alt="BN标准化原理示例"></p>
<p>上图展示了一个batch size为2（两张图片）的Batch Normalization的计算过程。</p>
<blockquote>
<p>假设feature1、feature2分别是由image1、image2经过一系列卷积池化后得到的特征矩阵，feature的channel为2，那么$x^{(1)}$代表该batch的所有feature的channel1的数据，即$x^{(1)}$={1，1，1，2，0，-1，2，2}，$x^{(2)}$同理。</p>
<p><strong>注意：最终所得到的均值和方差是一个向量，其向量的长度即为特征矩阵的深度</strong></p>
</blockquote>
<h3 id="使用BN需要注意的问题"><a href="#使用BN需要注意的问题" class="headerlink" title="使用BN需要注意的问题"></a>使用BN需要注意的问题</h3><ul>
<li><p>训练时要将traning参数设置为True，在验证时将trainning参数设置为False。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。</p>
</li>
<li><p>batch size尽可能设置大点，设置小后表现可能很糟糕，设置的越大求的均值和方差越接近整个训练集的均值和方差。</p>
</li>
<li><p>建议将bn层放在卷积层（Conv）和激活层（例如Relu）之间，且卷积层不要使用偏置bias，因为没有用，参考下图推理，即使使用了偏置bias求出的结果也是一样的$y^b_i=y_i$</p>
</li>
</ul>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/使用bias和不使用bias效果一致.png" alt="使用bias和不使用bias效果一致"></p>
<h1 id="迁移学习简介"><a href="#迁移学习简介" class="headerlink" title="迁移学习简介"></a>迁移学习简介</h1><p>迁移学习是一个比较大的领域，我们这里说的迁移学习是指神经网络训练中使用到的迁移学习。</p>
<p>如下图所示，神经网络逐层提取图像的深层信息，这样，预训练网络就相当于一个特征提取器。</p>
<p>迁移学习，即将学习好了的浅层网络的一些参数，迁移到新的网络当中，这一新的网络中也具备识别底层通用特征的能力了。当新网络拥有了底层通用特征检测识别能力之后，就能更快速去学习新的数据集的高维特征。</p>
<p><img src="/2023/05/11/ResNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8CBN%E4%BB%A5%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AF%A6%E8%A7%A3/简单的网络模型.png" alt="简单的网络模型"></p>
<p>使用迁移学习的优势</p>
<ul>
<li>能够快速的训练出一个理想的结果</li>
<li>当数据集较小时也能训练出理想的效果</li>
</ul>
<p><strong>注意：使用别人预训练模型参数时，要注意别人的预处理方式</strong></p>
<p>常见的迁移学习方式：</p>
<ul>
<li>载入权重后训练所有参数（准确率最高，需要参数最多，硬件要求最高，时间最长）</li>
<li>载入权重后只训练最后几层参数</li>
<li>载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本节课程讲解ResNet网络结构，针对残差结构做了详细讲解、讲解Batch Normalization标准化处理过程以及迁移学习内容介绍，下节课将会使用pytorch搭建ResNet网络结构。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之绪论</title>
    <url>/2023/04/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%BB%AA%E8%AE%BA/</url>
    <content><![CDATA[<h1 id="第一章绪论"><a href="#第一章绪论" class="headerlink" title="第一章绪论"></a>第一章绪论</h1><h2 id="第一节基本概念"><a href="#第一节基本概念" class="headerlink" title="第一节基本概念"></a>第一节基本概念</h2><p>数据：是信息的载体</p>
<p>数据对象：具有<strong>相同性质</strong>的数据元素的集合，是数据的一个子集</p>
<span id="more"></span>
<p>数据元素：是数据的<strong>基本单位</strong>，通常作为一个整体进行考虑和处理</p>
<p>数据项：是构成数据元素的不可分割的<strong>最小单位</strong></p>
<p>数据结构：是指相互之间存在<strong>一种或者多种特定关系</strong>的数据元素的集合</p>
<p>数据类型：原子类型（bool，int…）、结构类型（struct Coordinate{    int x; int y;}; ）</p>
<p>抽象数据类型（ADT）：抽象数据组织及与之相关的操作</p>
<h2 id="第二节三要素"><a href="#第二节三要素" class="headerlink" title="第二节三要素"></a>第二节三要素</h2><h3 id="1-逻辑结构"><a href="#1-逻辑结构" class="headerlink" title="1.逻辑结构"></a>1.逻辑结构</h3><p>集合结构：各个元素同属于一个集合，并无其他关系</p>
<p>线性结构：一对一关系，除第一个元素，所有元素都有唯一前驱；除最后一个元素，所有元素都有唯一后继</p>
<p>树形结构：一对多关系，例：思维导图、文件夹</p>
<p>网状结构：多对多关系，例：道路信息，朋友圈关系</p>
<h3 id="2-数据的运算"><a href="#2-数据的运算" class="headerlink" title="2.数据的运算"></a>2.数据的运算</h3><p>——针对某种逻辑结构，结合实际需求，定义基本运算</p>
<h3 id="3-物理结构（存储结构）"><a href="#3-物理结构（存储结构）" class="headerlink" title="3.物理结构（存储结构）"></a>3.物理结构（存储结构）</h3><p>——如何用计算机表示数据元素的逻辑关系</p>
<p>顺序存储：把<strong>逻辑上相邻的元素存储在物理位置上也相邻的存储单元</strong>中，元素之间的关系由存储单元的邻接关系来体现</p>
<p>链式存储：<strong>逻辑上相邻的元素在物理位置上可以不相邻</strong>，借助指示元素存储地址的<strong>指针</strong>来表示元素之间的逻辑关系</p>
<p>索引存储：在存储元素信息的同时，还建立附加的索引表。索引表中的每项称为索引项，索引项的一般形式是（关键字，地址）</p>
<p>散列存储：根据元素的关键字直接计算出该元素的存储地址，又称哈希（Hash）存储</p>
<p><em>小结：</em></p>
<p><em>1）若采用<strong>顺序存储</strong>，则各个数据元素在物理上必须是<strong>连续的</strong>；若采用<strong>非顺序存储</strong>，则各个数据元素在物理上可以是<strong>离散的</strong></em></p>
<p><em>2）数据的<strong>存储结构</strong>会<strong>影响存储空间分配的方便程度</strong></em></p>
<p><em>3）数据的<strong>存储结构</strong>会<strong>影响对数据运算的速度</strong>，例：在元素之间插入新元素</em></p>
<h2 id="第三节算法"><a href="#第三节算法" class="headerlink" title="第三节算法"></a>第三节算法</h2><p>——是对特定问题求解步骤的一种描述</p>
<p>五个特性：有穷性、确定性、可行性、输入、输出</p>
<p>“好”算法的特性：正确性、可读性、健壮性、高效率和低存储量需求</p>
<h3 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1.时间复杂度"></a>1.时间复杂度</h3><p>——时间开销与问题规模n之间的关系</p>
<h3 id="2-空间复杂度"><a href="#2-空间复杂度" class="headerlink" title="2.空间复杂度"></a>2.空间复杂度</h3><p>——空间开销（内存开销）与问题规模n之间的关系</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之树和二叉树</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B8%83%E7%AB%A0%E6%95%B0%E5%92%8C%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    <content><![CDATA[<h1 id="第七章树和二叉树"><a href="#第七章树和二叉树" class="headerlink" title="第七章树和二叉树"></a>第七章树和二叉树</h1><h2 id="第一节树的概念"><a href="#第一节树的概念" class="headerlink" title="第一节树的概念"></a>第一节树的概念</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>​    是n（n≥0）个节点的有限集。当n=0时，称为空树。在任意一棵非空树中应满足：</p>
<ul>
<li>有且仅有一个特定的称为根的结点；</li>
<li>当n&gt;1时，其余结点可分为m（m＞0）个互不相交的有限集，其中每个集合本身又是一棵树，并且成为根的子树</li>
</ul>
<span id="more"></span>
<h3 id="2-特点"><a href="#2-特点" class="headerlink" title="2.特点"></a>2.特点</h3><ul>
<li>树的根结点没有前驱，除根节点外的所有结点有且只有一个前驱；</li>
<li>树中所有结点可以由零个或多个后继</li>
</ul>
<h3 id="3-基本性质"><a href="#3-基本性质" class="headerlink" title="3.基本性质"></a>3.基本性质</h3><p>\begin{flalign}<br>&amp;1)树中的结点数等于所有结点的度数之和+1&amp;&amp;\\<br>&amp;2)度为m的树中第i层上至多有m^{i-1}个结点（i≥1）\\<br>&amp;3)高度为h的m叉树至多有(m^h-1)/(m-1)个结点\\<br>&amp;4)n结点m叉树最小高度为\lceil log_m[n(m-1)+1] \rceil \\<br>\end{flalign}</p>
<h3 id="4-存储结构"><a href="#4-存储结构" class="headerlink" title="4.存储结构"></a>4.存储结构</h3><p>（1）双亲表示法</p>
<pre class="mermaid">graph TB;
R((R))
A((A))
B((B))
C((C))
D((D))
E((E))
F((F))
G((G))
H((H))
K((K))
R-->A
R-->B
R-->C
A-->D
A-->E
C-->F
F-->G
F-->H
F-->K</pre>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">data</th>
<th style="text-align:center">parent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">R</td>
<td style="text-align:center">-1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">A</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">B</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">C</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">D</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">E</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">F</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">G</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">H</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">K</td>
<td style="text-align:center">6</td>
</tr>
</tbody>
</table>
</div>
<p>（2）孩子表示法</p>
<p>将每个节点的孩子结点都用单链表链接起来形成一个线性结构，此时n个结点就有n个孩子链表（叶子结点链表为空表）；</p>
<p>这种存储方式寻找子女的操作非常直接，而寻找双亲的操作需要遍历n个结点中孩子链表指针域所指向的n个孩子链表。</p>
<p>（3）孩子兄弟表示法</p>
<p>优点：方便地实现树转换为二叉树的操作，易于查找结点的孩子等；</p>
<p>缺点：从当前结点查找其双亲结点比较麻烦</p>
<p>①在兄弟结点之间加一条线；</p>
<p>②对每个结点，只保留它与i一个孩子的连线，而与其他孩子 连线全部抹掉；</p>
<p>③以树根为轴心，顺时针旋转45°。</p>
<h3 id="5-树、森林与二叉树遍历的对应关系"><a href="#5-树、森林与二叉树遍历的对应关系" class="headerlink" title="5.树、森林与二叉树遍历的对应关系"></a>5.树、森林与二叉树遍历的对应关系</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">树</th>
<th style="text-align:center">森林</th>
<th style="text-align:center">二叉树</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">先根遍历</td>
<td style="text-align:center">先序遍历</td>
<td style="text-align:center">先序遍历</td>
</tr>
<tr>
<td style="text-align:center">后根遍历</td>
<td style="text-align:center">中序遍历</td>
<td style="text-align:center">中序遍历</td>
</tr>
</tbody>
</table>
</div>
<h2 id="第二节二叉树"><a href="#第二节二叉树" class="headerlink" title="第二节二叉树"></a>第二节二叉树</h2><h3 id="1-概念和性质"><a href="#1-概念和性质" class="headerlink" title="1.概念和性质"></a>1.概念和性质</h3><h4 id="（1）概念"><a href="#（1）概念" class="headerlink" title="（1）概念"></a>（1）概念</h4><p>​     一棵二叉树是结点的一个有限集合，该集合：</p>
<ul>
<li>​    或者为空；</li>
<li>​    由一个根节点加上两棵别称为左子树和右子树的二叉树组成</li>
</ul>
<p>满二叉树：一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。</p>
<p>完全二叉树：效率很高的数据结构，由满二叉树而引出来的。是一种特殊的完全二叉树。</p>
<p>二叉排序树：左&lt;中&lt;右.</p>
<p>平衡二叉树：左子树与右子树深度之差不超过1。</p>
<h4 id="（2）性质"><a href="#（2）性质" class="headerlink" title="（2）性质"></a>（2）性质</h4><p>\begin{flalign}<br>&amp;1)n_0 = n_2 +1\\<br>&amp;2)非空二叉树上第k层至多有2^{k-1}个结点（k≥1）\\<br>&amp;3)高度为h的二叉树至少有2^h-1个结点（h≥1）\\<br>&amp;4)对完全二叉树按向上到下、从左到右的顺序依次编号1,2,…,n，则有以下关系：\\<br>&amp;~①当i&gt;1时，结点i的双亲的编号是\lfloor i/2 \rfloor，即当i为偶数时，其双亲的编号是i/2，它是双亲的左孩子。当i为奇数时，其双亲的编号是（i-1）/2，它是双亲的右孩子。\\<br>&amp;~②当2i≤n时，结点i的左孩子编号为2i，否则无左孩子。\\<br>&amp;~③当2i+1≤n时，结点i的右孩子编号为2i+1，否则无右孩子。\\<br>&amp;~④结点i所在层次（深度）为\lfloor log_2i \rfloor+1。\\<br>&amp;5)具有n个（n&gt;0）结点的完全二叉树的高度为\lceil log_2(n+1) \rceil 或\lfloor log_2n \rfloor +1。<br>\end{flalign}</p>
<h3 id="2-存储结构和基本算法"><a href="#2-存储结构和基本算法" class="headerlink" title="2.存储结构和基本算法"></a>2.存储结构和基本算法</h3><p>（1）顺序存储结构</p>
<p>（2）链式存储结构</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">lchild</th>
<th style="text-align:center">data</th>
<th style="text-align:center">rchild</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">BiTree</span>&#123;</span></span><br><span class="line">	ElemType data;  <span class="comment">// 数据域</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">BiTree</span> *<span class="title">lchile</span>,*<span class="title">rchild</span>;</span> <span class="comment">// 左、右孩子指针</span></span><br><span class="line">&#125;BiTNode,*BiTree;</span><br></pre></td></tr></table></figure>
<h2 id="第三节二叉树的遍历算法"><a href="#第三节二叉树的遍历算法" class="headerlink" title="第三节二叉树的遍历算法"></a>第三节二叉树的遍历算法</h2><h3 id="1-先序遍历"><a href="#1-先序遍历" class="headerlink" title="1.先序遍历"></a>1.先序遍历</h3><p>递归算法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">PreOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T)&#123;</span><br><span class="line">        <span class="built_in">visit</span>(T);		<span class="comment">//访问根结点</span></span><br><span class="line">        <span class="built_in">PreOrder</span>(T-&gt;lchild);	<span class="comment">//递归遍历左子树</span></span><br><span class="line">        <span class="built_in">PreOrder</span>(T-&gt;rchild);	<span class="comment">//递归遍历右子树</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>非递归算法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">PreOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="built_in">InitStack</span>(S);	BiTree p = T;	<span class="comment">//初始化栈；p是遍历指针</span></span><br><span class="line">    <span class="keyword">while</span>(p||!<span class="built_in">IsEmpty</span>(S))&#123;		<span class="comment">//栈不空或p不空时一直循环</span></span><br><span class="line">        <span class="keyword">if</span>(p)&#123;		<span class="comment">//一路向左</span></span><br><span class="line">            <span class="built_in">visit</span>(p);	<span class="built_in">Push</span>(S,p);	<span class="comment">//访问当前结点，并入栈</span></span><br><span class="line">            p = p-&gt;lchild;		<span class="comment">//左孩子不空，一直向左走</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;		<span class="comment">//出栈，并转向出栈结点的右子树</span></span><br><span class="line">            <span class="built_in">Pop</span>(S,p);				<span class="comment">//栈顶元素出栈</span></span><br><span class="line">            p = p-&gt;rchild;			<span class="comment">//向右子树走，p赋值为当前结点的右孩子</span></span><br><span class="line">        &#125;<span class="comment">//else</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-中序遍历"><a href="#2-中序遍历" class="headerlink" title="2.中序遍历"></a>2.中序遍历</h3><p>递归算法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">InOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T)&#123;</span><br><span class="line">        <span class="built_in">InOrder</span>(T-&gt;lchild);	<span class="comment">//递归遍历左子树</span></span><br><span class="line">        <span class="built_in">visit</span>(T);		<span class="comment">//访问根结点</span></span><br><span class="line">        <span class="built_in">InOrder</span>(T-&gt;rchild);	<span class="comment">//递归遍历右子树</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>非递归算法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">InOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="built_in">InitStack</span>(S);	BiTree p = T;	<span class="comment">//初始化栈；p是遍历指针</span></span><br><span class="line">    <span class="keyword">while</span>(p||!<span class="built_in">IsEmpty</span>(S))&#123;		<span class="comment">//栈不空或p不空时一直循环</span></span><br><span class="line">        <span class="keyword">if</span>(p)&#123;		<span class="comment">//一路向左</span></span><br><span class="line">            <span class="built_in">Push</span>(S,p);			<span class="comment">//当前结点入栈</span></span><br><span class="line">            p = p-&gt;lchild;		<span class="comment">//左孩子不空，一直向左走</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;		<span class="comment">//出栈，并转向出栈结点的右子树</span></span><br><span class="line">            <span class="built_in">Pop</span>(S,p);	<span class="built_in">visit</span>(p);	<span class="comment">//栈顶元素出栈，p赋值为当前结点的右孩子</span></span><br><span class="line">            p = p-&gt;rchild;			<span class="comment">//向右子树走，p赋值为当前结点的右孩子</span></span><br><span class="line">        &#125;<span class="comment">//else</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-后序遍历"><a href="#3-后序遍历" class="headerlink" title="3.后序遍历"></a>3.后序遍历</h3><p>递归算法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">PostOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T)&#123;</span><br><span class="line">       	<span class="built_in">PostOrder</span>(T-&gt;lchild);	<span class="comment">//递归遍历左子树</span></span><br><span class="line">        <span class="built_in">PostOrder</span>(T-&gt;rchild);	<span class="comment">//递归遍历右子树</span></span><br><span class="line">        <span class="built_in">visit</span>(T);		<span class="comment">//访问根结点</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>非递归算法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">PostOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="built_in">InitStack</span>(S);</span><br><span class="line">    BiTree p = T,r = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">while</span>(p||!<span class="built_in">IsEmpty</span>(S))&#123;</span><br><span class="line">        <span class="keyword">if</span>(p)&#123;		<span class="comment">//走到最左边</span></span><br><span class="line">            <span class="built_in">Push</span>(S,p);			<span class="comment">//当前结点入栈</span></span><br><span class="line">            p = p-&gt;lchild;		<span class="comment">//左孩子不空，一直向左走</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;		<span class="comment">//向右</span></span><br><span class="line">            <span class="built_in">GetTop</span>(S,p);		<span class="comment">//读栈顶结点（非出栈）</span></span><br><span class="line">            <span class="keyword">if</span>(p-&gt;rchild&amp;&amp;p-&gt;rchild!=r)		<span class="comment">//若右子树存在，且未被访问过</span></span><br><span class="line">                p = p-&gt;rchild;		<span class="comment">//转向右</span></span><br><span class="line">            <span class="keyword">else</span>&#123;	<span class="comment">//否则，弹出结点并访问</span></span><br><span class="line">                <span class="built_in">Pop</span>(S,p);			 <span class="comment">//将结点弹出</span></span><br><span class="line">                <span class="built_in">visit</span>(p-&gt;data);		 <span class="comment">//访问该结点</span></span><br><span class="line">                r = p;				<span class="comment">//记录最近访问过的结点</span></span><br><span class="line">                p = <span class="literal">NULL</span>;			<span class="comment">//结点访问完后，重置p指针</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="comment">//else</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-层次遍历"><a href="#4-层次遍历" class="headerlink" title="4.层次遍历"></a>4.层次遍历</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LevelOrder</span><span class="params">(BiTree T)</span></span>&#123;</span><br><span class="line">    <span class="built_in">InitQueue</span>(Q);				<span class="comment">//初始化辅助队列</span></span><br><span class="line">    BiTree p;</span><br><span class="line">    <span class="built_in">EnQueue</span>(Q,T);				<span class="comment">//将根结点入队</span></span><br><span class="line">    <span class="keyword">while</span>(!<span class="built_in">IsEmpty</span>(Q))&#123;			<span class="comment">//队列不空则循环</span></span><br><span class="line">        <span class="built_in">DeQueue</span>(Q,p);			<span class="comment">//队头结点出队</span></span><br><span class="line">        <span class="built_in">visit</span>(p);				<span class="comment">//访问出队结点</span></span><br><span class="line">        <span class="keyword">if</span>(p-&gt;lchild!=<span class="literal">NULL</span>)</span><br><span class="line">            <span class="built_in">EnQueue</span>(Q,p-&gt;lchild);	<span class="comment">//左子树不空，则左子树根结点入队</span></span><br><span class="line">        <span class="keyword">if</span>(p-rchild!=<span class="literal">NULL</span>)</span><br><span class="line">            <span class="built_in">EnQueue</span>(Q,p-&gt;rchild);	<span class="comment">//右子树不空，则右子树根结点入队</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第四节线索二叉树"><a href="#第四节线索二叉树" class="headerlink" title="第四节线索二叉树"></a>第四节线索二叉树</h2><p>是以一定的规则将二叉树中的结点排列成一个线性序列，从而得到集中遍历序列，使得该序列中的每个结点（第一个和最后一个结点除外）都有一个直接前去和直接后继。</p>
<p>目的：为了加快查找结点前驱和后继的速度。</p>
<p>空指针 = 线索数 = n+1</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">lchild</th>
<th style="text-align:center">ltag</th>
<th style="text-align:center">data</th>
<th style="text-align:center">rtag</th>
<th style="text-align:center">rchild</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p>ltag</p>
<ul>
<li>​    0    lchild域指示结点的左孩子</li>
<li>​    1    lchild域指示结点的前驱</li>
</ul>
<p>rtag</p>
<ul>
<li>​    0    rchild域指示结点的右孩子</li>
<li>​    1    rchild域指示结点的前驱</li>
</ul>
<p>存储结构</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">ThreadNode</span>&#123;</span><br><span class="line">    ElemType data;						<span class="comment">//数据元素</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ThreadNode</span> *lchild,*rchild;		<span class="comment">//左、右孩子指针</span></span><br><span class="line">    <span class="type">int</span> ltag,rtag;						<span class="comment">//左、右线索标志</span></span><br><span class="line">&#125;ThreadNode,*ThreadTree;</span><br></pre></td></tr></table></figure>
<h2 id="第五节哈夫曼树"><a href="#第五节哈夫曼树" class="headerlink" title="第五节哈夫曼树"></a>第五节哈夫曼树</h2><h3 id="1-概念-1"><a href="#1-概念-1" class="headerlink" title="1.概念"></a>1.概念</h3><p>树中所有叶结点的带权路径长度之和称为该树的带权路径长度，记作：</p>
<script type="math/tex; mode=display">
WPL = \sum_{i=1}^nw_il_i</script><p>举例如下哈夫曼树，WPL = (4+2)<em>3+5</em>2+7*1 = 35</p>
<pre class="mermaid">graph TB;
A((A))
B((B))
C((C))
D((D:7))
E((E:5))
F((F:2))
G((G:4))
A-->D
A-->B
B-->C
B-->E
C-->G
C-->F</pre>

<h3 id="2-编码问题（左0右1）"><a href="#2-编码问题（左0右1）" class="headerlink" title="2.编码问题（左0右1）"></a>2.编码问题（左0右1）</h3><pre class="mermaid">graph TB;
A((A))
B((B))
C((C))
D((D:7))
E((E:5))
F((F:2))
G((G:4))
A--1-->D
A--0-->B
B--0-->C
B--1-->E
C--0-->G
C--1-->F</pre>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">D</th>
<th style="text-align:center">E</th>
<th style="text-align:center">F</th>
<th style="text-align:center">G</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">01</td>
<td style="text-align:center">001</td>
<td style="text-align:center">000</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之栈与队列</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97/</url>
    <content><![CDATA[<h1 id="第三章栈与队列"><a href="#第三章栈与队列" class="headerlink" title="第三章栈与队列"></a>第三章栈与队列</h1><h2 id="第三节堆栈和队列的应用"><a href="#第三节堆栈和队列的应用" class="headerlink" title="第三节堆栈和队列的应用"></a>第三节堆栈和队列的应用</h2><span id="more"></span>
<h3 id="1-表达式求值"><a href="#1-表达式求值" class="headerlink" title="1.表达式求值"></a>1.表达式求值</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OK 1</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ERROR 0</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">char</span> SElemType;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> Status;</span><br><span class="line"><span class="comment">//链栈存储结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">    SElemType data;</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">Node</span>* next;</span><br><span class="line">&#125;Node, * LinkStack;</span><br><span class="line"></span><br><span class="line"><span class="comment">//链栈初始化</span></span><br><span class="line"><span class="function">Status <span class="title">InitStack</span><span class="params">(LinkStack &amp;S)</span> </span>&#123;   <span class="comment">//构造空栈，栈顶指针置空</span></span><br><span class="line">    S = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//链栈入栈</span></span><br><span class="line"><span class="function">Status <span class="title">Push</span><span class="params">(LinkStack &amp;S, SElemType e)</span> </span>&#123;<span class="comment">//栈顶插入元素e</span></span><br><span class="line">    LinkStack p = <span class="keyword">new</span> Node;    <span class="comment">//生成新节点</span></span><br><span class="line">    p-&gt;data = e;   <span class="comment">//新节点数据域置为e</span></span><br><span class="line">    p-&gt;next = S;   <span class="comment">//将新节点插入栈顶</span></span><br><span class="line">    S = p; <span class="comment">//修改栈顶指针为p</span></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//链栈的出栈</span></span><br><span class="line"><span class="function">Status <span class="title">Pop</span><span class="params">(LinkStack &amp;S, SElemType &amp;e)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (S == <span class="literal">NULL</span>) <span class="comment">//栈为空</span></span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    LinkStack p;   <span class="comment">//定义临时链栈</span></span><br><span class="line">    e = S-&gt;data;   <span class="comment">//将栈顶元素赋值给e</span></span><br><span class="line">    p = S; <span class="comment">//临时存储栈顶空间，以备释放</span></span><br><span class="line">    S = S-&gt;next;   <span class="comment">//修改栈顶指针</span></span><br><span class="line">    <span class="keyword">delete</span> p;  <span class="comment">//删除栈顶指针</span></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//取栈顶元素</span></span><br><span class="line"><span class="function">SElemType <span class="title">GetTop</span><span class="params">(LinkStack S)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (S != <span class="literal">NULL</span>) <span class="comment">//栈非空</span></span><br><span class="line">        <span class="keyword">return</span> S-&gt;data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断读入的字符ch是否为运算符</span></span><br><span class="line"><span class="function">Status <span class="title">In</span><span class="params">(SElemType ch)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (ch == <span class="string">&#x27;+&#x27;</span> || ch == <span class="string">&#x27;-&#x27;</span> || ch == <span class="string">&#x27;*&#x27;</span> || ch == <span class="string">&#x27;/&#x27;</span> || ch == <span class="string">&#x27;(&#x27;</span> || ch == <span class="string">&#x27;)&#x27;</span>||ch==<span class="string">&#x27;#&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> OK;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断运算符栈顶元素与读入运算符ch的优先级</span></span><br><span class="line"><span class="function">SElemType <span class="title">Precede</span><span class="params">(SElemType a, SElemType b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a == <span class="string">&#x27;+&#x27;</span> || a == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b == <span class="string">&#x27;+&#x27;</span> || b == <span class="string">&#x27;-&#x27;</span> || b == <span class="string">&#x27;#&#x27;</span> || b == <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&gt;&#x27;</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&lt;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (a == <span class="string">&#x27;*&#x27;</span> || a == <span class="string">&#x27;/&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b == <span class="string">&#x27;(&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&lt;&#x27;</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&gt;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (a == <span class="string">&#x27;(&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b == <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;=&#x27;</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&lt;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (a == <span class="string">&#x27;#&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b == <span class="string">&#x27;#&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;=&#x27;</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&lt;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (a == <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&gt;&#x27;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//运算函数</span></span><br><span class="line"><span class="function">SElemType <span class="title">Operate</span><span class="params">(SElemType a, SElemType t, SElemType b)</span> </span>&#123;<span class="comment">//进行运算的函数</span></span><br><span class="line">    <span class="keyword">switch</span> (t) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;+&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> a + b;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;-&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> a - b;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> a * b;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;/&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> a / b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//算法表达式求值的优先算法，设OPTR和OPND分别为运算符栈和操作数栈</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">EvaluateExpression</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    LinkStack OPND, OPTR;  <span class="comment">//定义栈</span></span><br><span class="line">    SElemType ch, t,t1,t2;</span><br><span class="line">    <span class="built_in">InitStack</span>(OPND);   <span class="comment">//初始化操作数栈</span></span><br><span class="line">    <span class="built_in">InitStack</span>(OPTR);   <span class="comment">//初始化运算符栈</span></span><br><span class="line">    <span class="built_in">Push</span>(OPTR,<span class="string">&#x27;#&#x27;</span>);    <span class="comment">//入栈</span></span><br><span class="line">    cin &gt;&gt; ch;</span><br><span class="line">    <span class="keyword">while</span> (ch != <span class="string">&#x27;#&#x27;</span> || <span class="built_in">GetTop</span>(OPTR) != <span class="string">&#x27;#&#x27;</span>) &#123;<span class="comment">//表达式没有扫描完毕或者OPTR栈顶不为‘#’时执行</span></span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">In</span>(ch)) &#123; <span class="comment">//ch不是运算符</span></span><br><span class="line">            <span class="built_in">Push</span>(OPND, ch-<span class="string">&#x27;0&#x27;</span>);    <span class="comment">//进入OPND栈</span></span><br><span class="line">            cin &gt;&gt; ch; <span class="comment">//读取下一个字符</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">switch</span> (<span class="built_in">Precede</span>(<span class="built_in">GetTop</span>(OPTR), ch)) &#123;   <span class="comment">//比较OPTR栈顶元素和ch的优先级</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;&lt;&#x27;</span>:</span><br><span class="line">                    <span class="built_in">Push</span>(OPTR, ch);    <span class="comment">//将ch压入OPTR栈</span></span><br><span class="line">                    cin &gt;&gt; ch; <span class="comment">//读取下一个字符</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;&gt;&#x27;</span>:</span><br><span class="line">                    <span class="built_in">Pop</span>(OPTR, t);  <span class="comment">//弹出OPTR栈顶运算符，赋值给t</span></span><br><span class="line">                    <span class="built_in">Pop</span>(OPND, t2); <span class="comment">//后进先出</span></span><br><span class="line">                    <span class="built_in">Pop</span>(OPND, t1); <span class="comment">//弹出OPND栈顶两个运算数</span></span><br><span class="line">                    <span class="built_in">Push</span>(OPND, <span class="built_in">Operate</span>(t1, t, t2));    <span class="comment">//将运算结果压入OPND栈</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;=&#x27;</span>:  <span class="comment">//OPTR栈顶元素是&#x27;(&#x27;且ch是&#x27;)&#x27;</span></span><br><span class="line">                    <span class="built_in">Pop</span>(OPTR, t);  <span class="comment">//弹出栈顶&#x27;(&#x27;</span></span><br><span class="line">                    cin &gt;&gt; ch; <span class="comment">//读取下一个字符</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;(<span class="type">int</span>)<span class="built_in">GetTop</span>(OPND)&lt;&lt;endl; <span class="comment">//将char转换为int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">EvaluateExpression</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-括号匹配问题"><a href="#2-括号匹配问题" class="headerlink" title="2.括号匹配问题"></a>2.括号匹配问题</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义栈</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> max_size 200<span class="comment">//栈的最大容量</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">char</span> datatype;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    datatype zhan[max_size];</span><br><span class="line">    <span class="type">int</span> top;<span class="comment">//栈顶</span></span><br><span class="line">&#125;stack;</span><br><span class="line"></span><br><span class="line"><span class="comment">//栈的初始化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initial</span><span class="params">(stack &amp;st)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    st.top = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//类型为datatype的x入栈</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(stack &amp;st, datatype x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//当栈顶和max_size相等时，栈满</span></span><br><span class="line">    <span class="keyword">if</span>(st.top == max_size)&#123;</span><br><span class="line">        <span class="comment">// cout&lt;&lt;&quot;This stack has already full!&quot;;</span></span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;no&quot;</span>;</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        st.zhan[st.top] = x;</span><br><span class="line">        st.top++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//出栈</span></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">pop</span><span class="params">(stack &amp;st)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(st.top == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="comment">// cout&lt;&lt;&quot;This stack is empty!&quot;;</span></span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;no&quot;</span>;</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        st.top--;</span><br><span class="line">        <span class="keyword">return</span> st.zhan[st.top];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    stack s;</span><br><span class="line">    <span class="built_in">initial</span>(s);</span><br><span class="line">    <span class="comment">/*输入字符串，并将字符串放到字符数组中，</span></span><br><span class="line"><span class="comment">    实现能够逐个扫描字符串中的字符，并且不跳过空格符*/</span></span><br><span class="line">    string str;</span><br><span class="line">    <span class="built_in">getline</span>(cin, str);</span><br><span class="line">    <span class="type">char</span> ch[<span class="number">200</span>]=&#123;<span class="string">&#x27;\0&#x27;</span>&#125;;</span><br><span class="line">    <span class="built_in">strcpy</span>(ch,str.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="comment">//flag标志状态 1为括号匹配，0为不匹配</span></span><br><span class="line">    <span class="type">int</span> flag=<span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>; ch[i]!=<span class="string">&#x27;\0&#x27;</span>; i++)&#123;</span><br><span class="line">        <span class="comment">//元素若为&#123;，(，[则入栈</span></span><br><span class="line">        <span class="keyword">if</span>((ch[i] == <span class="string">&#x27;&#123;&#x27;</span> )|| (ch[i] ==<span class="string">&#x27;[&#x27;</span>) || (ch[i] ==<span class="string">&#x27;(&#x27;</span>))&#123;</span><br><span class="line">            <span class="built_in">push</span>(s, ch[i]);</span><br><span class="line">        &#125;<span class="comment">//元素若为&#125;，)，]则出栈 赋值给a</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>((ch[i] == <span class="string">&#x27;&#125;&#x27;</span>) || (ch[i] ==<span class="string">&#x27;]&#x27;</span>) || (ch[i] ==<span class="string">&#x27;)&#x27;</span>))&#123;</span><br><span class="line">            <span class="type">char</span> a;</span><br><span class="line">            a = <span class="built_in">pop</span>(s);</span><br><span class="line">            <span class="comment">//若a与ch[i]匹配，进行下一个字符扫描</span></span><br><span class="line">            <span class="keyword">if</span>((a == <span class="string">&#x27;&#123;&#x27;</span> &amp;&amp; ch[i] == <span class="string">&#x27;&#125;&#x27;</span>) || (a == <span class="string">&#x27;(&#x27;</span> &amp;&amp; ch[i] == <span class="string">&#x27;)&#x27;</span>) || (a == <span class="string">&#x27;[&#x27;</span> &amp;&amp; ch[i] == <span class="string">&#x27;]&#x27;</span>))&#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> flag = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(s.top != <span class="number">0</span>)&#123;    <span class="comment">//当左括号多出没有与右括号匹配的时候（如：&quot; &#123;() &quot;）</span></span><br><span class="line">        flag = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(flag == <span class="number">0</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;no&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;yes&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-迷宫问题（栈，深度检索）"><a href="#3-迷宫问题（栈，深度检索）" class="headerlink" title="3.迷宫问题（栈，深度检索）"></a>3.迷宫问题（栈，深度检索）</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ROW  6<span class="comment">//行</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> COL  6<span class="comment">//列</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//地图</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">_Maze</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> map[ROW][COL];</span><br><span class="line">&#125;Maze;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_SIZE 128</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">_Postion</span><span class="comment">//地图中点的坐标,这个栈中存的元素就是点的坐标</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> _x;</span><br><span class="line">    <span class="type">int</span> _y;</span><br><span class="line">&#125;Postion;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> Postion DataType;</span><br><span class="line"></span><br><span class="line"><span class="comment">//栈的结构体</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">_Stack</span></span><br><span class="line">&#123;</span><br><span class="line">    DataType* top;</span><br><span class="line">    DataType* base;</span><br><span class="line">&#125;Stack;</span><br><span class="line"></span><br><span class="line"><span class="comment">//栈的初始化</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">initStack</span><span class="params">(Stack&amp; S)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    S.base = <span class="keyword">new</span> DataType[MAX_SIZE];</span><br><span class="line">    <span class="keyword">if</span> (!S.base)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    S.top = S.base;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//入栈</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">pushStack</span><span class="params">(Stack&amp; S, DataType data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!S.base)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (S.top - S.base == MAX_SIZE)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    *(S.top) = data;</span><br><span class="line">    S.top++;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//出栈</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">popStack</span><span class="params">(Stack&amp; S,DataType&amp; e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (S.top == S.base)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    e = *(--S.top);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//返回栈顶元素</span></span><br><span class="line"><span class="function">DataType* <span class="title">getTop</span><span class="params">(Stack&amp; S)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (S.top - S.base == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="comment">//注意何时自增何时不自增</span></span><br><span class="line">    <span class="keyword">return</span> S.top<span class="number">-1</span>;<span class="comment">//返回栈顶元素的指针</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//返回栈中元素个数</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getSize</span><span class="params">(Stack&amp; S)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> S.top - S.base;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断栈是否为空</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isEmpty</span><span class="params">(Stack&amp; S)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (S.top == S.base)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//销毁栈</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">destoryStack</span><span class="params">(Stack&amp; S)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (S.base)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">delete</span>[] S.base;</span><br><span class="line">        S.top = S.base = <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据给出给出的地图数据初始化结构体地图</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initMaze</span><span class="params">(Maze&amp; m, <span class="type">int</span> map[ROW][COL])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ROW; i++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; COL; j++)</span><br><span class="line">            m.map[i][j] = map[i][j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//打印迷宫(地图)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">printMaze</span><span class="params">(Maze&amp; m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ROW; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; COL; j++) &#123;</span><br><span class="line">            cout &lt;&lt; m.map[i][j] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断是否是有效的入口</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isValidEnter</span><span class="params">(Maze* m,Postion enter)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(m);<span class="comment">//断言-里面的表达式为0直接终止程序,注意里面的内容是什么</span></span><br><span class="line">    <span class="comment">//只要入口在四个边界上就是合法的,并且是1(道路)</span></span><br><span class="line">    <span class="keyword">if</span> (((enter._x == <span class="number">0</span> || enter._x == ROW - <span class="number">1</span>) || (enter._y == <span class="number">0</span> || enter._y == COL - <span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断当前位置是否是出口</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isVaildExit</span><span class="params">(Maze* m, Postion cur, Postion enter)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(m);</span><br><span class="line">    <span class="comment">//该结点不能是入口点，除了入口点，在边界上就是合法出口</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((cur._x != enter._x || cur._y != enter._y) &amp;&amp; ((cur._x == <span class="number">0</span> || cur._x == ROW - <span class="number">1</span>) || (cur._y == <span class="number">0</span> || cur._y == COL - <span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断当前结点的下一个结点是否能走通-是不是可以走的点</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isNextPass</span><span class="params">(Maze* m, Postion cur, Postion next)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(m);</span><br><span class="line">    <span class="comment">//判断next是不是cur的下一个结点</span></span><br><span class="line">    <span class="comment">//同一行相邻或者同一列相邻</span></span><br><span class="line">    <span class="keyword">if</span> (((next._x == cur._x) &amp;&amp; ((next._y == cur._y + <span class="number">1</span>) || (next._y == cur._y - <span class="number">1</span>)))</span><br><span class="line">        || ((next._y == cur._y) &amp;&amp; ((next._x = cur._x + <span class="number">1</span>) || (next._x = cur._x - <span class="number">1</span>))))</span><br><span class="line">        <span class="comment">//确实是cur的下一个结点(相邻的 )</span></span><br><span class="line">        <span class="comment">//判断这个点是不是在迷宫里</span></span><br><span class="line">        <span class="comment">//合法坐标并且那个位置的值是1</span></span><br><span class="line">        <span class="keyword">if</span> (((next._x &gt;= <span class="number">0</span> &amp;&amp; next._x &lt; ROW) &amp;&amp; (next._y &gt;= <span class="number">0</span> &amp;&amp; next._y &lt; COL))</span><br><span class="line">            &amp;&amp; (m-&gt;map[next._x][next._y] == <span class="number">1</span>))</span><br><span class="line">            <span class="comment">//最后的参数==1，不仅仅是看是否是可以走的位置(道路是1)，</span></span><br><span class="line">            <span class="comment">//同时有了这个我们就不用倒着往往前走了(不走重复的路)，不是有效的结点不只是墙(0)</span></span><br><span class="line">            <span class="comment">//走过的也不是有效结点，直接pop出栈，通过出栈来往前回退</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//寻找迷宫通路</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">PassMaze</span><span class="params">(Maze* m, Postion enter, Stack&amp; s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(m &amp;&amp; <span class="built_in">isValidEnter</span>(m, enter));</span><br><span class="line"></span><br><span class="line">    Postion cur = enter;<span class="comment">//cur存储当前结点</span></span><br><span class="line">    Postion next;<span class="comment">//下一个结点，从入口开始出发向四周移动</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//先将入口压入栈中</span></span><br><span class="line">    <span class="built_in">pushStack</span>(s, cur);</span><br><span class="line">    m-&gt;map[cur._x][cur._y] = <span class="number">2</span>;<span class="comment">//将入口值改为2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//循环求解-当栈中还有路径时</span></span><br><span class="line">    <span class="keyword">while</span> (!<span class="built_in">isEmpty</span>(s))</span><br><span class="line">    &#123;</span><br><span class="line">        cur = *<span class="built_in">getTop</span>(s);<span class="comment">//取到栈顶元素</span></span><br><span class="line">        <span class="comment">//判断当前位置是否是出口</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isVaildExit</span>(m, cur, enter))<span class="comment">//注意参数传递顺序</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;<span class="comment">//是出口直接返回</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//不是出口继续在周围判断</span></span><br><span class="line">        <span class="comment">//把cur当前刚才那个位置拿过来向四周判断</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//先向左判断</span></span><br><span class="line">        next = cur;</span><br><span class="line">        next._y = cur._y - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isNextPass</span>(m,cur,next))<span class="comment">//如果下一个结点走得通</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//走得通就走到那个位置-压进栈</span></span><br><span class="line">            <span class="built_in">pushStack</span>(s, next);</span><br><span class="line">            <span class="comment">//走过的位置-标记</span></span><br><span class="line">            m-&gt;map[next._x][next._y] = m-&gt;map[cur._x][cur._y] + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//之后</span></span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//走不通向另一个方向判断</span></span><br><span class="line">        <span class="comment">//向右走一步</span></span><br><span class="line">        next = cur;</span><br><span class="line">        next._y = cur._y + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isNextPass</span>(m, cur, next))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">pushStack</span>(s, next);</span><br><span class="line">            m-&gt;map[next._x][next._y] = m-&gt;map[cur._x][cur._y] + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//向下走一步</span></span><br><span class="line">        next = cur;</span><br><span class="line">        next._x = cur._x + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isNextPass</span>(m, cur, next))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">pushStack</span>(s, next);</span><br><span class="line">            m-&gt;map[next._x][next._y] = m-&gt;map[cur._x][cur._y] + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//向上走一步</span></span><br><span class="line">        next = cur;</span><br><span class="line">        next._x = cur._x - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isNextPass</span>(m, cur, next))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">pushStack</span>(s, next);</span><br><span class="line">            m-&gt;map[next._x][next._y] = m-&gt;map[cur._x][cur._y] + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//走到这里说明此结点的四个方向都走不通</span></span><br><span class="line">        <span class="comment">//进行回溯</span></span><br><span class="line">        Postion tmp;<span class="comment">//没用 临时接收</span></span><br><span class="line">        <span class="built_in">popStack</span>(s, tmp);<span class="comment">//出栈</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//0-墙 1-路</span></span><br><span class="line">    <span class="type">int</span> map[ROW][COL] = &#123;</span><br><span class="line">            <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">            <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,</span><br><span class="line">            <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">            <span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,</span><br><span class="line">            <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,</span><br><span class="line">            <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    Maze m;<span class="comment">//创建一个迷宫(地图)</span></span><br><span class="line">    <span class="built_in">initMaze</span>(m, map);<span class="comment">//初始化迷宫</span></span><br><span class="line">    <span class="built_in">printMaze</span>(m);<span class="comment">//打印迷宫</span></span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;_______&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//迷宫入口</span></span><br><span class="line">    Postion enter;</span><br><span class="line">    enter._x = <span class="number">0</span>;</span><br><span class="line">    enter._y = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义栈</span></span><br><span class="line">    Stack s;<span class="comment">//用于保存走过的轨迹，便于回溯</span></span><br><span class="line">    <span class="built_in">initStack</span>(s);<span class="comment">//初始化栈</span></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> ret = <span class="built_in">PassMaze</span>(&amp;m, enter, s);</span><br><span class="line">    <span class="keyword">if</span> (ret)</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;有解&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;无解&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">printMaze</span>(m);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-调度问题"><a href="#4-调度问题" class="headerlink" title="4.调度问题"></a>4.调度问题</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INF=<span class="number">1000</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> max_task=<span class="number">10</span>;            	<span class="comment">//最大任务数</span></span><br><span class="line"><span class="type">int</span> n;                           	<span class="comment">//任务数</span></span><br><span class="line"><span class="type">int</span> a[max_task][<span class="number">2</span>];             	<span class="comment">//存储每个作业分别在机器1与机器2上的时间消耗</span></span><br><span class="line"><span class="type">int</span> result[max_task];          		<span class="comment">//存储排列树中的一条路径</span></span><br><span class="line"><span class="type">int</span> best_result[max_task];    		<span class="comment">//存储最优路径</span></span><br><span class="line"><span class="type">int</span> min=INF;                 		<span class="comment">//安排任务最小完成时间</span></span><br><span class="line"><span class="type">int</span> rop[max_task+<span class="number">1</span>];        		<span class="comment">//每个深度对应遍历进度</span></span><br><span class="line"><span class="type">int</span> finish[max_task+<span class="number">1</span>];    			<span class="comment">//每一层消耗时间和</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">swap</span><span class="params">(<span class="type">int</span> result[max_task],<span class="type">int</span> best_result[max_task])</span>   <span class="comment">//复制数组</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">        best_result[i]=result[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">allot_task</span><span class="params">(<span class="type">int</span> depth)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">bool</span> visited[max_task];    	<span class="comment">//标记已安排任务</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=rop[depth];i&lt;n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(!visited[i])                 <span class="comment">//寻找该层未访问节点</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> f1=<span class="number">0</span>;                 	<span class="comment">//在机器一上的完成时间</span></span><br><span class="line">            <span class="type">int</span> f2=<span class="number">0</span>;                  	<span class="comment">//在机器二上的完成时间</span></span><br><span class="line">            rop[depth]=i+<span class="number">1</span>;           	<span class="comment">//更新该层访问进度</span></span><br><span class="line">            visited[i]=<span class="literal">true</span>;         	<span class="comment">//标记作业</span></span><br><span class="line">            result[depth<span class="number">-1</span>]=i;        	<span class="comment">//加入结果序列</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(visited[j])</span><br><span class="line">                &#123;</span><br><span class="line">                    f1+=a[j][<span class="number">0</span>];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            f2=f1+a[i][<span class="number">1</span>];            	<span class="comment">//该作业完成时间</span></span><br><span class="line">            <span class="keyword">if</span>(f2&lt;finish[depth<span class="number">-1</span>])    	<span class="comment">//该序列执行到此作业消耗时间</span></span><br><span class="line">                f2=finish[depth<span class="number">-1</span>];</span><br><span class="line">            finish[depth]=f2;</span><br><span class="line">            <span class="keyword">if</span>(depth==n)             	<span class="comment">//最后一个作业</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(f2&lt;min)</span><br><span class="line">                &#123;</span><br><span class="line">                    min=f2;</span><br><span class="line">                    swap(result,best_result);      <span class="comment">//保存最优序列</span></span><br><span class="line">                &#125;</span><br><span class="line">                visited[i]=<span class="literal">false</span>;</span><br><span class="line">                rop[depth]=<span class="number">0</span>;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(depth&lt;n)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(f2&gt;min)</span><br><span class="line">                &#123;</span><br><span class="line">                    visited[i]=<span class="literal">false</span>;</span><br><span class="line">                    allot_task(depth<span class="number">-1</span>);     <span class="comment">//回溯</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    allot_task(depth+<span class="number">1</span>);      <span class="comment">//继续添加作业</span></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        visited[rop[depth]<span class="number">-1</span>]=<span class="literal">false</span>;         <span class="comment">//跟换当前层次作业需要把之前作业解标记</span></span><br><span class="line">    &#125;</span><br><span class="line">    visited[rop[depth]<span class="number">-1</span>]=<span class="literal">false</span>;</span><br><span class="line">    rop[depth]=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;请输入任务数：&quot;</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;n);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;作业%d分别在机器1与机器2上运行时间：&quot;</span>,i+<span class="number">1</span>);</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d %d&quot;</span>,&amp;a[i][<span class="number">0</span>],&amp;a[i][<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    allot_task(<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;最优调度序列为：&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>,best_result[i]+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n消耗时间为：%d\n&quot;</span>,min);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之排序</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B9%9D%E7%AB%A0%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="第九章排序"><a href="#第九章排序" class="headerlink" title="第九章排序"></a>第九章排序</h1><h2 id="1-排序的概念"><a href="#1-排序的概念" class="headerlink" title="1.排序的概念"></a>1.排序的概念</h2><span id="more"></span>
<p>\begin{flalign}<br>&amp;排序：按照一定的关键字，将一个序列排列成想要得到的一个新的序列。\\<br>&amp;稳定性：若待排序表中有两个元素 R_i 和 R_j，其对应的关键字相同即 key_i = key_j，且在排序前 R_i 在 R_j 的前面，\\<br>&amp;~若使用某一排序算法排序后，R_i 仍然在 R_j 的前⾯，则称这个排序算法是稳定的，否则称排序算法是不稳定的。\\<br>&amp;内部排序和外部排序：整个排序过程完全在内存中进行，叫做内部排序。数据量较大需要借助外部存储设备才能完成，叫做外部排序。<br>\end{flalign}</p>
<h2 id="2-插入排序"><a href="#2-插入排序" class="headerlink" title="2.插入排序"></a>2.插入排序</h2><h3 id="（1）直接插入排序"><a href="#（1）直接插入排序" class="headerlink" title="（1）直接插入排序"></a>（1）直接插入排序</h3><p><em>①</em>  思想：最基本的插入排序，将第<em>i</em>个插入到前<em>i-1</em>个中的适当位置。</p>
<p><em>②</em>  时间复杂度：<em>T(n) = O(n</em>²<em>)</em>。</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(1)</em>。</p>
<p><em>④</em>  稳定性：稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//直接插入排序</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">InsertSort</span><span class="params">(<span class="type">int</span> A[], <span class="type">int</span> n)</span> </span>&#123;	<span class="comment">//将各元素插入已排好序的序列中,位置0的是有序的，所以从1开始</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>, j; i &lt; n; ++i) &#123;    </span><br><span class="line">        <span class="keyword">if</span> (A[i] &lt; A[i<span class="number">-1</span>]) &#123;          <span class="comment">//若A[i]小于前驱元素</span></span><br><span class="line">            <span class="type">int</span> temp = A[i];</span><br><span class="line">            <span class="keyword">for</span> (j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span> &amp;&amp; A[j] &gt; temp; --j) &#123;   <span class="comment">//检查所有前面已排好序的元素</span></span><br><span class="line">                A[j+<span class="number">1</span>] = A[j];    <span class="comment">//所有大于A[i]的元素都向后挪位</span></span><br><span class="line">            &#125;</span><br><span class="line">            A[j+<span class="number">1</span>] = temp;        <span class="comment">//复制到插入位置</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="（2）折半插入排序"><a href="#（2）折半插入排序" class="headerlink" title="（2）折半插入排序"></a>（2）折半插入排序</h3><p><em>①</em>  思想：因为是已经确定了前部分是有序序列，所以在查找插入位置的时候可以用折半查找的方法进行查找，提高效率。</p>
<p><em>②</em>  时间复杂度：比较时的时间减为O(nlogn)，但是移动元素的时间耗费未变，所以总是得时间复杂度还是O(n²)。</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(1)</em>。</p>
<p><em>④</em>  稳定性：稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//折半插入排序</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">InsertSort2</span><span class="params">(<span class="type">int</span> A[], <span class="type">int</span> n)</span> </span>&#123;		<span class="comment">//依次将A[1]~A[i-1]插入到前面已排序序列</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="type">int</span> temp = A[i];</span><br><span class="line">        <span class="type">int</span> low = <span class="number">0</span>, high = i - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (low &lt;= high) &#123;   <span class="comment">//折半查找</span></span><br><span class="line">            <span class="type">int</span> mid = (low + high) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (A[mid] &gt; temp) </span><br><span class="line">                high = mid - <span class="number">1</span>;</span><br><span class="line">            low = mid + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i<span class="number">-1</span>; j &gt;= low; --j) &#123;		<span class="comment">//将[low, i-1]内的元素全部右移，空出插入位置</span></span><br><span class="line">            A[j+<span class="number">1</span>] = A[j];</span><br><span class="line">        &#125;</span><br><span class="line">        A[low] = temp;  <span class="comment">//插入操作</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="（3）希尔排序"><a href="#（3）希尔排序" class="headerlink" title="（3）希尔排序"></a>（3）希尔排序</h3><p><em>①</em>  思想：又称缩小增量排序法。把待排序序列分成若干较小的子序列，然后逐个使用直接插入排序法排序，最后再对一个较为有序的序列进行一次排序，主要是为了减少移动的次数，提高效率。原理应该就是从无序到渐渐有序，要比直接从无序到有序移动的次数会少一些。</p>
<p><em>②</em>  时间复杂度：时间复杂度与增量序列的选择有关，最坏时间复杂度为O(n²)，当n在某个范围内时，可达O(n^1.3)。</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(1)</em>。</p>
<p><em>④</em>  稳定性：不稳定排序。仅适用于顺序表，不适用于链表。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//希尔排序</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">ShellSort</span><span class="params">(<span class="type">int</span> A[], <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = n/<span class="number">2</span>; d &gt;= <span class="number">1</span>; d /= <span class="number">2</span>) &#123;   <span class="comment">//步长变化</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = d, j; i &lt; n; i += d) &#123;</span><br><span class="line">            <span class="keyword">if</span> (A[i] &lt; A[i-d]) &#123;  <span class="comment">//需将A[i]插入有序增量子表</span></span><br><span class="line">                <span class="type">int</span> temp = A[i];    <span class="comment">//暂存元素</span></span><br><span class="line">                <span class="keyword">for</span> (j = i-d; j &gt;= <span class="number">0</span> &amp;&amp; A[j] &gt; temp; j -= d) &#123;</span><br><span class="line">                    A[j+d] = A[j];    <span class="comment">//元素后移，查找插入位置</span></span><br><span class="line">                &#125;</span><br><span class="line">                A[j+d] = temp;        <span class="comment">//插入</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="3-选择排序"><a href="#3-选择排序" class="headerlink" title="3.选择排序"></a>3.选择排序</h2><h3 id="（1）直接选择排序"><a href="#（1）直接选择排序" class="headerlink" title="（1）直接选择排序"></a>（1）直接选择排序</h3><p><em>①</em>  思想：首先在所有<strong>记录中选出关键字值最小的记录</strong>，把它与第一个记录进行位置交换，然后在其余的记录中再选出关键字值次小的记录与第二个记录进行位置交换，依此类推，直到所有记录排好序。</p>
<p><em>②</em>  时间复杂度：最好最坏平均时间复杂度为O(n²)。</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(1)</em>。</p>
<p><em>④</em>  稳定性：不稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">SelectionSort</span><span class="params">(<span class="type">int</span> A[],<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> temp,flag;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; n<span class="number">-1</span>;i++)&#123;</span><br><span class="line">        flag = i;						<span class="comment">//flag记录此刻需要确定最小值的位置</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = i+<span class="number">1</span>;j &lt; n;j++)&#123;			<span class="comment">//在i+1的位置开始在后寻找最小关键字</span></span><br><span class="line">            <span class="keyword">if</span>(A[flag] &gt; A[j])</span><br><span class="line">                flag = j;</span><br><span class="line">        &#125;</span><br><span class="line">        temp = A[flag];	A[flag] = A[i];	A[i] = temp;		<span class="comment">//将flag的数和后面的最小关键字交换</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="（2）堆排序"><a href="#（2）堆排序" class="headerlink" title="（2）堆排序"></a>（2）堆排序</h3><p><em>①</em>  思想<br>        作为选择排序的改进版，堆排序可以把每一趟元素的比较结果保存下来，以便我们在选择最小/大元素时对已经比较过的元素做出相应的调整。<br>        堆排序是一种树形选择排序，在排序过程中可以把元素看成是一颗完全二叉树，每个节点都大（小）于它的两个子节点，每个节点都大于等于它的两个子节点时，就称为大顶堆，也叫堆有序； 当每个节点都小于等于它的两个子节点时，就称为小顶堆。</p>
<p><em>②</em>  时间复杂度：最好最坏平均时间复杂度为O(nlogn)。</p>
<p>​    调整时间和树的高度有关，为O(h)，在建立含n个元素的堆时，关键字的比较总次数不超过4*n，时间复杂度为O(n)</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(1)</em>。</p>
<p><em>④</em>  稳定性：不稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//建立大根堆</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">BuildMaxHeap</span><span class="params">(ElemType A[],<span class="type">int</span> len)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = len/<span class="number">2</span>;i&gt;<span class="number">0</span>;i++)				<span class="comment">//从i = [n/2]~1，反复调整堆</span></span><br><span class="line">		<span class="built_in">HeapAdjust</span>(A,i,len);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">HeapAdjust</span><span class="params">(ElemType A[],<span class="type">int</span> k,<span class="type">int</span> len)</span></span>&#123;	<span class="comment">//函数HeapAdjust将元素k为根的子树进行调整</span></span><br><span class="line">    A[<span class="number">0</span>] = A[k];						<span class="comment">//A[0]暂存子树的根结点</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">2</span>*k;i &lt;= len;i++)&#123;			<span class="comment">//沿key较大的子结点向下筛选</span></span><br><span class="line">        <span class="keyword">if</span>(i&lt;len&amp;&amp;A[i]&lt;A[i+<span class="number">1</span>])		</span><br><span class="line">            i++;						<span class="comment">//取key较大的子结点的下标</span></span><br><span class="line">        <span class="keyword">if</span>(A[<span class="number">0</span>] &gt;= A[i])	<span class="keyword">break</span>;			<span class="comment">//筛选结束</span></span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            A[k] = A[i];				<span class="comment">//将A[i]调整到双亲结点上</span></span><br><span class="line">            k = i;						<span class="comment">//修改k值，以便继续向下筛选</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    A[k] = A[<span class="number">0</span>];						<span class="comment">//被筛选结点的值放入最终位置</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//堆排序算法</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">HeapSort</span><span class="params">(ElemType A[],<span class="type">int</span> len)</span></span>&#123;</span><br><span class="line">    <span class="built_in">BuildMaxHeap</span>(A,len);				<span class="comment">//初始建堆</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = len;i&gt;<span class="number">1</span>;i++)&#123;			<span class="comment">//n-1趟的交换和建堆过程</span></span><br><span class="line">        <span class="built_in">Swap</span>(A[i],A[<span class="number">1</span>]);				<span class="comment">//输出堆顶元素（和堆底元素交换）</span></span><br><span class="line">        <span class="built_in">HeapAdjust</span>(A,<span class="number">1</span>,i<span class="number">-1</span>);			<span class="comment">//调整，把剩余的i-1个元素整理成堆</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-交换排序"><a href="#4-交换排序" class="headerlink" title="4.交换排序"></a>4.交换排序</h2><h3 id="（1）冒泡排序"><a href="#（1）冒泡排序" class="headerlink" title="（1）冒泡排序"></a>（1）冒泡排序</h3><p><em>①</em>  思想：两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止。以升序冒泡为例：每趟排序过程中通过两两比较相邻元素，将小的数字放到前面，大的数字放到后面。</p>
<p><em>②</em>  时间复杂度：最好O(n)，最坏平均时间复杂度为O(n²)。</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(1)</em>。</p>
<p><em>④</em>  稳定性：稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">BubbleSort</span><span class="params">(ElemType A[],<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; n<span class="number">-1</span>;i++)&#123;</span><br><span class="line">        flag = <span class="literal">false</span>;				<span class="comment">//表示本趟冒泡是否发生交换的标志</span></span><br><span class="line">        <span class="keyword">for</span>(j = n<span class="number">-1</span>;j &gt; i;j--)		<span class="comment">//一趟冒泡过程</span></span><br><span class="line">            <span class="keyword">if</span>(A[j<span class="number">-1</span>]&gt;A[j])&#123;		<span class="comment">//若为逆序</span></span><br><span class="line">                <span class="built_in">swap</span>(A[j<span class="number">-1</span>],A[j]);	<span class="comment">//交换</span></span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span>(flag == <span class="literal">false</span>)</span><br><span class="line">            <span class="keyword">return</span>;					<span class="comment">//本趟遍历后没有发生变换，说明表已经有序</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="（2）快速排序"><a href="#（2）快速排序" class="headerlink" title="（2）快速排序"></a>（2）快速排序</h3><p><strong>当每次枢纽都把表等分为长度相近的两个子表时，速度是最快的</strong></p>
<p><em>①</em>  思想：快速排序时所有内部排序算法中平均性能最优的排序算法<br>        在待排序的元素任取一个元素作为基准(通常选第一个元素，但最好的选择方法是从待排序元素中随机选取一个作为基准)，称为基准元素；<br>        将待排序的元素进行分区，比基准元素大的元素放在它的右边，比其小的放在它的左边；<br>        对左右两个分区重复以上步骤直到所有元素都是有序的。</p>
<p><em>②</em>  时间复杂度：最好O(nlogn)，最坏时间复杂度为O(n²)。</p>
<p><em>③</em>  空间复杂度：<em>最好平均情况O(logn)，最坏情况O(n)</em>。</p>
<p><em>④</em>  稳定性：不稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">QuickSort</span><span class="params">(ElemType A[],<span class="type">int</span> low,<span class="type">int</span> high)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(low&lt;high)&#123;								<span class="comment">//递归跳出条件</span></span><br><span class="line">        <span class="type">int</span> pos = <span class="built_in">Partitio</span>(A,low,high);			<span class="comment">//划分，将A[low...high]划分为满足上述条件要求的两个字表</span></span><br><span class="line">        <span class="built_in">QuickSort</span>(A,low,pos<span class="number">-1</span>);					<span class="comment">//依次对两个子表进行递归排序</span></span><br><span class="line">        <span class="built_in">QuickSort</span>(A,pos+<span class="number">1</span>,high);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Partition</span><span class="params">(ElemType A[],<span class="type">int</span> low,<span class="type">int</span> high)</span></span>&#123;		<span class="comment">//一趟划分</span></span><br><span class="line">    ElemType pivot = A[low];						<span class="comment">//设当前表中第一个元素为枢纽，对表进行划分</span></span><br><span class="line">    <span class="keyword">while</span>(low&lt;high)&#123;								<span class="comment">//循环跳出条件</span></span><br><span class="line">        <span class="keyword">while</span>(low&lt;high&amp;&amp;A[high]&gt;=pivot)	--high;</span><br><span class="line">        A[low] = A[high];							<span class="comment">//将比枢纽小的元素移动到左端</span></span><br><span class="line">        <span class="keyword">while</span>(low&lt;high&amp;&amp;A[low]&lt;=pivot)	++low;</span><br><span class="line">        A[high] = A[low];							<span class="comment">//将比枢纽大的元素移动到右端</span></span><br><span class="line">    &#125;</span><br><span class="line">    A[low] = pivot;								<span class="comment">//枢纽存放到最终位置</span></span><br><span class="line">    <span class="keyword">return</span> low;									<span class="comment">//返回存放枢纽的最终位置</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-归并排序"><a href="#5-归并排序" class="headerlink" title="5.归并排序"></a>5.归并排序</h2><p><em>①</em>  思想：将待排序序列R[0…n-1]看成是n个长度为1的有序序列，将相邻的有序表成对归并，得到n/2个长度为2的有序表；将这些有序序列再次归并，得到n/4个长度为4的有序序列；如此反复进行下去，最后得到一个长度为n的有序序列。</p>
<p><em>②</em>  时间复杂度：归并排序的形式就是一棵二叉树，它需要遍历的次数就是二叉树的深度，而根据完全二叉树的可以得出它的时间复杂度是<strong>O(nlogn)</strong>。</p>
<p><em>③</em>  空间复杂度：<em>S(n) = O(n)</em>。</p>
<p><em>④</em>  稳定性：稳定排序。</p>
<p><img src="/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B9%9D%E7%AB%A0%E6%8E%92%E5%BA%8F/归并排序.png" style="zoom:80%;"></p>
<h2 id="6-基数排序"><a href="#6-基数排序" class="headerlink" title="6.基数排序"></a>6.基数排序</h2><p><strong>基数排序的效率和初始序列是否有序没有关联。</strong></p>
<p><em>①</em>  思想：<strong>不需要比较关键字的大小</strong>。根据关键字中各位的值，通过对排序的N个元素进行若干趟“分配”与“收集”来实现排序的。</p>
<p><em>②</em>  时间复杂度：假设在基数排序中，r为基数，d为位数。则基数排序的时间复杂度为<strong>O(d(n+r))</strong>。</p>
<p><em>③</em>  空间复杂度：对于任何位数上的基数进行“装桶”操作时，都需要<strong>n+r</strong>个临时空间。</p>
<p><em>④</em>  稳定性：稳定排序。</p>
<h2 id="7-排序算法总结"><a href="#7-排序算法总结" class="headerlink" title="7.排序算法总结"></a>7.排序算法总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">排序类别</th>
<th style="text-align:center">排序算法</th>
<th style="text-align:center">平均情况</th>
<th style="text-align:center">最好情况</th>
<th style="text-align:center">最坏情况</th>
<th style="text-align:center">空间复杂度</th>
<th style="text-align:center">稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">插入排序</td>
<td style="text-align:center">直接插入排序</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(n)</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">折半插入排序</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(n)</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">希尔排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center"></td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">选择排序</td>
<td style="text-align:center">直接选择排序</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">堆排序</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">交换排序</td>
<td style="text-align:center">冒泡排序</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(n)</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">快速排序</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(n²)</td>
<td style="text-align:center">O(logn)</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(nlogn)</td>
<td style="text-align:center">O(n)</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">基数排序</td>
<td style="text-align:center">基数排序</td>
<td style="text-align:center">O(d(n+r))</td>
<td style="text-align:center">O(d(n+r))</td>
<td style="text-align:center">O(d(n+r))</td>
<td style="text-align:center">O(n+r)</td>
<td style="text-align:center">稳定</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之图</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%9B%BE/</url>
    <content><![CDATA[<h1 id="第八章图"><a href="#第八章图" class="headerlink" title="第八章图"></a>第八章图</h1><h2 id="第一节图的概念和存储结构"><a href="#第一节图的概念和存储结构" class="headerlink" title="第一节图的概念和存储结构"></a>第一节图的概念和存储结构</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>线性表可以为空，树可以为空，但是图不能为空。图中不能一个顶点也没有，图的顶点集V一定非空，但边集E可以为空，此时图中只有顶点而没有边。</p>
<span id="more"></span>
<p>（1）有向图</p>
<script type="math/tex; mode=display">
G_1 = (V_1,E_1)\\
V_1 = \{1,2,3\}\\</script><script type="math/tex; mode=display">
E_1 = \{<1,2>,<2,1>,<2,3>\}</script><pre class="mermaid">graph LR;
A((1))
B((2))
C((3))
A-->B
B-->A
B-->C</pre>

<p>（2）无向图</p>
<script type="math/tex; mode=display">
G_1 = (V_1,E_1)\\
V_1 = \{1,2,3,4\}\\</script><script type="math/tex; mode=display">
E_1 = \{(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)\}</script><p>（3）简单图、多重图</p>
<ul>
<li>简单图：①不存在重复边；②不存在顶点到自身的边。</li>
<li>多重图：图G中某两个顶点之间的边数大于1条，有允许顶点通过一条边和自身关联。</li>
</ul>
<p>（4）完全图（简单完全图）</p>
<script type="math/tex; mode=display">
\begin{flalign}
&无向完全图|E|取值范围 = [~~0,n(n-1)/2~~]\\
&有向完全图|E|取值范围 = [~~0,n(n-1)~~~~~~]
\end{flalign}</script><p>（5）子图：G = (V，E)和G<code>= (V</code>，E<code>)，若V</code>是V的子集，且E`是E的子集</p>
<p>（6）连通、连通图和连通分量（无向图）</p>
<ul>
<li>连通：无向图，顶点v和顶点w有路径存在，则称v和w是连通的</li>
<li>连通图：无向图图G任意两个顶点都是连通的，称连通图，否则为非连通图</li>
<li>连通分量：无向图的极大连通子图</li>
</ul>
<p>（7）强连通图、强连通分量（有向图）</p>
<ul>
<li>强连通：有向图，有一对顶点v和w，从v到w和w到v之间都有路径，则称两个顶点是强连通的</li>
<li>强连通图：任意一对顶点都是强连通的</li>
<li>强连通分量：有向图中的极大强连通分量</li>
</ul>
<p>（8）生成树、生成森林</p>
<p>连通图的生成树是包含图中全部顶点的一个极小连通子图。若顶点数为n，则生成树含有n-1条边</p>
<p>（9）顶点的度、入度和出度</p>
<script type="math/tex; mode=display">
\begin{flalign}
&n个顶点e条边的无向图:\sum_{i=1}^nTD(v_i) = 2e\\
&n个顶点e条边的有向图：\sum_{i=1}^nID(v_i) = \sum_{i=1}^nOD(v_i) = e
\end{flalign}</script><p>（10）路径、路径长度、回路</p>
<p>若有一个图有n个顶点，并且有大于n-1条边，则该图一定有环</p>
<p>（11）简单路径、简单贿赂</p>
<ul>
<li>简单路径：顶点不重复出现的路径</li>
<li>简单回路：除第一个顶点和最后一个顶点外，其余顶点不重复出现的回路</li>
</ul>
<h3 id="2-存储结构"><a href="#2-存储结构" class="headerlink" title="2.存储结构"></a>2.存储结构</h3><h4 id="（1）邻接矩阵法"><a href="#（1）邻接矩阵法" class="headerlink" title="（1）邻接矩阵法"></a>（1）邻接矩阵法</h4><p>空间复杂度O(n^2)</p>
<p>无权图</p>
<script type="math/tex; mode=display">
A[i][j] = 
\begin{cases}
1,~~~~~~~若(v_i,v_j)或<v_i.v_j>是E(G)中的边\\\\
0,~~~~~~~若(v_i,v_j)或<v_i.v_j>不是E(G)中的边\\
\end{cases}</script><p>​    举例如下：</p>
<pre class="mermaid">graph LR;
A((1))
B((2))
C((3))
D((4))
A-->B
A-->C
C-->D
D-->A</pre>

<script type="math/tex; mode=display">
A_1=\left[
\matrix{
 0 & 1 & 1 & 0\\
 0 & 0 & 0 & 0\\
 0 & 0 & 0 & 1\\
 1 & 0 & 0 & 0\\
}
\right]</script><p>有权图</p>
<script type="math/tex; mode=display">
A[i][j] = 
\begin{cases}
w_{ij},~~~~~~~~~~~若(v_i,v_j)或<v_i.v_j>是E(G)中的边\\\\
0或∞,~~~~~~~若(v_i,v_j)或<v_i.v_j>不是E(G)中的边\\
\end{cases}</script><p>举例如下：</p>
<pre class="mermaid">graph LR;
A((1))
B((2))
C((3))
D((4))
A--5-->B
A--8-->C
C--10-->D
D--21-->A</pre>

<script type="math/tex; mode=display">
A_2=\left[
\matrix{
 ∞ & 5 & 8 & ∞\\
 ∞ & ∞ & ∞ & ∞\\
 ∞ & ∞ & ∞ & 10\\
 21 & ∞ & ∞ & ∞\\
}
\right]</script><p>存储结构定义：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MaxVertextNum 100							<span class="comment">//顶点数目的最大值</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">char</span> VertexType;							<span class="comment">//顶点的数据类型</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> EdgeType;								<span class="comment">//带权图中边上权值的数据类型</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    VertexType Vex[MaxVertextNum];						<span class="comment">//顶点表</span></span><br><span class="line">    Edgetype Edge[MaxVertextNum][MaxVertextNum];		<span class="comment">//邻接矩阵，边表</span></span><br><span class="line">    <span class="type">int</span> vexnum,arcnum;									<span class="comment">//图的当前顶点数和弧数</span></span><br><span class="line">&#125;MGraph;</span><br></pre></td></tr></table></figure>
<h4 id="（2）邻接表法"><a href="#（2）邻接表法" class="headerlink" title="（2）邻接表法"></a>（2）邻接表法</h4><ul>
<li>无向图：存储空间O(|V|+2|E|)</li>
<li>有向图：存储空间O(|V|+|E|)</li>
</ul>
<p>顶点表结点结构</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">顶点域</th>
<th style="text-align:center">边表头指针</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">data</td>
<td style="text-align:center">firstarc</td>
</tr>
</tbody>
</table>
</div>
<p>边表结点结构</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">邻接点域</th>
<th style="text-align:center">指针域</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">adjvex</td>
<td style="text-align:center">nextarc</td>
</tr>
</tbody>
</table>
</div>
<p>存储结构定义</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MaxVertexNum 100							<span class="comment">//顶点数目的最大值</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">ArcNode</span>&#123;								<span class="comment">//边表结点</span></span><br><span class="line">    <span class="type">int</span> adjvex;									<span class="comment">//该弧所指向的顶点的位置</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ArcNode</span> *next;						<span class="comment">//指向下一条弧的指针</span></span><br><span class="line">    <span class="comment">//InfoType info;							//网的边权值</span></span><br><span class="line">&#125;ArcNode;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">VNode</span>&#123;							<span class="comment">//顶点表结点</span></span><br><span class="line">    VertexType data;							<span class="comment">//顶点信息</span></span><br><span class="line">    ArcNode *first;								<span class="comment">//指向第一条依附该顶点的弧的指针</span></span><br><span class="line">&#125;VNode,AdiList[MaxVertexNum];</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    AdiList vertices;							<span class="comment">//邻接表</span></span><br><span class="line">    <span class="type">int</span> vexnum,arcnum;							<span class="comment">//图的顶点数和弧数</span></span><br><span class="line">&#125;ALGraph;										<span class="comment">//ALGraph是以邻接表存储的图的类型</span></span><br></pre></td></tr></table></figure>
<h4 id="（3）十字链表法（有向图）"><a href="#（3）十字链表法（有向图）" class="headerlink" title="（3）十字链表法（有向图）"></a>（3）十字链表法（有向图）</h4><p>图的十字链表表示不唯一，但一个十字链表表示确定一个图</p>
<p>弧结点</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">tailvex</th>
<th style="text-align:center">headvex</th>
<th style="text-align:center">hlink</th>
<th style="text-align:center">tlink</th>
<th style="text-align:center">info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p>顶点结点</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">data</th>
<th style="text-align:center">firstin</th>
<th style="text-align:center">firstout</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%9B%BE/十字链表法.png" style="zoom:80%;"></p>
<h4 id="（4）邻接多重表（无向图）"><a href="#（4）邻接多重表（无向图）" class="headerlink" title="（4）邻接多重表（无向图）"></a>（4）邻接多重表（无向图）</h4><p>顶点结点</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">mark</th>
<th style="text-align:center">ivex</th>
<th style="text-align:center">ilink</th>
<th style="text-align:center">jvex</th>
<th style="text-align:center">jlink</th>
<th style="text-align:center">info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p>弧顶点</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">data</th>
<th style="text-align:center">firstedge</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%9B%BE/邻接多重表.png" style="zoom:80%;"></p>
<h3 id="3-基本算法实现"><a href="#3-基本算法实现" class="headerlink" title="3.基本算法实现"></a>3.基本算法实现</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Adjacent</span>(G,x,y)：判断图G是否存在边&lt;x,y&gt;或(x,y)。</span><br><span class="line"><span class="built_in">Neighbors</span>(G,x)：列出图G中与结点x邻接的边。</span><br><span class="line"><span class="built_in">InsertVertex</span>(G,x)：在图G中插入顶点x。</span><br><span class="line"><span class="built_in">DeleteVertex</span>(G,x)：在图G中删除顶点x。</span><br><span class="line"><span class="built_in">AddEdge</span>(G,x,y)：若无边边(x,y)或有向边&lt;x,y&gt;不存在，则向图G中添加该边。</span><br><span class="line"><span class="built_in">RemoveEdge</span>(G,x,y)：若无边边(x,y)或有向边&lt;x,y&gt;存在，则向图G中删除该边。 </span><br><span class="line"><span class="built_in">FirstNeighbor</span>(G,x)：求图G中顶点x的第一个邻接点，若有则返回顶点号。若x没有邻接点或图中不存在x，则返回<span class="number">-1</span>。</span><br><span class="line"><span class="built_in">NextNeighbor</span>(G,x,y)：假设图G中顶点y是顶点x的一个邻接点，返回除y外顶点x的下一个邻接点的顶点号，若y是x的最后一个邻接点，则返回<span class="number">-1</span>。</span><br><span class="line"><span class="built_in">Get_edge_value</span>(G,x,y)：获取图中G中边(x,y)或&lt;x,y&gt;对应的权值。</span><br><span class="line"><span class="built_in">Set_edge_value</span>(G,x,y,v)：设置图中G中边(x,y)或&lt;x,y&gt;对应的权值为v。</span><br></pre></td></tr></table></figure>
<h2 id="第二节图的遍历算法"><a href="#第二节图的遍历算法" class="headerlink" title="第二节图的遍历算法"></a>第二节图的遍历算法</h2><h3 id="1-广度优先搜索（BFS）"><a href="#1-广度优先搜索（BFS）" class="headerlink" title="1.广度优先搜索（BFS）"></a>1.广度优先搜索（BFS）</h3><ul>
<li>邻接表：空间复杂度O(|V|)    时间复杂度O(|V|+|E|)</li>
<li>邻接矩阵：空间复杂度O(|V|)    时间复杂度O(|V|^2)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">bool</span> visited[MAX_VERTEX_NUM];				<span class="comment">//访问标记数组</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">BFSTraverse</span><span class="params">(Graph G)</span></span>&#123;					<span class="comment">//对图G进行广度优先遍历</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>;i&lt;G.vexnum;++i)</span><br><span class="line">        visited[i] = FALSE;					<span class="comment">//访问标记数组初始化</span></span><br><span class="line">    <span class="built_in">InitQueue</span>(Q);							<span class="comment">//初始化辅助队列Q</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>;i&lt;G.vexnum;++i)				<span class="comment">//从0号顶点开始遍历</span></span><br><span class="line">        <span class="keyword">if</span>(!visited[i])						<span class="comment">//对每个连通分量调用一次BFS</span></span><br><span class="line">            <span class="built_in">BFS</span>(G,i);						<span class="comment">//vi未访问过，从vi开始BFS</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">BFS</span><span class="params">(Graph G,<span class="type">int</span> v)</span></span>&#123;					<span class="comment">//从顶点v出发，广度优先遍历图G</span></span><br><span class="line">    <span class="built_in">visit</span>(v);								<span class="comment">//访问初始顶点v</span></span><br><span class="line">    visited[v] = TRUE;						<span class="comment">//对v做已访问标记</span></span><br><span class="line">    <span class="built_in">EnQueue</span>(Q,v);							<span class="comment">//顶点v入队列Q</span></span><br><span class="line">    <span class="keyword">while</span>(!<span class="built_in">osEmpty</span>(Q))&#123;</span><br><span class="line">        <span class="built_in">DeQueue</span>(Q,v);						<span class="comment">//顶点v出队列</span></span><br><span class="line">        <span class="keyword">for</span>(w = <span class="built_in">FirstNeighbor</span>(G,v);w&gt;=<span class="number">0</span>;w = <span class="built_in">NextNeighbor</span>(G,v,w))	<span class="comment">//检测v所有邻接点</span></span><br><span class="line">            <span class="keyword">if</span>(!visited[w])&#123;				<span class="comment">//w为v的尚未访问的邻接顶点</span></span><br><span class="line">                <span class="built_in">visit</span>(w);					<span class="comment">//访问顶点w</span></span><br><span class="line">                wisited[w] = TRUE;			<span class="comment">//对w做已访问标记</span></span><br><span class="line">                <span class="built_in">EnQueue</span>(Q,w);				<span class="comment">//顶点w入队列</span></span><br><span class="line">            &#125;<span class="comment">//if</span></span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-深度优先搜索"><a href="#2-深度优先搜索" class="headerlink" title="2.深度优先搜索"></a>2.深度优先搜索</h3><ul>
<li>邻接表：空间复杂度O(|V|)    时间复杂度O(|V|+|E|)</li>
<li>邻接矩阵：空间复杂度O(|V|)    时间复杂度O(|V|^2)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">bool</span> visited[MAX_VERTEX_NUM];				<span class="comment">//访问标记数组</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">DFSTraverse</span><span class="params">(Graph G)</span></span>&#123;					<span class="comment">//对图G进行深度优先遍历</span></span><br><span class="line">    <span class="keyword">for</span>(v = <span class="number">0</span>;v&lt;G.vexnum;++v)</span><br><span class="line">        visited[v] = FALSE;					<span class="comment">//访问标记数组初始化</span></span><br><span class="line">    <span class="keyword">for</span>(v = <span class="number">0</span>;v&lt;G.vexnum;++v)				<span class="comment">//从0号顶点开始遍历</span></span><br><span class="line">        <span class="keyword">if</span>(!visited[v])</span><br><span class="line">            <span class="built_in">DFS</span>(G,v);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">DFS</span><span class="params">(Graph G,<span class="type">int</span> v)</span></span>&#123;					<span class="comment">//从顶点v出发，深度优先遍历图G</span></span><br><span class="line">    <span class="built_in">visit</span>(v);								<span class="comment">//访问初始顶点v</span></span><br><span class="line">    visited[v] = TRUE;						<span class="comment">//对v做已访问标记</span></span><br><span class="line">    <span class="keyword">for</span>(w = <span class="built_in">FirstNeighbor</span>(G,v);w&gt;=<span class="number">0</span>;w = <span class="built_in">NextNeighbor</span>(G,v,w))</span><br><span class="line">        <span class="keyword">if</span>(!visited[w])&#123;					<span class="comment">//w为v的尚未访问的邻接顶点</span></span><br><span class="line">            <span class="built_in">DFS</span>(G,w);</span><br><span class="line">        &#125;<span class="comment">//if</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第三节最小生成树"><a href="#第三节最小生成树" class="headerlink" title="第三节最小生成树"></a>第三节最小生成树</h2><h3 id="1-性质"><a href="#1-性质" class="headerlink" title="1.性质"></a>1.性质</h3><ul>
<li>最小生成树不是唯一的，即最小生成树的树形不唯一。当图G中的个边权值互不相等时，G的最小生成树时唯一的；若无向连通图G的边数比顶点数少1，即G本身就是一棵树，最小生成树就是它本身。</li>
<li>最小生成树的边的权值之和总是唯一的</li>
<li>最小生成树的边数为顶点数-1</li>
</ul>
<h3 id="2-普利姆算法（稠密图）"><a href="#2-普利姆算法（稠密图）" class="headerlink" title="2.普利姆算法（稠密图）"></a>2.普利姆算法（稠密图）</h3><script type="math/tex; mode=display">
时间复杂度：O(|V|^2)</script><p><img src="/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%9B%BE/普利姆.png" style="zoom:80%;"></p>
<h3 id="3-克鲁斯卡尔算法（稀疏图）"><a href="#3-克鲁斯卡尔算法（稀疏图）" class="headerlink" title="3.克鲁斯卡尔算法（稀疏图）"></a>3.克鲁斯卡尔算法（稀疏图）</h3><script type="math/tex; mode=display">
时间复杂度：O(|E|log|E|)</script><p><img src="/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%9B%BE/克鲁斯卡尔.png" style="zoom:80%;"></p>
<h2 id="第四节最短路径、拓扑排序和关键路径"><a href="#第四节最短路径、拓扑排序和关键路径" class="headerlink" title="第四节最短路径、拓扑排序和关键路径"></a>第四节最短路径、拓扑排序和关键路径</h2><h3 id="1-最短路径"><a href="#1-最短路径" class="headerlink" title="1.最短路径"></a>1.最短路径</h3><h4 id="（1）Dijkstra算法-只适合正数"><a href="#（1）Dijkstra算法-只适合正数" class="headerlink" title="（1）Dijkstra算法(只适合正数)"></a>（1）Dijkstra算法(只适合正数)</h4><script type="math/tex; mode=display">
时间复杂度：O(|V|^2)</script><p>dist[]：记录从源点v0到其他个顶点当前的最短路径长度。</p>
<p>path[]：path[i]表示从源点到顶点i之间的最短路径的前驱结点。</p>
<p><img src="/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%9B%BE/Dijkstra.png" style="zoom:80%;"></p>
<h4 id="（2）Floyd算法"><a href="#（2）Floyd算法" class="headerlink" title="（2）Floyd算法"></a>（2）Floyd算法</h4><script type="math/tex; mode=display">
时间复杂度：O(|V|^3)~~~~~~~~
空间复杂度：O(|V|^2)</script><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Floyd</span><span class="params">(n)</span></span>&#123;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">1</span>;k&lt;=n;k++)&#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">			<span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=n;j++)&#123;</span><br><span class="line">				dist[i][j]=<span class="built_in">min</span>(dist[i][j],dist[i][k]+dist[k][j]);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（3）有向无环图DAG图"><a href="#（3）有向无环图DAG图" class="headerlink" title="（3）有向无环图DAG图"></a>（3）有向无环图DAG图</h4><p>用来描述公共子式的表达式的有效工具</p>
<h3 id="2-拓扑排序-AOV网"><a href="#2-拓扑排序-AOV网" class="headerlink" title="2.拓扑排序(AOV网)"></a>2.拓扑排序(AOV网)</h3><pre class="mermaid">graph LR;
A((1))
B((2))
C((3))
D((4))
E((5))
A-->B
A-->D
B-->C
B-->D
D-->C
C-->E
D-->E</pre>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">结点号</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初始入度</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">第一轮</td>
<td style="text-align:center"></td>
<td style="text-align:center">0</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">第二轮</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">第三轮</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0</td>
<td style="text-align:center"></td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">第四轮</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">第五轮</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
\begin{flalign}
&邻接表时间复杂度：O(|V|+|E|)\\
&邻接矩阵时间复杂度：O(|V|^2)
\end{flalign}</script><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">TopologicalSort</span><span class="params">(Graph G)</span></span>&#123;</span><br><span class="line">    <span class="built_in">InitStack</span>(S);							<span class="comment">//初始化栈，存储入度为0的顶点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i&lt;G.vexnum;i++)</span><br><span class="line">        <span class="keyword">if</span>(indegree[i] == <span class="number">0</span>)</span><br><span class="line">            <span class="built_in">Push</span>(S,i);						<span class="comment">//将所有入度为0的顶点进栈</span></span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;							<span class="comment">//计数，记录当前已经输出的顶点数</span></span><br><span class="line">    <span class="keyword">while</span>(!<span class="built_in">IsEmpty</span>(S))&#123;						<span class="comment">//栈不空，则存在入度为0的顶点</span></span><br><span class="line">        <span class="built_in">Pop</span>(S,i);							<span class="comment">//栈顶元素出栈</span></span><br><span class="line">        print[count++] = i;					<span class="comment">//输出顶点i</span></span><br><span class="line">        <span class="keyword">for</span>(p = G.vertices[i].firstarc;p;p = p-&gt;nextarc)&#123;		</span><br><span class="line">            <span class="comment">//将所有i指向的顶点的入度-1，并且将入度减为0的顶点压入栈S</span></span><br><span class="line">            v = p-&gt;adjvex;</span><br><span class="line">            <span class="keyword">if</span>(!(--indegree[v]))</span><br><span class="line">                <span class="built_in">Push</span>(S,v);					<span class="comment">//入度为0，则入栈</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;<span class="comment">//while</span></span><br><span class="line">    <span class="keyword">if</span>(count&lt;G.vexnum)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;						<span class="comment">//排序失败，有向图中有回路</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;						<span class="comment">//拓扑排序成功</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-关键路径-AOE网"><a href="#3-关键路径-AOE网" class="headerlink" title="3.关键路径(AOE网)"></a>3.关键路径(AOE网)</h3><p>\begin{flalign}<br>&amp;（1）事件v_k的最早发生时间ve(k)\\<br>&amp;指从源点v_1到顶点v_k的最长路径的长度。事件v_k的最早发生时间决定了所有从v_k开始的活动能够开工的最早时间。\\<br>&amp;（2）事件v_k的最迟发生时间vl(k)\\<br>&amp;指在不推迟整个工程完成的前提下，即保证它的后继时间v_j在其最迟发生时间vl(j)能够发生时，该事件最迟必须发生的时间。\\<br>&amp;（3）活动a_i的最早发生时间e(i)\\<br>&amp;指该活动弧的起点所表示的时间的最早发生时间\\<br>&amp;（4）活动a_i的最迟发生时间l(i)\\<br>&amp;指该活动弧的终点所表示时间的最迟发生时间与该活动所需时间之差。\\<br>\end{flalign}</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之数组</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%BA%94%E7%AB%A0%E6%95%B0%E7%BB%84/</url>
    <content><![CDATA[<h1 id="第五章数组"><a href="#第五章数组" class="headerlink" title="第五章数组"></a>第五章数组</h1><h2 id="第一节数组的概念"><a href="#第一节数组的概念" class="headerlink" title="第一节数组的概念"></a>第一节数组的概念</h2><h3 id="1-数据概念"><a href="#1-数据概念" class="headerlink" title="1.数据概念"></a>1.数据概念</h3><p>​    数组是由n（n≥1）个相同类型的数据元素构成的有限序列</p>
<span id="more"></span>
<h3 id="2-数组的实现"><a href="#2-数组的实现" class="headerlink" title="2.数组的实现"></a>2.数组的实现</h3><p>​    以一维数组A[0…n-1]为例，其存储结构关系式为</p>
<script type="math/tex; mode=display">
LOC(a_i) = LOC(a_0)+i*L\quad\quad (0≤i<n)</script><p>​    其中，L时每个数组元素所占的存储单元</p>
<h2 id="第二节特殊矩阵和稀疏矩阵的压缩存储"><a href="#第二节特殊矩阵和稀疏矩阵的压缩存储" class="headerlink" title="第二节特殊矩阵和稀疏矩阵的压缩存储"></a>第二节特殊矩阵和稀疏矩阵的压缩存储</h2><h3 id="1-特殊矩阵的压缩存储"><a href="#1-特殊矩阵的压缩存储" class="headerlink" title="1.特殊矩阵的压缩存储"></a>1.特殊矩阵的压缩存储</h3><h4 id="（1）对称矩阵（1，0）"><a href="#（1）对称矩阵（1，0）" class="headerlink" title="（1）对称矩阵（1，0）"></a>（1）对称矩阵（1，0）</h4><script type="math/tex; mode=display">
k=
\begin{cases}
i(i-1)/2+j-1,\quad\quad i≥j（下三角区和主对角线元素）\\\\
j(j-1)/2+i-1,\quad\quad i<j（上三角区元素a_{ij} = a_{ji}）
\end{cases}</script><h4 id="（2）三角矩阵（1，0）"><a href="#（2）三角矩阵（1，0）" class="headerlink" title="（2）三角矩阵（1，0）"></a>（2）三角矩阵（1，0）</h4><p>下三角矩阵</p>
<script type="math/tex; mode=display">
k=
\begin{cases}
i(i-1)/2+j-1,\quad\quad i≥j（下三角区和主对角线元素）\\\\
n(n+1)/2,\quad\quad~~~~~~~~~~~ i<j（上三角区元素）
\end{cases}</script><p>上三角矩阵</p>
<script type="math/tex; mode=display">
k=
\begin{cases}
(i-1)(2n-i+2)/2+(j-i),\quad\quad i≤j（上三角区和主对角线元素）\\\\
n(n+1)/2,\quad\quad~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ i>j（下三角区元素）
\end{cases}</script><h4 id="（3）三对角矩阵（1，0）"><a href="#（3）三对角矩阵（1，0）" class="headerlink" title="（3）三对角矩阵（1，0）"></a>（3）三对角矩阵（1，0）</h4><script type="math/tex; mode=display">
k = 2i+j-3</script><h3 id="2-稀疏矩阵的压缩存储"><a href="#2-稀疏矩阵的压缩存储" class="headerlink" title="2.稀疏矩阵的压缩存储"></a>2.稀疏矩阵的压缩存储</h3><p>稀疏矩阵压缩存储后便失去了随机存取特性</p>
<script type="math/tex; mode=display">
M=\left[
\matrix{
 4 & 0 & 0 & 0\\
 0 & 0 & 6 & 0\\
 0 & 9 & 0 & 0\\
 0 & 23 & 0 & 0\\
}
\right]</script><p>对应三元组</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">i</th>
<th style="text-align:center">j</th>
<th style="text-align:center">v</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">9</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1</td>
<td style="text-align:center">23</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之递归算法和广义表</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AD%E7%AB%A0%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95%E5%92%8C%E5%B9%BF%E4%B9%89%E8%A1%A8/</url>
    <content><![CDATA[<h1 id="第六章递归算法和广义表"><a href="#第六章递归算法和广义表" class="headerlink" title="第六章递归算法和广义表"></a>第六章递归算法和广义表</h1><h2 id="第一节递归算法"><a href="#第一节递归算法" class="headerlink" title="第一节递归算法"></a>第一节递归算法</h2><h3 id="1-递归算法概念"><a href="#1-递归算法概念" class="headerlink" title="1.递归算法概念"></a>1.递归算法概念</h3><p>​    是一种直接或者间接调用自身函数或者方法的算法。说简单点就是程序自身的调用。</p>
<span id="more"></span>
<h3 id="2-递归算法设计"><a href="#2-递归算法设计" class="headerlink" title="2.递归算法设计"></a>2.递归算法设计</h3><h4 id="（1）阶乘"><a href="#（1）阶乘" class="headerlink" title="（1）阶乘"></a>（1）阶乘</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">fac</span><span class="params">(<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> n*<span class="built_in">f</span>(n<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    cout&lt;&lt;<span class="built_in">f</span>(<span class="number">5</span>)&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//120</span></span><br></pre></td></tr></table></figure>
<h4 id="（2）斐波那契数列"><a href="#（2）斐波那契数列" class="headerlink" title="（2）斐波那契数列"></a>（2）斐波那契数列</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">1</span> || n == <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">fib</span>(n<span class="number">-1</span>)+<span class="built_in">fib</span>(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    cout&lt;&lt;<span class="built_in">fib</span>(<span class="number">10</span>)&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//55</span></span><br></pre></td></tr></table></figure>
<h4 id="（3）杨辉三角的取值"><a href="#（3）杨辉三角的取值" class="headerlink" title="（3）杨辉三角的取值"></a>（3）杨辉三角的取值</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="comment">//递归函数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">func</span><span class="params">(<span class="type">int</span> m,<span class="type">int</span> n)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">0</span>||n == m )<span class="comment">//递归终止条件</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> func(m<span class="number">-1</span>,n)+func(m<span class="number">-1</span>,n<span class="number">-1</span>);<span class="comment">//核心代码</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    <span class="type">int</span> m,i,j;</span><br><span class="line">    m=<span class="number">6</span>;<span class="comment">//打印前6行杨辉三角</span></span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;=m;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;m-i;j++)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;   &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;=i;j++)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%6d&quot;</span>,func(i,j));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（4）汉诺塔"><a href="#（4）汉诺塔" class="headerlink" title="（4）汉诺塔"></a>（4）汉诺塔</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Move</span><span class="params">(<span class="type">int</span> n, <span class="type">char</span> A, <span class="type">char</span> B, <span class="type">char</span> C)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="comment">//圆盘只有一个时，只需将其从A塔移到C塔</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;move &quot;</span> &lt;&lt; n &lt;&lt; <span class="string">&quot; from &quot;</span> &lt;&lt; A &lt;&lt; <span class="string">&quot; to &quot;</span> &lt;&lt; C &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">Move</span>(n - <span class="number">1</span>, A, C, B);<span class="comment">//递归，把A塔上编号1~n-1的圆盘移到B上，以C为辅助塔</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;move &quot;</span> &lt;&lt; n &lt;&lt; <span class="string">&quot; from &quot;</span> &lt;&lt; A &lt;&lt; <span class="string">&quot; to &quot;</span> &lt;&lt; C &lt;&lt; endl;<span class="comment">//把A塔上编号为n的圆盘移到C上</span></span><br><span class="line">        <span class="built_in">Move</span>(n - <span class="number">1</span>, B, A, C);<span class="comment">//递归，把B塔上编号1~n-1的圆盘移到C上，以A为辅助塔</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">Move</span>(<span class="number">3</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第二节广义表"><a href="#第二节广义表" class="headerlink" title="第二节广义表"></a>第二节广义表</h2><h3 id="1-广义表概念"><a href="#1-广义表概念" class="headerlink" title="1.广义表概念"></a>1.广义表概念</h3><p>​    是一种非线性的数据结构，它的表元素可以是原子或者广义表的一种线性表的扩展结构。</p>
<ul>
<li>​    广义表的长度：表中最上层元素的个数</li>
<li>​    广义表的深度：表中括号的最大层数</li>
<li>​    表头和表尾：当广义表非空时，第一个元素为广义表的表头，其余元素组成的表是广义表的表尾</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">E=()	<span class="comment">//E是一个空表，其长度为0，其深度为1</span></span><br><span class="line">L=(a，b)	<span class="comment">//L是长度为2的广义表，它的两个元素都是原子，因此它是一个线性表，其深度为1</span></span><br><span class="line">A=(x，L)=(x，(a，b))	<span class="comment">//A是长度为2的广义表，第一个元素是原子x，第二个元素是子表L，其深度为2</span></span><br><span class="line">B=(A，y)=((x，(a，b))，y)	<span class="comment">//B是长度为2的广义表，第一个元素是子表A，第二个元素是原子y，其深度为3</span></span><br><span class="line">C=(A，B)=((x，(a，b))，((x，(a，b))，y))	<span class="comment">//C的长度为2，两个元素都是子表，其深度为4</span></span><br><span class="line">D=(a，D)=(a，(a，(a，(…))))	<span class="comment">//D的长度为2，第一个元素是原子，第二个元素是D自身，展开后它是一个无限的广义表，其深度为∞</span></span><br></pre></td></tr></table></figure>
<h3 id="2-广义表存储结构和操作实现"><a href="#2-广义表存储结构和操作实现" class="headerlink" title="2.广义表存储结构和操作实现"></a>2.广义表存储结构和操作实现</h3><h4 id="（1）广义表结点的代码结构"><a href="#（1）广义表结点的代码结构" class="headerlink" title="（1）广义表结点的代码结构"></a>（1）广义表结点的代码结构</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//定义广义表的数据结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">GList</span>&#123;</span><br><span class="line">	NodeTag tag; <span class="comment">//用以区分是原子结点还是子表结点</span></span><br><span class="line">	<span class="keyword">union</span>&#123;</span><br><span class="line">		DataType data; <span class="comment">//用以存放原子结点值，其类型由用户自定义</span></span><br><span class="line">		GList *slink; <span class="comment">//指向子表的指针</span></span><br><span class="line">	&#125;;</span><br><span class="line">	GList *next; <span class="comment">//指向下一个表结点</span></span><br><span class="line">&#125; *GListPtr;</span><br></pre></td></tr></table></figure>
<h4 id="（2）求表头、表尾"><a href="#（2）求表头、表尾" class="headerlink" title="（2）求表头、表尾"></a>（2）求表头、表尾</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*求广义表L的表头，并返回表头指针*/</span></span><br><span class="line"><span class="function">GList <span class="title">Head</span><span class="params">(GList L)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (L == <span class="literal">NULL</span>)					<span class="comment">//空表无表头</span></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">	<span class="keyword">if</span> (L-&gt;tag == ATOM)				<span class="comment">//原子不是表</span></span><br><span class="line">		<span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">return</span> L-&gt;atom_htp.htp.hp;	<span class="comment">//返回表头指针</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*求广义表L的表尾，并返回表尾指针*/</span></span><br><span class="line"><span class="function">GList <span class="title">Tail</span><span class="params">(GList L)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (L == <span class="literal">NULL</span>)					<span class="comment">//空表无表尾</span></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">	<span class="keyword">if</span> (L-&gt;tag == ATOM)				<span class="comment">//原子不是表</span></span><br><span class="line">		<span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">return</span> L-&gt;atom_htp.htp.tp;	<span class="comment">//返回表尾指针</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（3）求长度、深度"><a href="#（3）求长度、深度" class="headerlink" title="（3）求长度、深度"></a>（3）求长度、深度</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*求广义表长度*/</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Length</span><span class="params">(GList L)</span> </span>&#123;</span><br><span class="line">	<span class="type">int</span> k = <span class="number">0</span>;</span><br><span class="line">	GLNode* s;</span><br><span class="line">	<span class="keyword">if</span> (L == <span class="literal">NULL</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;					<span class="comment">//空表长度为0</span></span><br><span class="line">	<span class="keyword">if</span> (L-&gt;tag == ATOM)				<span class="comment">//原子不是表</span></span><br><span class="line">		<span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">	s = L;</span><br><span class="line">	<span class="keyword">while</span> (s != <span class="literal">NULL</span>) &#123;				<span class="comment">//统计最上层表的长度</span></span><br><span class="line">		k++;</span><br><span class="line">		s = s-&gt;atom_htp.htp.tp;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> k;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*求广义表的深度*/</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Depth</span><span class="params">(GList L)</span> </span>&#123;</span><br><span class="line">	<span class="type">int</span> d, max;</span><br><span class="line">	GLNode* s;</span><br><span class="line">	<span class="keyword">if</span> (L == <span class="literal">NULL</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;					<span class="comment">//空表深度为1</span></span><br><span class="line">	<span class="keyword">if</span> (L-&gt;tag == ATOM)</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;					<span class="comment">//原子深度为0</span></span><br><span class="line">	s = L;</span><br><span class="line">	<span class="keyword">while</span> (s != <span class="literal">NULL</span>) &#123;				<span class="comment">//求每个子表的深度的最大值</span></span><br><span class="line">		d = <span class="built_in">Depth</span>(s-&gt;atom_htp.htp.hp);</span><br><span class="line">		<span class="keyword">if</span> (d &gt; max)</span><br><span class="line">			max = d;</span><br><span class="line">		s = s-&gt;atom_htp.htp.tp;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> (max + <span class="number">1</span>);				<span class="comment">//表的深度等于最深子表的深度+1</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">PrintGList</span><span class="params">(GListPtr gl)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(gl != <span class="literal">NULL</span>)&#123;</span><br><span class="line">		<span class="keyword">if</span>(gl-&gt;tag == list)&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;(&quot;</span>);</span><br><span class="line">			<span class="keyword">if</span>(gl-&gt;slink == <span class="literal">NULL</span>)&#123;</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">&quot;&quot;</span>);</span><br><span class="line">			&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">				<span class="built_in">PrintGList</span>(gl-&gt;slink); <span class="comment">//递归调用输出子表</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>, gl-&gt;data); <span class="comment">//输出结点数据域值</span></span><br><span class="line">		&#125;</span><br><span class="line"> </span><br><span class="line">		<span class="keyword">if</span>(gl-&gt;tag == list)&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;)&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line"> </span><br><span class="line">		<span class="keyword">if</span>(gl-&gt;next != <span class="literal">NULL</span>)&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">			<span class="built_in">PrintGList</span>(gl-&gt;next); <span class="comment">//递归调用输出下一个节点</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（4）统计原子个数"><a href="#（4）统计原子个数" class="headerlink" title="（4）统计原子个数"></a>（4）统计原子个数</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*统计广义表中原子结点数目*/</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">CountAtom</span><span class="params">(GList L)</span> </span>&#123;</span><br><span class="line">	<span class="type">int</span> n1, n2;</span><br><span class="line">	<span class="keyword">if</span> (L == <span class="literal">NULL</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;							<span class="comment">//空表中没有原子</span></span><br><span class="line">	<span class="keyword">if</span> (L-&gt;tag == ATOM)</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;							<span class="comment">//L指向单个原子</span></span><br><span class="line">	n1 = <span class="built_in">CountAtom</span>(L-&gt;atom_htp.htp.hp);		<span class="comment">//统计表头中的原子数目</span></span><br><span class="line">	n2 = <span class="built_in">CountAtom</span>(L-&gt;atom_htp.htp.tp);		<span class="comment">//统计表尾中的原子数目</span></span><br><span class="line">	<span class="keyword">return</span> (n1 + n2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（5）广义表的复制"><a href="#（5）广义表的复制" class="headerlink" title="（5）广义表的复制"></a>（5）广义表的复制</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*复制广义表*/</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">CopyGList</span><span class="params">(GList S, GList* T)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (S == <span class="literal">NULL</span>) &#123;				<span class="comment">//复制空表</span></span><br><span class="line">		*T = <span class="literal">NULL</span>;</span><br><span class="line">		<span class="keyword">return</span> OK;</span><br><span class="line">	&#125;</span><br><span class="line">	*T = (GLNode*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(GLNode));</span><br><span class="line">	<span class="keyword">if</span> (*T == <span class="literal">NULL</span>)</span><br><span class="line">		<span class="keyword">return</span> ERROR;</span><br><span class="line">	(*T)-&gt;tag = S-&gt;tag;</span><br><span class="line">	<span class="keyword">if</span> (S-&gt;tag == ATOM)</span><br><span class="line">		(*T)-&gt;atom_htp.atom = S-&gt;atom_htp.atom;		<span class="comment">//复制单个原子</span></span><br><span class="line">	<span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">/*复制表头*/</span></span><br><span class="line">		<span class="built_in">CopyGList</span>(S-&gt;atom_htp.htp.hp, &amp;((*T)-&gt;atom_htp.htp.hp));</span><br><span class="line">		<span class="comment">/*复制表尾*/</span></span><br><span class="line">		<span class="built_in">CopyGList</span>(S-&gt;atom_htp.htp.tp, &amp;((*T)-&gt;atom_htp.htp.tp));</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之串</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E4%B8%B2/</url>
    <content><![CDATA[<h1 id="第四章串"><a href="#第四章串" class="headerlink" title="第四章串"></a>第四章串</h1><h2 id="第一节串的概念和存储结构"><a href="#第一节串的概念和存储结构" class="headerlink" title="第一节串的概念和存储结构"></a>第一节串的概念和存储结构</h2><h3 id="1-串的概念"><a href="#1-串的概念" class="headerlink" title="1.串的概念"></a>1.串的概念</h3><p>​    由0个或多个字符组成的有效序列，一般记作</p>
<script type="math/tex; mode=display">
S='a_1a_2a_3...a_n'~~~（n≥0）</script><span id="more"></span>
<h3 id="2-串的存储结构和基本算法的实现"><a href="#2-串的存储结构和基本算法的实现" class="headerlink" title="2.串的存储结构和基本算法的实现"></a>2.串的存储结构和基本算法的实现</h3><h4 id="（1）定长顺序存储表示"><a href="#（1）定长顺序存储表示" class="headerlink" title="（1）定长顺序存储表示"></a>（1）定长顺序存储表示</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MAXLEN 255		<span class="comment">//预定义最大串长255</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">	<span class="type">char</span> ch[MAXLEN];		<span class="comment">//每个分量存储一个字符</span></span><br><span class="line">	<span class="type">int</span> length;				<span class="comment">//串的实际长度</span></span><br><span class="line">&#125;SString;</span><br></pre></td></tr></table></figure>
<h4 id="（2）堆分配存储表示"><a href="#（2）堆分配存储表示" class="headerlink" title="（2）堆分配存储表示"></a>（2）堆分配存储表示</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">char</span> *ch;		<span class="comment">//按串长分配存储区，ch指向串的基地址</span></span><br><span class="line">    <span class="type">int</span> length;		<span class="comment">//串的长度</span></span><br><span class="line">&#125;HString;</span><br></pre></td></tr></table></figure>
<h4 id="（3）基本操作"><a href="#（3）基本操作" class="headerlink" title="（3）基本操作"></a>（3）基本操作</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">StrAssign</span>(&amp;T,chars)：赋值操作。把串T赋值为chars。</span><br><span class="line"><span class="built_in">StrCopy</span>(&amp;T,S)：复制操作。由串S复制得到串T。</span><br><span class="line"><span class="built_in">StrEmpty</span>(S)：判空操作。若S为空串，则返回TRUE，否则返回FALSE。</span><br><span class="line"><span class="built_in">StrLength</span>(S)：求串长。返回串S的元素个数。</span><br><span class="line"><span class="built_in">ClearString</span>(&amp;S)：清空操作。将S清为空串。</span><br><span class="line"><span class="built_in">Concat</span>(&amp;T,S1,S2)：串联接。用T返回由S1和S2联接而成的新串</span><br><span class="line"><span class="built_in">SubString</span>(&amp;Sub,S,pos,len)：求子串。用Sub返回串S的第pos个字符起长度为len的子串。</span><br><span class="line"><span class="built_in">Index</span>(S,T)：定位操作。若主串S中存在与串T值相同的子串，则返回它在主串S中第一次出现的位置；否则函数值为<span class="number">0</span>。</span><br><span class="line"><span class="built_in">StrCompare</span>(S,T)：比较操作。若S&gt;T，则返回值&gt;<span class="number">0</span>；若S=T，则返回值=<span class="number">0</span>；若S&lt;T，则返回值&lt;<span class="number">0</span>。</span><br><span class="line"><span class="built_in">DestroyString</span>(&amp;S)：销毁中。将串S销毁。</span><br></pre></td></tr></table></figure>
<h2 id="第二节串的匹配算法"><a href="#第二节串的匹配算法" class="headerlink" title="第二节串的匹配算法"></a>第二节串的匹配算法</h2><h3 id="1-BF算法（简单模式匹配算法）"><a href="#1-BF算法（简单模式匹配算法）" class="headerlink" title="1.BF算法（简单模式匹配算法）"></a>1.BF算法（简单模式匹配算法）</h3><p>时间复杂度：最好O（n），最坏O（nm）</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">BF</span><span class="params">(string S,string T,<span class="type">int</span> pos)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = pos;</span><br><span class="line">    <span class="type">int</span> j = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( i &lt;= S.<span class="built_in">size</span>() &amp;&amp; j &lt;= T.<span class="built_in">size</span>() )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>( S[i<span class="number">-1</span>] == T[j<span class="number">-1</span>] )</span><br><span class="line">        &#123;</span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            i = i-j+<span class="number">2</span>;<span class="comment">//i为母串S的匹配失败的位置，j为模式串S的匹配失败的位置，i-j为S第一次匹配位置之前的长度，</span></span><br><span class="line">            <span class="comment">//+2的原因是一个1是本次匹配的开始位置 ，另一个1是下一次匹配的开始位置。</span></span><br><span class="line">            j = <span class="number">1</span>;<span class="comment">//模式串的下一次匹配开始位置依旧是第一个字符</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>( j &gt; T.<span class="built_in">size</span>() )</span><br><span class="line">        <span class="keyword">return</span> i - T.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> pos;</span><br><span class="line">    string S,T;</span><br><span class="line">    <span class="keyword">while</span>( <span class="literal">true</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;请输入母串：&quot;</span>;</span><br><span class="line">        cin&gt;&gt;S;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;请输入模式串：&quot;</span>;</span><br><span class="line">        cin&gt;&gt;T;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;请输入在母串中开始寻找的位置（小于等于&quot;</span>&lt;&lt;S.<span class="built_in">size</span>()&lt;&lt;<span class="string">&quot;）：&quot;</span>;</span><br><span class="line">        cin&gt;&gt;pos;</span><br><span class="line">        <span class="keyword">while</span> ( pos &gt; S.<span class="built_in">size</span>() )</span><br><span class="line">        &#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;请重新输入在母串中开始寻找的位置（小于等于&quot;</span>&lt;&lt;S.<span class="built_in">size</span>()&lt;&lt;<span class="string">&quot;）：&quot;</span>;</span><br><span class="line">            cin&gt;&gt;pos;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;模式串在自母串第 &quot;</span>&lt;&lt;pos&lt;&lt;<span class="string">&quot; 位开始出现的位置为 &quot;</span>&lt;&lt;<span class="built_in">BF</span>(S,T,pos)&lt;&lt;endl&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-KMP算法"><a href="#2-KMP算法" class="headerlink" title="2.KMP算法"></a>2.KMP算法</h3><h4 id="（1）next数组"><a href="#（1）next数组" class="headerlink" title="（1）next数组"></a>（1）next数组</h4><p>以‘ababa’举例说明：</p>
<ul>
<li>‘a’的前缀和后缀都是空集，最长相等前后缀长度为0；</li>
<li>‘ab’的前缀为{a}，后缀为{b}，{a}且{b}=空集，最长相等前后缀长度为0；</li>
<li>‘aba’的前缀是{a,ab}，后缀是{a,ba}，{a,ab}且{a,ba}={a}，最长相等前后缀长度为1；</li>
<li>‘abab’的前缀是{a,ab,aba}，后缀是{a,ba,bab}，{a,ab,aba}且{a,ba,bab}={a}，最长相等前后缀长度为1；</li>
<li>‘ababa’的前缀是{a,ab,aba,abab}，后缀是{a,ba,aba,baba}，{a,ab,aba,abab}且{a,ba,aba,baba}={a,aba}，最长相等前后缀长度为3；</li>
</ul>
<p>时间复杂度：O（n+m）；优点：主串不回溯</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Next</span><span class="params">(<span class="type">char</span>*T,<span class="type">int</span> *next)</span>&#123;</span><br><span class="line">    <span class="type">int</span> i=<span class="number">1</span>;</span><br><span class="line">    next[<span class="number">1</span>]=<span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i&lt;<span class="built_in">strlen</span>(T)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (j==<span class="number">0</span>||T[i<span class="number">-1</span>]==T[j<span class="number">-1</span>]) &#123;</span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">            next[i]=j;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            j=next[j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">KMP</span><span class="params">(<span class="type">char</span> * S,<span class="type">char</span> * T)</span>&#123;</span><br><span class="line">    <span class="type">int</span> next[<span class="number">10</span>];</span><br><span class="line">    Next(T,next);<span class="comment">//根据模式串T,初始化next数组</span></span><br><span class="line">    <span class="type">int</span> i=<span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (i&lt;=<span class="built_in">strlen</span>(S)&amp;&amp;j&lt;=<span class="built_in">strlen</span>(T)) &#123;</span><br><span class="line">        <span class="comment">//j==0:代表模式串的第一个字符就和当前测试的字符不相等；S[i-1]==T[j-1],如果对应位置字符相等，两种情况下，指向当前测试的两个指针下标i和j都向后移</span></span><br><span class="line">        <span class="keyword">if</span> (j==<span class="number">0</span> || S[i<span class="number">-1</span>]==T[j<span class="number">-1</span>]) &#123;</span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            j=next[j];<span class="comment">//如果测试的两个字符不相等，i不动，j变为当前测试字符串的next值</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (j&gt;<span class="built_in">strlen</span>(T)) &#123;<span class="comment">//如果条件为真，说明匹配成功</span></span><br><span class="line">        <span class="keyword">return</span> i-(<span class="type">int</span>)<span class="built_in">strlen</span>(T);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> i=KMP(<span class="string">&quot;ababcabcacbab&quot;</span>,<span class="string">&quot;abcac&quot;</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>,i);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）nextval数组"><a href="#（2）nextval数组" class="headerlink" title="（2）nextval数组"></a>（2）nextval数组</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">j</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
<th style="text-align:center">6</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th style="text-align:center">9</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">模式串T</td>
<td style="text-align:center">a</td>
<td style="text-align:center">b</td>
<td style="text-align:center">a</td>
<td style="text-align:center">b</td>
<td style="text-align:center">a</td>
<td style="text-align:center">a</td>
<td style="text-align:center">a</td>
<td style="text-align:center">b</td>
<td style="text-align:center">a</td>
</tr>
<tr>
<td style="text-align:center">next</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">4</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">nextval</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">4</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之三大算法</title>
    <url>/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%89%E5%A4%A7%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>以算法区分深度学习应用，算法类别可分成三大类：</p>
<ul>
<li>常用于影像数据进行分析处理的<strong>卷积神经网络(简称CNN)</strong></li>
<li>文本分析或自然语言处理的<strong>递归神经网络(简称RNN)</strong></li>
<li>常用于数据生成或非监督式学习应用的<strong>生成对抗网络(简称GAN)</strong></li>
</ul>
<span id="more"></span>
<h1 id="卷积神经网络CNN"><a href="#卷积神经网络CNN" class="headerlink" title="卷积神经网络CNN"></a><strong>卷积神经网络CNN</strong></h1><p>因为应用种类多样，本篇会以算法类别细分，CNN主要应用可分为图像分类(image classification)、目标检测(object detection)及语义分割(semantic segmentation)。</p>
<h2 id="1、图像分类-Classification"><a href="#1、图像分类-Classification" class="headerlink" title="1、图像分类(Classification)"></a><strong>1、图像分类(Classification)</strong></h2><p>将图像进行类别筛选，通过深度学习方法识别图片属于哪种分类类别，其主要重点在于一张图像只包含一种分类类别，即使该影像内容可能有多个目标，所以单纯图像分类的应用并不普遍。不过由于单一目标识别对深度学习算法来说是正确率最高的，所以实际上很多应用会先通过目标检测方法找到该目标，再缩小撷取影像范围进行图像分类。所以只要是目标检测可应用的范围，通常也会使用图像分类方法。</p>
<p>图像分类也是众多用来测试算法基准的方法之一，常使用由ImageNet举办的大规模视觉识别挑战赛(ILSVRC)中提供的公开图像数据进行算法测试。图像分类属于CNN的基础，其相关算法也是最易于理解，故初学者应该都先以图像分类做为跨入深度学习分析的起步。使用图像分类进行识别，通常输入为一张图像，而输出为一个文字类别。</p>
<h2 id="2、目标检测-Object-Detection"><a href="#2、目标检测-Object-Detection" class="headerlink" title="2、目标检测 (Object Detection)"></a><strong>2、目标检测 (Object Detection)</strong></h2><p>一张图像内可有一或多个目标物，目标物也可以是属于不同类别。算法主要能达到两种目的：找到目标坐标及识别目标类别。简单来说，就是除了需要知道目标是什么，还需要知道它在哪个位置。</p>
<p>目标检测应用非常普遍，包含文章开头提到的人脸识别相关技术结合应用，或是制造业方面的瑕疵检测，甚至医院用于X光、超音波进行特定身体部位的病况检测等。目标识别的基础可想象为在图像分类上增加标示位置的功能，故学习上也不离图像分类的基础。不过目标检测所标示的坐标通常为矩形或方形，仅知道目标所在位置，并无法针对目标的边缘进行描绘，所以常用见的应用通常会以「知道目标位置即可」作为目标。</p>
<p>最常见的算法为YOLO及R-CNN。其中YOLO因算法特性具有较快的识别速度，目前已来到v3版本。R-CNN针对目标位置搜寻及辨识算法和YOLO稍有不同，虽然速度稍较YOLO慢，但正确率稍高于YOLO。使用目标检测进行识别，通常输入为一张图像，而输出为一个或数个文字类别和一组或多组坐标。</p>
<h2 id="3、语义分割-Semantic-Segmentation"><a href="#3、语义分割-Semantic-Segmentation" class="headerlink" title="3、语义分割 (Semantic Segmentation)"></a><strong>3、语义分割 (Semantic Segmentation)</strong></h2><p>算法会针对一张图像中的每个像素进行识别，也就是说不同于目标检测，语义分割可以正确区别各目标的边界像素，简单来说，语义分割就是像素级别的图像分类，针对每个像素进行分类。当然这类应用的模型就会需要较强大的GPU和花较多时间进行训练。</p>
<p>常见应用类似目标检测，但会使用在对于图像识别有较高精细度，如需要描绘出目标边界的应用。例如制造业上的瑕疵检测，针对不规则形状的大小瑕疵，都可以正确描绘。医学上常用于分辨病理切片上的病变细胞，或是透过MRI、X光或超音波描绘出病变的区块及类别。算法如U-Net或是Mask R-CNN都是常见的实作方法。使用语义分割进行识别，通常输入为一张图像，而输出也为一张等大小的图像，但图像中会以不同色调描绘不同类别的像素。</p>
<h1 id="递归神经网络RNN"><a href="#递归神经网络RNN" class="headerlink" title="递归神经网络RNN"></a><strong>递归神经网络RNN</strong></h1><p>有别于CNN，RNN的特色在于可处理图像或数值数据，并且由于网络本身具有记忆能力，可学习具有前后相关的数据类型。例如进行语言翻译或文本翻译，一个句子中的前后词汇通常会有一定的关系，但CNN网络无法学习到这层关系，而RNN因具有内存，所以性能会比较好。因为可以通过RNN进行文字理解，其他应用如输入一张图像，但是输出为一段关于图像叙述的句子。</p>
<p>RNN虽然解决了CNN无法处理的问题，但其本身仍然有些缺点，所以现在很多RNN的变形网络，其中最常被使用的网络之一为长短记忆网络(Long Short-Term Network，简称LSTM)。这类网络的输入数据不限于是图像或文字，解决的问题也不限于翻译或文字理解。数值相关数据也同样可以使用LSTM进行分析，例如工厂机器预测性维修应用，可透过LSTM分析机台震动讯号，预测机器是否故障。在医学方面，LSTM可协助解读数以千计的文献，并找出特定癌症的相关信息，例如肿瘤部位、肿瘤大小、期数，甚至治疗方针或存活率等等，透过文字理解进行解析。也可结合图像识别提供病灶关键词，以协助医生撰写病理报告。</p>
<h1 id="生成对抗网络GAN"><a href="#生成对抗网络GAN" class="headerlink" title="生成对抗网络GAN"></a><strong>生成对抗网络GAN</strong></h1><p>除了深度学习外，有一种新兴的网络称为强化学习(Reinforcement Learning)，其中一种很具有特色的网络为生成式对抗网络(GAN)。</p>
<p>GAN的应用相关论文成长幅度相当大</p>
<p>深度学习领域最需要的是数据，但往往不是所有应用都可以收集到大量数据，并且数据也需要人工进行标注，这是非常消耗时间及人力成本。图像数据可以通过旋转、裁切或改变明暗等方式增加数据量，但如果数据还是不够呢？目前有相当多领域透过GAN方法生成非常近似原始数据的数据，例如3D-GAN就是可以生成高质量3D对象。当然，比较有趣的应用例如人脸置换或表情置换。</p>
<p>另外，SRGAN (Super Resolution GAN)可用于提高原始图像的分辨率，将作为低分辨率影像输入进GAN模型，并生成较高画质的影像(如下图)。这样的技术可整合至专业绘图软件中，协助设计师更有效率完成设计工作。</p>
<p>NVIDIA也有提供一些基于GAN的平台的应用，包含透过GauGAN网络，仅需绘制简单的线条，即可完成漂亮的画作，并且还能随意修改场景的风格。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构之查找</title>
    <url>/2023/04/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%8D%81%E7%AB%A0%E6%9F%A5%E6%89%BE/</url>
    <content><![CDATA[<h1 id="第十章查找"><a href="#第十章查找" class="headerlink" title="第十章查找"></a>第十章查找</h1><h2 id="第一节查找的概念"><a href="#第一节查找的概念" class="headerlink" title="第一节查找的概念"></a>第一节查找的概念</h2><p>在数据集合中寻找满足某种条件的数据元素的过程</p>
<span id="more"></span>
<h2 id="第二节静态查找"><a href="#第二节静态查找" class="headerlink" title="第二节静态查找"></a>第二节静态查找</h2><h3 id="1-顺序（线性）查找"><a href="#1-顺序（线性）查找" class="headerlink" title="1.顺序（线性）查找"></a>1.顺序（线性）查找</h3><h4 id="（1）一般线性表的查找"><a href="#（1）一般线性表的查找" class="headerlink" title="（1）一般线性表的查找"></a>（1）一般线性表的查找</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;					<span class="comment">//查找表的数据结构</span></span><br><span class="line">    ElemType *elem;				<span class="comment">//元素存储空间基址，建表时按实际长度分配，0号单元留空</span></span><br><span class="line">    <span class="type">int</span> TableLen;				<span class="comment">//表的长度</span></span><br><span class="line">&#125;SSTable;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Search_Seq</span><span class="params">(SSTable ST,ElemType key)</span></span>&#123;</span><br><span class="line">    ST.elem[<span class="number">0</span>] = key;			<span class="comment">//哨兵</span></span><br><span class="line">    <span class="keyword">for</span>(i = ST.TableLen;ST.elem[i]!=key,--i);	<span class="comment">//从后往前找</span></span><br><span class="line">    <span class="keyword">return</span> i;					<span class="comment">//若表中不存在关键字为key的元素，将查找到i=0时退出for循环</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于有n个元素的表，给定值key与表中第i个元素相等，即定位第i个元素时，需进行n-i+1次关键词的比较。</p>
<p>\begin{flalign}<br>&amp;1.查找成功时:\\<br>&amp;顺序查找的平均长度为：ASL_{成功} = \sum_{i=1}^nP_i(n-i+1)\\<br>&amp;当每个元素的查找概率相等，即P_i=1/n时，有：ASL_{成功} = \sum_{i=1}^nP_i(n-i+1)=(n+1)/2\\<br>&amp;2.查找失败时：\\<br>&amp;与表中各关键词的比较次数时n+1次，从而顺序查找不成功的平均查找长度为：ASL_{不成功} = n+1<br>\end{flalign}</p>
<h4 id="（2）有序表的查找"><a href="#（2）有序表的查找" class="headerlink" title="（2）有序表的查找"></a>（2）有序表的查找</h4><p>\begin{flalign}<br>&amp;1.查找成功时:\\<br>&amp;顺序查找的平均长度为：ASL_{成功} = \sum_{i=1}^nP_i(n-i+1)\\<br>&amp;当每个元素的查找概率相等，即P_i=1/n时，有：ASL_{成功} = \sum_{i=1}^nP_i(n-i+1)=(n+1)/2\\<br>&amp;2.查找失败时：\\<br>&amp;查找不成功的平均查找长度为：ASL_{不成功} = \sum_{j=1}^nq_i(l_j-1)=(1+2…+n+n)/(n+1)=n/2+n/(n+1)<br>\end{flalign}</p>
<h3 id="2-二分（折半）查找"><a href="#2-二分（折半）查找" class="headerlink" title="2.二分（折半）查找"></a>2.二分（折半）查找</h3><p>时间复杂度：O(logn)</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">Binary_Search</span><span class="params">(SeqList L,ElemType key)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>,high = L.TableLen<span class="number">-1</span>,mid;</span><br><span class="line">    <span class="keyword">while</span>(low&lt;=high)&#123;</span><br><span class="line">        min = (low+high)/<span class="number">2</span>;							<span class="comment">//取中间位置</span></span><br><span class="line">        <span class="keyword">if</span>(L.elem[mid] = key)	<span class="keyword">return</span> min;			<span class="comment">//查找成功则返回所在位置</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(L.elem[mid]&gt;key)	high = mid<span class="number">-1</span>;	<span class="comment">//从前半部分继续查找</span></span><br><span class="line">        <span class="keyword">else</span>	low = mid+<span class="number">1</span>;						<span class="comment">//从后半部分继续查找</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;										<span class="comment">//查找失败，返回-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>\begin{flalign}<br>&amp;ASL_{成功} = 1/n\sum_{i=1}^nl_i=1/n(1<em>1+2</em>2+…+h<em>2^{h-1})=(n+1)/n</em>log_2(n+1)≈log_2(n+1)-1\\<br>\end{flalign}</p>
<h3 id="3-索引（分块）查找"><a href="#3-索引（分块）查找" class="headerlink" title="3.索引（分块）查找"></a>3.索引（分块）查找</h3><p>基本思想：将查找表氛围若干子块。块内元素可以无序，但块之间时有序的，即第一个块中的最大关键字小于第二块中的所有记录的关键字，以此类推。再建立一个索引表，索引表中每个元素含有各块的最大关键字和各块中的第一个元素的地址，索引表按关键字有序排列。</p>
<script type="math/tex; mode=display">
\begin{flalign}
&1.分块查找的平均查找长度为索引查找和块内查找的平均长度之和，设索引查找和块内查找的平均查找长度分别为L_l和L_s\\
&~~~则分块查找的平均查找长度为：ASL = L_l+L_s\\
&2.将长度为n的查找表均匀的分成b块，每块有s各记录，在等概率情况下，若块内和索引表中均采用顺序查找\\
&~~~则平均查找长度为：ASL = L_l+L_s = (b+1)/2+(s+1)/2 = (s^2+2s+n)/(2s)\\
&3.若s = \sqrt n，则平均查找长度取最小值\sqrt n+1；\\
&4.若对索引表采用折半查找时，\\
&~~~则平均查找长度为ASL= L_l+L_s = \lceil log_2(b+1)\rceil+(s+1)/2
\end{flalign}</script><h2 id="第三节动态查找"><a href="#第三节动态查找" class="headerlink" title="第三节动态查找"></a>第三节动态查找</h2><h3 id="二叉排序树（BST）"><a href="#二叉排序树（BST）" class="headerlink" title="二叉排序树（BST）"></a>二叉排序树（BST）</h3><p>二叉排序树（二叉查找树）或者是一棵空树，或者时具有下列特性的二叉树：</p>
<p>1）若左子树非空，则左子树上所有结点的值均小于根结点的值；</p>
<p>2）若右子树非空，则右子树上所有结点的值均大于根结点的值；</p>
<p>3）左、右子数也分别是一棵二叉排序树。</p>
<h4 id="（1）查找"><a href="#（1）查找" class="headerlink" title="（1）查找"></a>（1）查找</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">BSTNode *<span class="title">BST_Search</span><span class="params">(BiTree T,ElemType key)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(T&amp;&amp;key!=T-&gt;data)&#123;				<span class="comment">//若树空或等于根结点值，则结束循环</span></span><br><span class="line">        <span class="keyword">if</span>(key&lt;T-&gt;data)	T = T-&gt;lchild;	<span class="comment">//小于，则在左子树上查找</span></span><br><span class="line">        <span class="keyword">else</span>	T = T-&gt;rchild;			<span class="comment">//大于，则在右子树上查找</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> T;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（2）插入"><a href="#（2）插入" class="headerlink" title="（2）插入"></a>（2）插入</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">BST_Insert</span><span class="params">(BiTree &amp;T,KetType k)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T)&#123;									<span class="comment">//原树为空，新插入的记录为新结点</span></span><br><span class="line">        T = (BiTree)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(BSTNode));</span><br><span class="line">        T-&gt;data = k;</span><br><span class="line">        T-&gt;lchild = T-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;							<span class="comment">//返回1，表示插入成功</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(k == T-&gt;data)	<span class="keyword">return</span> <span class="number">0</span>;		<span class="comment">//树中存在相同关键字的结点，插入失败</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(k &lt; T-data)		<span class="keyword">return</span> <span class="built_in">BST_Insert</span>(T-&gt;lchild,k);		<span class="comment">//插入到T的左子树</span></span><br><span class="line">    <span class="keyword">else</span>	<span class="keyword">return</span> <span class="built_in">BST_Insert</span>(T-&gt;rchild,k);		<span class="comment">//插入到T的右子树</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（3）构造"><a href="#（3）构造" class="headerlink" title="（3）构造"></a>（3）构造</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Creat_BST</span><span class="params">(BiTree &amp;T,KetType str[],<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    T = <span class="literal">NULL</span>;					<span class="comment">//初始时T为空树</span></span><br><span class="line">    <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; n)&#123;				<span class="comment">//依次将每个关键字插入到二叉排序树中</span></span><br><span class="line">        <span class="built_in">BST_Insert</span>(T,str[i]);	</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="（4）删除"><a href="#（4）删除" class="headerlink" title="（4）删除"></a>（4）删除</h4><p>①若被删除结点z时叶结点，则直接删除，不会破坏二叉排序树的性质；</p>
<p>②若结点z只有一棵左子树或右子树，则让z的子数称为z父节点的子数，代替z的位置；</p>
<p>③若结点z有左、右两棵子数，则令z的直接后继（或直接前驱）替代z，然后二叉排序树中删去这个直接后继（或直接前驱），这样就转换成了第一种或第二种情况。</p>
<pre class="mermaid">graph TB;
A((53))
B((17))
C((78))
D((09))
E((45))
F((65))
G((94))
H((81))
I((88))
J((23))
A-->B
A-->C
B-->D
B-->E
C-->F
C-->G
E-->J
G-->H
H-->I</pre>

<p>删除78结点后</p>
<pre class="mermaid">graph TB;
A((53))
B((17))
D((09))
E((45))
F((65))
G((94))
H((81))
I((88))
J((23))
A-->B
A-->H
B-->D
B-->E
E-->J
H-->F
H-->G
G-->I</pre>

<p>\begin{flalign}<br>平衡二叉树的递推公式：n_0=0,n_1=1,n_2=2,n_h=1+n_{h-1}+n_{h-2}<br>\end{flalign}</p>
<h2 id="第四节哈希查找（散列表）"><a href="#第四节哈希查找（散列表）" class="headerlink" title="第四节哈希查找（散列表）"></a>第四节哈希查找（散列表）</h2><h3 id="1-哈希查找的概念"><a href="#1-哈希查找的概念" class="headerlink" title="1.哈希查找的概念"></a>1.哈希查找的概念</h3><p>散列函数：一个把查找表中的关键字映射成该关键字对应的地址的函数，记为Hash(key)=Addr（这里的地址可以实数组下标、索引或内存地址等）</p>
<p>散列表：根据关键字而直接进行访问的数据结构。即<em>散列表建立了关键字和存储地址之间的一种直接映射关系</em>。</p>
<h3 id="2-哈希函数"><a href="#2-哈希函数" class="headerlink" title="2.哈希函数"></a>2.哈希函数</h3><h4 id="（1）直接定址法"><a href="#（1）直接定址法" class="headerlink" title="（1）直接定址法"></a>（1）直接定址法</h4><p>\begin{flalign}<br>H(key)=key或H(key)=a×key+b<br>\end{flalign}</p>
<h4 id="（2）除留余数法"><a href="#（2）除留余数法" class="headerlink" title="（2）除留余数法"></a>（2）除留余数法</h4><p>假定散列表表长为m，取一个不大于m但最接近或等于m的质数p，则散列函数如下：</p>
<p>\begin{flalign}<br>H(key)=key\%p<br>\end{flalign}</p>
<h4 id="（3）数字分析法"><a href="#（3）数字分析法" class="headerlink" title="（3）数字分析法"></a>（3）数字分析法</h4><h4 id="（4）平方取中法"><a href="#（4）平方取中法" class="headerlink" title="（4）平方取中法"></a>（4）平方取中法</h4><p>取关键字的平方值的中间几位作为散列地址。适用于关键字的每位取值都不够均匀或均小于散列地址所需的位数。</p>
<h3 id="3-哈希冲突的解决方法"><a href="#3-哈希冲突的解决方法" class="headerlink" title="3.哈希冲突的解决方法"></a>3.哈希冲突的解决方法</h3><h4 id="（1）开放地址法"><a href="#（1）开放地址法" class="headerlink" title="（1）开放地址法"></a>（1）开放地址法</h4><p>\begin{flalign}<br>H_i = (H(key)+d_i)\%m\\<br>H(key)为散列函数：i = 0,1,2,..,k(k ≤ m-1)；m表示散列表表长；d_i为增量序列<br>\end{flalign}</p>
<h5 id="—a-线性探测法"><a href="#—a-线性探测法" class="headerlink" title="—a.线性探测法"></a>—a.线性探测法</h5><p>特点：冲突发生时，顺序查看表中下一个单元（探测到表尾地址m-1时，下一个探测地址时表首地址0），知道招初一个空闲单元（当表为填满时一定要找到一个空闲单元）或查遍全表。</p>
<p>缺点：容易造成大量元素在相邻的散列地址上“聚集”（或堆积）起来，大大降低查找效率。</p>
<h5 id="—b-平方探测法"><a href="#—b-平方探测法" class="headerlink" title="—b.平方探测法"></a>—b.平方探测法</h5><p>特点：当d~i~ = 0²，1²，-1²，2²，-2²，…，k²，-k²时，称为平方探测法，其中k ≤ m/2，散列表长度m必须是一个可以表示成4k+3的素数，又称二次探测法。</p>
<p>优点：可以避免出现“堆积”问题。</p>
<p>缺点：不能探测散列表上的所有单元，但至少可以探测到一半单元。</p>
<h5 id="—c-双散列法"><a href="#—c-双散列法" class="headerlink" title="—c.双散列法"></a>—c.双散列法</h5><p>​        当d~i~=Hash~2~(key)时，称为双散列法。当通过第一个散列函数H(key)得到的地址发生冲突时，利用第二个散列函数Hash~2~(key)计算该关键字的地址增量。具体散列函数形式如下：</p>
<p>\begin{flalign}<br>H_i=(H(key)+i×Hash_2(key))\%m<br>\end{flalign}</p>
<p>初始探测位置H~0~=H(key)%m。i是冲突的次数，初始为0。最多经过m-1次探测就会遍历表中所有位置，回到H~0~位置。</p>
<h5 id="—d-伪随机序列法"><a href="#—d-伪随机序列法" class="headerlink" title="—d.伪随机序列法"></a>—d.伪随机序列法</h5><p>d~i~ = 伪随机序列时，称为伪随机序列法。</p>
<h4 id="（2）拉链法（链接法）"><a href="#（2）拉链法（链接法）" class="headerlink" title="（2）拉链法（链接法）"></a>（2）拉链法（链接法）</h4><p>同线性探测法，但改为链表形式。</p>
<h3 id="4-散列查找及性能分析"><a href="#4-散列查找及性能分析" class="headerlink" title="4.散列查找及性能分析"></a>4.散列查找及性能分析</h3><p>散列表的查找效率取决三个因素：散列函数、处理冲突的方法和装填因子。</p>
<p>\begin{flalign}<br>\alpha = 表中记录数n/散列表长度m<br>\end{flalign}</p>
<p>散列表的平均查找长度依赖于散列表的装填因子α，而不直接依赖于n或m。直观地看，α越大，表示装填的记录越“满”，发生冲突的可能性越大，反之发生冲突的可能性越小。</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（一）CNN原理概述</title>
    <url>/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="卷积神经网络的架构"><a href="#卷积神经网络的架构" class="headerlink" title="卷积神经网络的架构"></a><strong>卷积神经网络的架构</strong></h1><p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积神经网络的架构.png" alt="卷积神经网络的架构" style="zoom:80%;"></p>
<span id="more"></span>
<h1 id="CNN解决了什么问题？"><a href="#CNN解决了什么问题？" class="headerlink" title="CNN解决了什么问题？"></a><strong>CNN解决了什么问题？</strong></h1><p>CNN，是近年发展起来的并引起广泛重视的一种高效识别方法，可以说是深度学习算法应用最成功的领域之一，其价值在于能够将大数据量的图片有效地降维成小数据量且不影响结果，同时与人类视觉原理类似，CNN能够较完整地保留图片的特征。</p>
<p>卷积神经网络包括一维卷积神经网络，二维卷积神经网络以及三维卷积神经网络。一维卷积神经网络主要用于序列类的数据处理，二维卷积神经网络常应用于图像类文本的识别，三维卷积神经网络主要应用于医学图像以及视频类数据识别。</p>
<p>在 CNN 出现之前，图像对于人工智能来说一直是一个难题，问题的主要原因有2个：</p>
<ul>
<li><strong>需要处理的数据量太大</strong></li>
<li><strong>保留图像特征</strong></li>
</ul>
<p><strong>需要处理的数据量太大</strong></p>
<p>图像是由每个带有颜色的像素构成的，且每个像素都有RGB（可简单理解为光学三原色：红、绿、蓝）3个参数来表示颜色信息。</p>
<p>假如现在需要处理一张1000×1000像素的图片，就需要处理1000×1000×3=3,000,000个参数！如此大量的数据处理起来是非常消耗资源的，而且这还只是一张不算太大的图片。</p>
<p><strong>保留图像特征</strong></p>
<p>假如将一张图片划分为四个区域，其中一个区域中有圆形表示为数字1，没有圆形则表示为0，那么圆形的位置不同就会产生完全不同的数据表达，上述的这个过程就相当于是传统的图片数字化过程简化版。</p>
<p>对于上述两个问题，CNN通过将复杂问题简化和保留图像特征就较为完美地解决了，那么，CNN是如何进行实现的呢？</p>
<h1 id="CNN的基本原理"><a href="#CNN的基本原理" class="headerlink" title="CNN的基本原理"></a><strong>CNN的基本原理</strong></h1><h2 id="层级结构"><a href="#层级结构" class="headerlink" title="层级结构"></a><strong>层级结构</strong></h2><p>一个卷积神经网络主要由以下 5 层组成：</p>
<ul>
<li>数据输入层/ Input layer</li>
<li>卷积计算层/ CONV layer</li>
<li>ReLU 激励层 / ReLU layer</li>
<li>池化层 / Pooling layer</li>
<li>全连接层 / FC layer</li>
</ul>
<h3 id="数据输入层"><a href="#数据输入层" class="headerlink" title="数据输入层"></a>数据输入层</h3><p>该层要做的处理主要是对原始图像数据进行预处理，其中包括：</p>
<ul>
<li><strong>去均值</strong>：把输入数据各个维度都中心化为 0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。</li>
<li><strong>归一化</strong>：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征 A 和 B，A 范围是 0 到 10，而 B 范围是 0 到 10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即 A 和 B 的数据都变为 0 到 1 的范围。</li>
<li><strong>PCA/白化</strong>：用 PCA 降维；白化是对数据各个特征轴上的幅度归一化</li>
</ul>
<p>去均值与归一化效果图：</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/去均值与归一化效果图.png" style="zoom:80%;"></p>
<p>去相关与白化效果图：</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/去相关与白化效果图.png" style="zoom:80%;"></p>
<h3 id="卷积计算层"><a href="#卷积计算层" class="headerlink" title="卷积计算层"></a>卷积计算层</h3><p>这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。</p>
<p>在这个卷积层，有两个关键操作：</p>
<ul>
<li><strong>局部关联</strong>。每个神经元看做一个滤波器(filter)</li>
<li><strong>窗口(receptive field)滑动</strong>， filter 对局部数据计算</li>
</ul>
<p>先介绍卷积层遇到的几个名词：</p>
<ul>
<li><strong>深度/depth</strong>（解释见下图）</li>
<li><strong>步幅/stride</strong> （窗口一次滑动的长度）</li>
<li><strong>填充值/zero-padding</strong></li>
</ul>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积计算层.png" style="zoom:80%;"></p>
<p>现在，要改变每一层的行为，有两个主要参数是我们可以调整的。选择了过滤器的尺寸以后，我们还需要选择步幅（stride）和填充（padding）。</p>
<p>步幅控制着过滤器围绕输入内容进行卷积计算的方式。在第一部分我们举的例子中，过滤器通过每次移动一个单元的方式对输入内容进行卷积。过滤器移动的距离就是步幅。在那个例子中，步幅被默认设置为 1。步幅的设置通常要确保输出内容是一个整数而非分数。让我们看一个例子。想象一个 7 x 7 的输入图像，一个 3 x 3 过滤器（简单起见不考虑第三个维度），步幅为 1。这是一种惯常的情况。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积-步长1.png" alt="img"></p>
<p>如果步幅增加到 2，输出内容会怎么样。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积-步长2.png" alt="img"></p>
<p>所以，正如你能想到的，感受野移动了两个单元，输出内容同样也会减小。注意，如果试图把我们的步幅设置成 3，那我们就会难以调节间距并确保感受野与输入图像匹配。正常情况下，程序员如果想让接受域重叠得更少并且想要更小的空间维度（spatial dimensions）时，他们会增加步幅。</p>
<p><strong>填充值是什么呢？</strong></p>
<p>在此之前，想象一个场景：当你把 5 x 5 x 3 的过滤器用在 32 x 32 x 3 的输入上时，会发生什么？输出的大小会是 28 x 28 x 3。注意，这里空间维度减小了。如果我们继续用卷积层，尺寸减小的速度就会超过我们的期望。在网络的早期层中，我们想要尽可能多地保留原始输入内容的信息，这样我们就能提取出那些低层的特征。比如说我们想要应用同样的卷积层，但又想让输出量维持为 32 x 32 x 3 。为做到这点，我们可以对这个层应用大小为 2 的零填充（zero padding）。零填充在输入内容的边界周围补充零。如果我们用两个零填充，就会得到一个 36 x 36 x 3 的输入卷。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/填充值.png" style="zoom:80%;"></p>
<p>如果我们在输入内容的周围应用两次零填充，那么输入量就为 32×32×3。然后，当我们应用带有 3 个 5×5×3 的过滤器，以 1 的步幅进行处理时，我们也可以得到一个 32×32×3 的输出</p>
<p>如果你的步幅为 1，而且把零填充设置为</p>
<p>\begin{flalign}<br>        Zero~Padding = (K-1)/2<br>        \end{flalign}</p>
<p>K 是过滤器尺寸，那么输入和输出内容就总能保持一致的空间维度。</p>
<p>计算任意给定卷积层的输出的大小的公式是</p>
<p>\begin{flalign}<br>        O = (W-K+2P)/S + 1<br>        \end{flalign}</p>
<p>其中 O 是输出尺寸，K 是过滤器尺寸，P 是填充，S 是步幅。</p>
<h4 id="卷积的计算"><a href="#卷积的计算" class="headerlink" title="卷积的计算"></a>卷积的计算</h4><p>下面蓝色矩阵周围有一圈灰色的框，那些就是上面所说到的填充值</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积的计算.png" style="zoom:80%;"></p>
<p>这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为 2。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积的计算-2.png" style="zoom:80%;"></p>
<p>蓝色的矩阵(输入图像)对粉色的矩阵（filter）进行矩阵内积计算并将三个内积运算的结果与偏置值 b 相加（比如上面图的计算：2+（-2+1-2）+（1-2-2） + 1= 2 - 3 - 3 + 1 = -3），计算后的值就是绿框矩阵的一个元素。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积层-降维.png" style="zoom:80%;"></p>
<p>下面的动态图形象地展示了卷积层的计算过程：</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积层计算过程.gif" style="zoom: 80%"></p>
<h4 id="参数共享机制"><a href="#参数共享机制" class="headerlink" title="参数共享机制"></a>参数共享机制</h4><p>在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。神经元就是图像处理中的滤波器，比如边缘检测专用的 Sobel 滤波器，即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。</p>
<p>需要估算的权重个数减少: AlexNet 1 亿 =&gt; 3.5w</p>
<p>一组固定的权重和不同窗口内数据做内积: 卷积</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/卷积效果图.png" style="zoom:80%;"></p>
<h3 id="非线性层（或激活层）"><a href="#非线性层（或激活层）" class="headerlink" title="非线性层（或激活层）"></a>非线性层（或激活层）</h3><p>把卷积层输出结果做非线性映射。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/激活层.png" style="zoom:80%;"></p>
<p>CNN 采用的激活函数一般为 ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/激活函数图像.png" style="zoom:80%;"></p>
<p><strong>激励层的实践经验：</strong></p>
<p>①不要用 sigmoid！不要用 sigmoid！不要用 sigmoid！</p>
<p>② 首先试 RELU，因为快，但要小心点</p>
<p>③ 如果 2 失效，请用 Leaky ReLU 或者 Maxout</p>
<p>④ 某些情况下 tanh 倒是有不错的结果，但是很少</p>
<p>参见 Geoffrey Hinton（即深度学习之父）的论文：Rectified Linear Units Improve Restricted Boltzmann Machines <strong>墙裂推荐此论文！</strong></p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。</p>
<p>简而言之，如<strong>果输入是图像的话，那么池化层的最主要作用就是压缩图像。</strong></p>
<p>池化层的具体作用：</p>
<ul>
<li><strong>特征不变性</strong>，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的 resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</li>
<li><strong>特征降维</strong>，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。</li>
<li>在一定程度上<strong>防止过拟合</strong>，更方便优化。</li>
</ul>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/池化层.png" style="zoom:80%;"></p>
<p>池化层用的方法有 Max pooling 和 average pooling，而实际用的较多的是 Max pooling。这里就说一下 Max pooling，其实思想非常简单。</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/池化层方法.png" style="zoom:80%;"></p>
<p>对于每个 2 <em> 2 的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个 2 </em> 2 窗口中最大的数是 6，那么输出矩阵的第一个元素就是 6，如此类推。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的：</p>
<p><img src="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8BCNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89CNN%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/全连接层.PNG" style="zoom:80%;"></p>
<p>一般 CNN 结构依次为</p>
<ul>
<li>INPUT</li>
<li>[ [ CONV -&gt; RELU ] N -&gt; POOL ? ] M</li>
<li>[ FC -&gt; RELU ] * K</li>
<li>FC</li>
</ul>
<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a><strong>说明</strong></h1><h2 id="训练算法"><a href="#训练算法" class="headerlink" title="训练算法"></a>训练算法</h2><ul>
<li>同一般机器学习算法，先定义 Loss function，衡量和实际结果之间差距。</li>
<li>找到最小化损失函数的 W 和 b， CNN 中用的算法是 SGD（随机梯度下降）。</li>
</ul>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点</strong></p>
<ul>
<li>共享卷积核，对高维数据处理无压力</li>
<li>无需手动选取特征，训练好权重，即得特征分类效果好</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>需要调参，需要大样本量，训练最好要 GPU</li>
<li>物理含义不明确（也就说，我们并不知道没个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”）</li>
</ul>
<h2 id="典型-CNN"><a href="#典型-CNN" class="headerlink" title="典型 CNN"></a>典型 CNN</h2><ul>
<li>LeNet，这是最早用于数字识别的 CNN</li>
<li>AlexNet， 2012 ILSVRC 比赛远超第 2 名的 CNN，比LeNet 更深，用多层小卷积层叠加替换单大卷积层。</li>
<li>ZF Net， 2013 ILSVRC 比赛冠军</li>
<li>GoogLeNet， 2014 ILSVRC 比赛冠军</li>
<li>VGGNet， 2014 ILSVRC 比赛中的模型，图像识别略差于 GoogLeNet，但是在很多图像转化学习问题(比如 object detection)上效果奇好</li>
</ul>
<h2 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine-tuning"></a>fine-tuning</h2><p><strong>何谓 fine-tuning？</strong></p>
<p>fine-tuning 就是使用已用于其他目标、预训练好模型的权重或者部分权重，作为初始值开始训练。</p>
<p>那为什么我们不用随机选取选几个数作为权重初始值？原因很简单，第一，自己从头训练卷积神经网络容易出现问题；第二，fine-tuning 能很快收敛到一个较理想的状态，省时又省心。</p>
<p><strong>那 fine-tuning 的具体做法是？</strong></p>
<ul>
<li>复用相同层的权重，新定义层取随机权重初始值</li>
<li>调大新定义层的的学习率，调小复用层学习率</li>
</ul>
<h2 id="常用框架"><a href="#常用框架" class="headerlink" title="常用框架"></a>常用框架</h2><p><strong>Caffe</strong></p>
<ul>
<li>源于 Berkeley 的主流 CV 工具包，支持 C++,python,matlab</li>
<li>Model Zoo 中有大量预训练好的模型供使用</li>
</ul>
<p><strong>PyTorch</strong></p>
<ul>
<li>Facebook 用的卷积神经网络工具包</li>
<li>通过时域卷积的本地接口，使用非常直观</li>
<li>定义新网络层简单</li>
</ul>
<p><strong>TensorFlow</strong></p>
<ul>
<li>Google 的深度学习框架</li>
<li>TensorBoard 可视化很方便</li>
<li>数据和模型并行化好，速度快</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h1><p>卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。</p>
<p>CNN 一个非常重要的特点就是头重脚轻（越往输入权值越小，越往输出权值越多），呈现出一个倒三角的形态，这就很好地避免了 BP 神经网络中反向传播的时候梯度损失得太快。</p>
<p>卷积神经网络 CNN 主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于 CNN 的特征检测层通过训练数据进行学习，所以在使用 CNN 时，避免了显式的特征抽取，而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（二）Pytorch官方demo（LeNet）</title>
    <url>/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/</url>
    <content><![CDATA[<p>因为之前一直没有搭建conda的环境，笔记也是后来补充，过程忘得七七八八了，就直接从demo代码开始记录。</p>
<p>By the way, <strong>Anaconda Navigator用管理员打开下载包更能省事</strong>。</p>
<h1 id="LeCun的LeNet（1998）网络架构"><a href="#LeCun的LeNet（1998）网络架构" class="headerlink" title="LeCun的LeNet（1998）网络架构"></a><strong>LeCun的LeNet（1998）网络架构</strong></h1><p>Pytorch Tensor的通道排序： <strong>[ batch, channel, height, width ]</strong></p>
<ul>
<li>batch：一批图像的个数，如图中示例，表示有32张图片</li>
</ul>
<p>又因为本次使用的官方数据集为CIFAR10（官方提供），是彩色图片，所以深度为3（RGB）</p>
<ul>
<li>channel：3（深度）    </li>
<li>height：32（高度）    </li>
<li>width：32（宽度）</li>
</ul>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/LeCun的LeNet（1988）网络架构.PNG" alt="LeCun的LeNet（1988）网络架构"></p>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><p>工程目录如下（data数据集的下载方式后文补充）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── Test1_official_demo</span><br><span class="line">	├── model.py（模型文件）  </span><br><span class="line">	├── train.py（调用模型训练）  </span><br><span class="line">	├── predict.py（调用模型进行预测） </span><br><span class="line">	└── data  </span><br><span class="line">		└── data数据集</span><br></pre></td></tr></table></figure>
<h2 id="模型文件model-py"><a href="#模型文件model-py" class="headerlink" title="模型文件model.py"></a>模型文件model.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">5</span>)  </span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line">        x = F.relu(self.conv2(x)) </span><br><span class="line">        x = self.pool2(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>用的两个包都是pytorch的包，虽然下载的是pytorch的包，但实际使用的时候，导入的还是pytorch.XX</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<h3 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a>代码解释</h3><h4 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h4><p>新建一个类，继承于nn.Module这个父类，类中实现两个方法：<strong>init</strong>()和forward()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module): </span><br></pre></td></tr></table></figure>
<p>在初始化函数中，会实现在搭建网络过程中所需要使用到的一些网络层结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br></pre></td></tr></table></figure>
<p>super函数：因为在定义类的过程中继承了nn.Module这个父类，而super函数是用来解决多重继承中调用父类方法中可能出现的一系列问题，简而言之就是——只要涉及到多继承，一般都会使用super函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python2.0版本</span></span><br><span class="line"><span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line"><span class="comment"># python3.0版本；在3.0版本下，2.0版本的方式的也能用</span></span><br><span class="line"><span class="comment"># super().__init__()</span></span><br></pre></td></tr></table></figure>
<p>conv1相当于图片中的第一个Convolutions，通过nn.Conv2d()函数来定义卷积层。</p>
<p>3:图片深度，16：使用了16个卷积核 = 输出16维度的特征矩阵，5：卷积核大小5×5</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">5</span>) </span><br></pre></td></tr></table></figure>
<p>pool1定义下采样层，相当于图片中第一个Subsampling</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>)</span><br><span class="line">self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>全连接层输入是一个一维向量，需要将特征矩阵展平，图片显示第一层的节点个数是120</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.fc1 = nn.Linear(<span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br></pre></td></tr></table></figure>
<p>第二层的输入是第一层的输出 120，输出为84</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br></pre></td></tr></table></figure>
<p>第三层的输入是第二层的输出 84，输出为10（<strong>需要根据实际的训练集进行修改，本次例子中采用CIFAR10数据集，具有10个类别的分类任务，因此这里设置为10</strong>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h4 id="forward函数"><a href="#forward函数" class="headerlink" title="forward函数"></a>forward函数</h4><p>forward函数中定义正向传播的过程。x代表输入的数据，数据指的是Tensor的通道排序：[batch, channel, height, width]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br></pre></td></tr></table></figure>
<p>relu激活函数。input(3, 32, 32) output(16, 28, 28)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = F.relu(self.conv1(x)) </span><br></pre></td></tr></table></figure>
<p>经过Maxpool2d处理，大小缩减为原来的一半，深度不变</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = self.pool1(x)	<span class="comment"># output(16, 14, 14)</span></span><br><span class="line">x = F.relu(self.conv2(x))    <span class="comment"># output(32, 10, 10)</span></span><br><span class="line">x = self.pool2(x)            <span class="comment"># output(32, 5, 5)</span></span><br></pre></td></tr></table></figure>
<p>数据经过view函数将它展成一维向量，-1代表第一个维度进行自动推理（batch），第二个维度值展平之后的节点个数。view中第一个参数为-1，代表动态调整这个维度上的元素个数，以保证元素的总数不变</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.view(-<span class="number">1</span>, <span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>)       <span class="comment"># output(32*5*5)</span></span><br><span class="line">x = F.relu(self.fc1(x))      <span class="comment"># output(120)</span></span><br><span class="line">x = F.relu(self.fc2(x))      <span class="comment"># output(84)</span></span><br><span class="line">x = self.fc3(x)              <span class="comment"># output(10)</span></span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Con2d函数"><a href="#Con2d函数" class="headerlink" title="Con2d函数"></a>Con2d函数</h3><p>ctrl+鼠标左键点击Conv2d，pycharm自动跳转到Conv2d的函数定义</p>
<p>简介：使用2d卷积的方法对输入的数据进行处理</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Conv2d函数定义1.png" alt="Conv2d函数定义1"></p>
<p>其中<strong>init</strong>()函数参数定义如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    <span class="comment"># 输入特征矩阵的深度</span></span></span><br><span class="line"><span class="params">    <span class="comment"># 例子中为彩色图像，所以in_channels应该为3</span></span></span><br><span class="line"><span class="params">    in_channels: <span class="built_in">int</span>,  </span></span><br><span class="line"><span class="params">    <span class="comment"># 使用卷积核的个数</span></span></span><br><span class="line"><span class="params">    <span class="comment"># 使用几个卷积核，那么就会生成一个深度为多少维的特征矩阵</span></span></span><br><span class="line"><span class="params">    out_channels: <span class="built_in">int</span>,  </span></span><br><span class="line"><span class="params">    <span class="comment"># 卷积核的大小</span></span></span><br><span class="line"><span class="params">    kernel_size: _size_2_t,</span></span><br><span class="line"><span class="params">    <span class="comment"># 步距，默认为1</span></span></span><br><span class="line"><span class="params">    stride: _size_2_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">    <span class="comment"># 四周进行补0处理，默认为0</span></span></span><br><span class="line"><span class="params">    padding: <span class="type">Union</span>[<span class="built_in">str</span>, _size_2_t] = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">    dilation: _size_2_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">    groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">    <span class="comment"># 偏置，默认使用</span></span></span><br><span class="line"><span class="params">    bias: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    padding_mode: <span class="built_in">str</span> = <span class="string">&#x27;zeros&#x27;</span>,</span></span><br><span class="line"><span class="params">    device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>
<p>2.1.1在<a href="https://pytorch.org/docs/stable/index.html">pytorch官方文档</a>中ctrl+F查找Conv2d函数，得到在官方的解释</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Conv2d函数定义2.PNG" alt="Conv2d函数定义2"></p>
<p>每个参数的解释</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Conv2d函数定义3参数解释.PNG" alt="Conv2d函数定义3参数解释"></p>
<p>以及卷积输出维度的变化</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Conv2d函数定义4卷积输出维度的变化.png" alt="Conv2d函数定义4卷积输出维度的变化"></p>
<p>实际相当于</p>
<p>\begin{flalign}<br>        N = (W-F+2P)/S + 1<br>        \end{flalign}</p>
<ul>
<li>输入图片大小：W×W</li>
<li>Filter大小：F×F</li>
<li>步长：S</li>
<li>padding的像素数：P</li>
</ul>
<h3 id="MaxPool2d函数"><a href="#MaxPool2d函数" class="headerlink" title="MaxPool2d函数"></a>MaxPool2d函数</h3><p>简介：没有初始化函数，因为继承来自_MaxPoolNd父类</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Maxpool2d函数定义.png" alt="Maxpool2d函数定义"></p>
<p>跳转到父类_MaxPoolNd</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Maxpool2d父类_MaxPoolNd函数定义.png" alt="Maxpool2d父类_MaxPoolNd函数定义"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">     <span class="comment"># 池化核的大小</span></span></span><br><span class="line"><span class="params">     kernel_size: _size_any_t, </span></span><br><span class="line"><span class="params">     <span class="comment"># 步距，默认和kernel_size = _size_any_t一致</span></span></span><br><span class="line"><span class="params">     stride: <span class="type">Optional</span>[_size_any_t] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">     padding: _size_any_t = <span class="number">0</span>, </span></span><br><span class="line"><span class="params">     dilation: _size_any_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">     return_indices: <span class="built_in">bool</span> = <span class="literal">False</span>, </span></span><br><span class="line"><span class="params">     ceil_mode: <span class="built_in">bool</span> = <span class="literal">False</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>
<h3 id="测试结果（debug）"><a href="#测试结果（debug）" class="headerlink" title="测试结果（debug）"></a>测试结果（debug）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input1 = torch.rand([<span class="number">32</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">model1 = LeNet()</span><br><span class="line"><span class="built_in">print</span>(model1)</span><br><span class="line">output = model1(input1)</span><br></pre></td></tr></table></figure>
<p>终端打印信息</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/model终端打印信息.png" alt="model终端打印信息"></p>
<h2 id="调用模型训练train-py"><a href="#调用模型训练train-py" class="headerlink" title="调用模型训练train.py"></a>调用模型训练train.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line"><span class="comment">#50000张训练照片</span></span><br><span class="line">transet = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(transet, batch_size=<span class="number">36</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#10000张测试图片</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_data_iter = <span class="built_in">iter</span>(testloader)</span><br><span class="line">test_image, test_label = test_data_iter.__next__()</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;flog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>   <span class="comment">#unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment">#print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[test_label[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"><span class="comment">#show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(test_image))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet()</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, start=<span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = loss_function(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印的过程</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">499</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                outputs = net(test_image)  <span class="comment"># [batch, 10]</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                accuracy = torch.eq(predict_y, test_label).<span class="built_in">sum</span>().item() / test_label.size(<span class="number">0</span>) </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">                        (epoch + <span class="number">1</span>, step + <span class="number">1</span>, running_loss / <span class="number">500</span>, accuracy))</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"><span class="comment"># 对模型进行保存</span></span><br><span class="line">save_path = <span class="string">&#x27;./Lenet.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), save_path)</span><br></pre></td></tr></table></figure>
<h3 id="下载及测试数据集"><a href="#下载及测试数据集" class="headerlink" title="下载及测试数据集"></a>下载及测试数据集</h3><p>使用Compose函数将使用的一些预处理方法给打包成一个整体，首先通过CIFAR10导入数据集，将训练集的每一个图像transform预处理函数进行预处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<p>50000张训练照片，下载数据集download=True,下载成功后改download=False；root代表将数据集下载到什么地方，train=True时会导入CIFAR10训练集的样本；download=True自动下载；transform=transform对图像进行预处理</p>
<p>或者在当前文件夹下新建data文件夹，<a href="https://pan.baidu.com/s/1300Lz9xthY11o2jfTPpx3g?pwd=1234">直接下载</a>CIFAR10数据集并在data文件夹下解压</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transet = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<p>将数据集导入进来，分成一个个批次，这里指每一批随机拿出batch_size=36张图片进行训练；shuffle=True表示是否要将数据集打乱，一般为True；num_workers理解为载入数据的线程数，目前windows环境下也可以设置，但不能超过支持的线程个数，可以加快图片载入的速度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainloader = torch.utils.data.DataLoader(transet, batch_size=<span class="number">36</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>10000张测试图片，测试时将batch_size改为4</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">10000</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>iter函数是将更改生成的testloader转化为一个可迭代的迭代器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data_iter = <span class="built_in">iter</span>(testloader)</span><br></pre></td></tr></table></figure>
<p>通过<strong>next</strong>()方法获取到一批数据，可拿到图像及图像对应的标签值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_image, test_label = test_data_iter.__next__()</span><br></pre></td></tr></table></figure>
<p>导入标签，元组类型，不可更改，0，1，……，9</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;flog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>测试代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>   <span class="comment">#unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[test_label[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(test_image))</span><br></pre></td></tr></table></figure>
<p><strong>数据集下载</strong></p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/下载data数据集.png" alt="下载data数据集"></p>
<p><strong>ToTensor函数</strong></p>
<p>将PIL图像或者numpy数据转化为tensor。导入的原始图片，无论是PIL或者通过numpy导入（一般图像为高度，宽度，深度，每一个像素值都是[0, 255]），通过ToTensor函数之后，将shape（长宽高的值）转化为[0.0, 1.0]</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/ToTensor.png" alt="ToTensor"></p>
<p><strong>Normalize函数</strong></p>
<p>使用均值和标准差转化tensor，计算方式 ： 输出 = （原始数据 - 均值）/ 标准差</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/Normalize.png" alt="Normalize"></p>
<p><strong>测试结果</strong></p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/测试结果.png" alt="测试结果"></p>
<h3 id="构造模型"><a href="#构造模型" class="headerlink" title="构造模型"></a>构造模型</h3><p>注释掉测试的输出代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = LeNet()</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<p>将训练集训练多少次，这里为5次</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br></pre></td></tr></table></figure>
<p>用来累计在训练过程中的损失</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">running_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>遍历训练集样本；enumerate函数不仅能返回每一批的数据data，还能返回这一批data所对应的步数index，相当于C++中的枚举</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, start=<span class="number">0</span>):</span><br></pre></td></tr></table></figure>
<p>输入的图像及标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs, labels = data</span><br></pre></td></tr></table></figure>
<p>将历史损失梯度清零；</p>
<p>为什么每计算一个batch，就需要调用一次optimizer.zero._grad()？</p>
<p>如果不清除历史梯度，就会对计算的历史梯度进行累加（通过这个特性你能够变相实现一个很大batch数值的训练），主要还是硬件设备受限，防止爆内存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>将我们得到的数的图片输入到网络进行正向传播，得到输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outputs = net(inputs)</span><br></pre></td></tr></table></figure>
<p>通过定义的loss_function来计算损失，outputs：网络预测的值，labels：真实标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = loss_function(outputs, labels)</span><br></pre></td></tr></table></figure>
<p>对loss进行反向传播</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<p>通过优化器optimizer中step函数进行参数更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>打印的过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">running_loss += loss.item()</span><br></pre></td></tr></table></figure>
<p>每隔500步，打印一次数据信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">499</span>:</span><br></pre></td></tr></table></figure>
<p>with是一个上下文管理器，意思是在接下来的计算过程中，不要去计算每个节点的误差损失梯度；否则会自动生成前向的传播图，会占用大量内存，测试时应该禁用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = net(test_image)  <span class="comment"># [batch, 10]</span></span><br></pre></td></tr></table></figure>
<p>predict_y寻找outputs中数值最大的，也就是最有可能的标签类型；dim：第几个维度，第0个维度是batch，第1个维度指10个标签结果；[1]指只需要知道index即可，不需要知晓具体的值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>将预测的标签类别和真实的标签类别进行比较，相同的地方返回1，不相同返回0；使用求和函数，得出在本次预测对了多少个样本；tensor得到的并不是数值，item()才可以拿到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据准确率</span></span><br><span class="line">accuracy = torch.eq(predict_y, test_label).<span class="built_in">sum</span>().item() / test_label.size(<span class="number">0</span>)    </span><br></pre></td></tr></table></figure>
<p>迭代到第几轮，在某一轮的多少步，训练过程中的累加误差，测试样本的准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">      (epoch + <span class="number">1</span>, step + <span class="number">1</span>, running_loss / <span class="number">500</span>, accuracy))</span><br><span class="line">running_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>debug一下，会暂时没反应，点开任务管理器，CPU已经100%，等待一段时间会有打印结果</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/CPU满.png" style="zoom:50%;"></p>
<p><strong>打印结果</strong></p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/模型预测结果.png" alt="模型预测结果"></p>
<p>训练完之后在当前目录下生成一个模型权重文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对模型进行保存</span></span><br><span class="line">save_path = <span class="string">&#x27;./Lenet.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), save_path)</span><br></pre></td></tr></table></figure>
<h2 id="调用模型进行预测predict-py"><a href="#调用模型进行预测predict-py" class="headerlink" title="调用模型进行预测predict.py"></a>调用模型进行预测predict.py</h2><p>在网上随便下载一张分类在模型中的图片，存放在当前文件夹下取名1.jpg</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">     <span class="comment">#将图像缩放在32×32的大小</span></span><br><span class="line">    [transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),   </span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化LeNet</span></span><br><span class="line">net = LeNet()</span><br><span class="line"><span class="comment"># 使用load_state_dict载入更改保存的Lenet.pth</span></span><br><span class="line">net.load_state_dict(torch.load(<span class="string">&#x27;Lenet.pth&#x27;</span>))</span><br><span class="line"><span class="comment"># 载入之后，根据python的PIL import Image的模块，去载入图像</span></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">im = transform(im)  <span class="comment"># [C, H, W]</span></span><br><span class="line"><span class="comment"># Tensor规定时需要4个维度，但transform输出仅有3个</span></span><br><span class="line"><span class="comment"># 因此需要在index = 0处增加一个新的维度</span></span><br><span class="line">im = torch.unsqueeze(im, dim=<span class="number">0</span>)  <span class="comment"># [N, C, H, W]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 将图像传入网络</span></span><br><span class="line">    outputs = net(im)</span><br><span class="line">    <span class="comment"># 寻找输出中的最大值</span></span><br><span class="line">    predict = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].numpy()</span><br><span class="line"><span class="comment"># 将index索引传入到classes，得出类别</span></span><br><span class="line"><span class="built_in">print</span>(classes[<span class="built_in">int</span>(predict)])</span><br></pre></td></tr></table></figure>
<h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a><strong>预测结果</strong></h3><p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/预测结果.png" alt="预测结果"></p>
<h3 id="假设：将max函数改为softmax函数"><a href="#假设：将max函数改为softmax函数" class="headerlink" title="假设：将max函数改为softmax函数"></a><strong>假设：将max函数改为softmax函数</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 将图像传入网络</span></span><br><span class="line">    outputs = net(im)</span><br><span class="line">    <span class="comment"># 寻找输出中的最大值</span></span><br><span class="line">    predict = torch.softmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(predict)</span><br><span class="line"><span class="comment">#classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;,&#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span></span><br></pre></td></tr></table></figure>
<p>输出结果为：经过softmax处理之后的概率分布</p>
<p>预测index  = 0的值概率为93.0%，后面都很小，忽略不看</p>
<p><img src="/2023/04/29/pytorch%E5%AE%98%E6%96%B9demo/softmax输出结果.png" alt="softmax输出结果"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>首先，回顾之前学习LeNet，通过pytorch搭建LeNet模型；</p>
<p>接着，介绍并下载了CIFAR10数据集，对数据集进行预处理，查看图片并导入到LeNet模型，定义了损失函数、优化器；</p>
<p>最后，进行了网络的训练，对训练好的权重进行保存，通过预测脚本，调用保存的模型权重进行预测。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（八）使用pytorch搭建GoogLeNet网络</title>
    <url>/2023/05/10/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAGoogLeNet%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="pytorch搭建GoogLeNet"><a href="#pytorch搭建GoogLeNet" class="headerlink" title="pytorch搭建GoogLeNet"></a>pytorch搭建GoogLeNet</h1><p>model模型搭建对照以下参数表及网络架构图</p>
<p><img src="/2023/05/10/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAGoogLeNet%E7%BD%91%E7%BB%9C/GoogLeNet网络参数.png" alt="GoogLeNet网络参数"></p>
<p><img src="/2023/05/10/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAGoogLeNet%E7%BD%91%E7%BB%9C/GoogLeNet网络架构.png" alt="GoogLeNet网络架构"></p>
<h2 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h2><p>相比于AlexNet 和 VggNet 只有卷积层和全连接层这两种结构，GoogLeNet多了Inception和辅助分类器（Auxiliary Classifier），而 Inception和辅助分类器也是由多个卷积层和全连接层组合的，因此在定义模型时可以将<strong>卷积、Inception 、辅助分类器</strong>定义成不同的类，调用时更加方便。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, self).__init__()</span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line"></span><br><span class="line">        self.conv1 = BasicConv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2 = BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = BasicConv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception3a = Inception(<span class="number">192</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.inception3b = Inception(<span class="number">256</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception4a = Inception(<span class="number">480</span>, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4b = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4c = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4d = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4e = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception5a = Inception(<span class="number">832</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.inception5b = Inception(<span class="number">832</span>, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits:</span><br><span class="line">            self.aux1 = InceptionAux(<span class="number">512</span>, num_classes)</span><br><span class="line">            self.aux2 = InceptionAux(<span class="number">528</span>, num_classes)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 3 x 224 x 224</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="comment"># N x 64 x 112 x 112</span></span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        <span class="comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="comment"># N x 192 x 56 x 56</span></span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># N x 192 x 28 x 28</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        <span class="comment"># N x 256 x 28 x 28</span></span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        <span class="comment"># N x 480 x 28 x 28</span></span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        <span class="comment"># N x 480 x 14 x 14</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux1 = self.aux1(x)</span><br><span class="line"></span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        <span class="comment"># N x 528 x 14 x 14</span></span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux2 = self.aux2(x)</span><br><span class="line"></span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        <span class="comment"># N x 832 x 14 x 14</span></span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        <span class="comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        <span class="comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        <span class="comment"># N x 1024 x 7 x 7</span></span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment"># N x 1024 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># N x 1024</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># N x 1000 (num_classes)</span></span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1 = self.branch1(x)</span><br><span class="line">        branch2 = self.branch2(x)</span><br><span class="line">        branch3 = self.branch3(x)</span><br><span class="line">        branch4 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        self.averagePool = nn.AvgPool2d(kernel_size=<span class="number">5</span>, stride=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># output[batch, 128, 4, 4]</span></span><br><span class="line">        self.conv = BasicConv2d(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14</span></span><br><span class="line">        x = self.averagePool(x)</span><br><span class="line">        <span class="comment"># aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># N x 128 x 4 x 4</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="comment"># N x 2048</span></span><br><span class="line">        x = F.relu(self.fc1(x), inplace=<span class="literal">True</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="comment"># N x 1024</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># N x num_classes</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="BasicConv2d类模板"><a href="#BasicConv2d类模板" class="headerlink" title="BasicConv2d类模板"></a>BasicConv2d类模板</h3><p>定义BasicConv2d类，包含了卷积层和ReLU激活函数的卷积模板，继承nn.Module，将卷积层和ReLU激活函数打包在一起</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Inception类模板"><a href="#Inception类模板" class="headerlink" title="Inception类模板"></a>Inception类模板</h3><p>定义Inception类模板，同样继承于nn.Module父类。初始函数中输入Inception函数所需要使用的参数：</p>
<ul>
<li><code>in_channels</code>：输入特征矩阵的深度；</li>
<li><code>ch1X1</code>：第一个分支中1X1卷积核的个数；</li>
<li><code>ch3X3red</code>：第二个分支中1X1卷积核的个数；</li>
<li><code>ch3x3</code>：第二个分支中3X3卷积核的个数；</li>
<li><code>ch5x5red</code>：第三个分支中1X1卷积核的个数；</li>
<li><code>ch5x5</code>：第三个分支中5X5卷积核的个数；</li>
<li><code>pool_proj</code>：第四个分支中1X1卷积核的个数</li>
</ul>
<p>分支1：self.branch1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>分支2：self.branch2</p>
<p>padding=1：保证输出大小等于输入大小。output_size = (input_size - 3 + 2*1)/1 + 1 = input_size</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.branch2 = nn.Sequential(</span><br><span class="line">    BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">    BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>分支3：self.branch3</p>
<p>padding=2：保证输出大小等于输入大小。output_size=(input_size - 5 + 2 * 2) / 1 + 1 = input_size</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.branch3 = nn.Sequential(</span><br><span class="line">    BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">    BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>分支4：self.branch4</p>
<p>padding=1：保证输出大小等于输入大小。output_size = (input_size - 3 + 2*1)/1 + 1 = input_size。池化操作不会改变特征矩阵的深度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.branch4 = nn.Sequential(</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    BasicConv2d(in_channels, pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在forward正向传播函数中，将输出放入列表中，再通过torch.cat（concatention）函数对输出进行在深度方向的合并</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outputs = [branch1, branch2, branch3, branch4]</span><br></pre></td></tr></table></figure>
<p>在pytorch中，通道[batch, channel, hight, width]，因为延深度方向拼接，因此位置在第1个（0，1，2，3）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="InceptionAux辅助分类器"><a href="#InceptionAux辅助分类器" class="headerlink" title="InceptionAux辅助分类器"></a>InceptionAux辅助分类器</h3><p>当实例化一个模型model之后，可以通过model.train()和model.eval()来控制模型的状态，在model.train()模式下self.training=True，在model.eval()模式下self.training=False</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = F.dropout(x, <span class="number">0.5</span>, training=self.training)</span><br></pre></td></tr></table></figure>
<h3 id="GoogLeNet类"><a href="#GoogLeNet类" class="headerlink" title="GoogLeNet类"></a>GoogLeNet类</h3><p>aux_logits：是否使用辅助分类器；init_weights：是否对权重进行初始化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">False</span></span>):</span><br></pre></td></tr></table></figure>
<p>为了将特征矩阵的高宽缩减为原来的一半，（224 - 7 + 2 * 3)/2 + 1 = 112.5在pytorch中默认向下取整，也就是112。当ceil_mode-True，表示结果数据如果为小数的话，那么向上取整，反之向下取整</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = BasicConv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>)</span><br><span class="line">self.maxpool1 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Inception的深度可以用过查看参数表格得到，或通过上一个Inception层的四个分支的特征矩阵深度加起来得到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.inception4a = Inception(<span class="number">480</span>, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">self.inception4b = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">self.inception4c = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">self.inception4d = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">self.inception4e = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">self.maxpool4 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>平均池化下采样层，AdaptiveAvgPool2d自适应平均池化下采样：无论输入特征矩阵高宽是多少，都能得到指定的特征矩阵的高和宽</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>输入的展平后的向量节点个数为1024，输出的节点个数是num_classes</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.fc = nn.Linear(<span class="number">1024</span>, num_classes)</span><br></pre></td></tr></table></figure>
<p>在forward正向传播函数中，self.training判断当前模型是处于训练模式还是验证模式，如果是训练模式的话九尾True，反之为False</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">    aux1 = self.aux1(x)</span><br></pre></td></tr></table></figure>
<h2 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GoogLeNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br><span class="line"></span><br><span class="line">data_transform = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                 transforms.RandomHorizontalFlip(),</span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]),</span><br><span class="line">    <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])&#125;</span><br><span class="line"></span><br><span class="line">data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))</span><br><span class="line">image_path = os.path.join(data_root, <span class="string">&quot;data_set&quot;</span>, <span class="string">&quot;flower_data&quot;</span>)</span><br><span class="line"><span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;train&quot;</span>),</span><br><span class="line">                                     transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">train_num = <span class="built_in">len</span>(train_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line">flower_list = train_dataset.class_to_idx</span><br><span class="line">cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line"><span class="comment"># write dict into json file</span></span><br><span class="line">json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                           batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                           num_workers=nw)</span><br><span class="line"></span><br><span class="line">validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>),</span><br><span class="line">                                        transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                              batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                              num_workers=nw)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(train_num,</span><br><span class="line">                                                                       val_num))</span><br><span class="line"></span><br><span class="line"><span class="comment"># test_data_iter = iter(validate_loader)</span></span><br><span class="line"><span class="comment"># test_image, test_label = test_data_iter.next()</span></span><br><span class="line"></span><br><span class="line">net = GoogLeNet(num_classes=<span class="number">5</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 如果要使用官方的预训练权重，注意是将权重载入官方的模型，不是我们自己实现的模型</span></span><br><span class="line"><span class="comment"># 官方的模型中使用了bn层以及改了一些参数，不能混用</span></span><br><span class="line"><span class="comment"># import torchvision</span></span><br><span class="line"><span class="comment"># net = torchvision.models.googlenet(num_classes=5)</span></span><br><span class="line"><span class="comment"># model_dict = net.state_dict()</span></span><br><span class="line"><span class="comment"># # 预训练权重下载地址: https://download.pytorch.org/models/googlenet-1378be20.pth</span></span><br><span class="line"><span class="comment"># pretrain_model = torch.load(&quot;googlenet.pth&quot;)</span></span><br><span class="line"><span class="comment"># del_list = [&quot;aux1.fc2.weight&quot;, &quot;aux1.fc2.bias&quot;,</span></span><br><span class="line"><span class="comment">#             &quot;aux2.fc2.weight&quot;, &quot;aux2.fc2.bias&quot;,</span></span><br><span class="line"><span class="comment">#             &quot;fc.weight&quot;, &quot;fc.bias&quot;]</span></span><br><span class="line"><span class="comment"># pretrain_dict = &#123;k: v for k, v in pretrain_model.items() if k not in del_list&#125;</span></span><br><span class="line"><span class="comment"># model_dict.update(pretrain_dict)</span></span><br><span class="line"><span class="comment"># net.load_state_dict(model_dict)</span></span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0003</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line">save_path = <span class="string">&#x27;./googleNet.pth&#x27;</span></span><br><span class="line">train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    net.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    train_bar = tqdm(train_loader, file=sys.stdout)</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        logits, aux_logits2, aux_logits1 = net(images.to(device))</span><br><span class="line">        loss0 = loss_function(logits, labels.to(device))</span><br><span class="line">        loss1 = loss_function(aux_logits1, labels.to(device))</span><br><span class="line">        loss2 = loss_function(aux_logits2, labels.to(device))</span><br><span class="line">        loss = loss0 + loss1 * <span class="number">0.3</span> + loss2 * <span class="number">0.3</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>, epochs, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># validate</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">            val_images, val_labels = val_data</span><br><span class="line">            outputs = net(val_images.to(device))</span><br><span class="line">            predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    val_accurate = acc / val_num</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">          (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">        best_acc = val_accurate</span><br><span class="line">        torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>train.py代码和前期VGG和AlexNet代码内容大同小异，有两点不同需要注意：</p>
<h3 id="实例化代码"><a href="#实例化代码" class="headerlink" title="实例化代码"></a>实例化代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = GoogLeNet(num_classes=<span class="number">5</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>有三个部分，分别是主干输出loss、两个辅助分类器输出loss（权重0.3）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logits, aux_logits2, aux_logits1 = net(images.to(device))</span><br><span class="line">loss0 = loss_function(logits, labels.to(device))</span><br><span class="line">loss1 = loss_function(aux_logits1, labels.to(device))</span><br><span class="line">loss2 = loss_function(aux_logits2, labels.to(device))</span><br><span class="line">loss = loss0 + loss1 * <span class="number">0.3</span> + loss2 * <span class="number">0.3</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<h2 id="predict-py"><a href="#predict-py" class="headerlink" title="predict.py"></a>predict.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GoogLeNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># load image</span></span><br><span class="line">img_path = <span class="string">&quot;../tulip.jpg&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">plt.imshow(img)</span><br><span class="line"><span class="comment"># [N, C, H, W]</span></span><br><span class="line">img = data_transform(img)</span><br><span class="line"><span class="comment"># expand batch dimension</span></span><br><span class="line">img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read class_indict</span></span><br><span class="line">json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">model = GoogLeNet(num_classes=<span class="number">5</span>, aux_logits=<span class="literal">False</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model weights</span></span><br><span class="line">weights_path = <span class="string">&quot;./googleNet.pth&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(weights_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(weights_path)</span><br><span class="line"></span><br><span class="line">missing_keys, unexpected_keys = model.load_state_dict(torch.load(weights_path, map_location=device), strict=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># predict class</span></span><br><span class="line">    output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">    predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">    predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                             predict[predict_cla].numpy())</span><br><span class="line">plt.title(print_res)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                              predict[i].numpy()))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>预测部分跟AlexNet和VGG类似，需要注意在<strong>实例化模型时不需要辅助分类器</strong>（因为辅助分类器目的在于在训练模型过程中防止过拟合的同时增强模型的正确率，当训练结束后，相关参数已经保存在权重文件中，因此在预测时，并不需要使用辅助分类器）</p>
<p>aux_logits=False不会构建辅助分类器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = GoogLeNet(num_classes=<span class="number">5</span>, aux_logits=<span class="literal">False</span>).to(device)</span><br></pre></td></tr></table></figure>
<p>strict默认为true，意思是精准匹配当前模型和需要载入的权重模型的结构。当设置为False后，现在搭建的GoogLeNet是没有辅助分类器的，所以于保存的模型结构会缺失一些结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">missing_keys, unexpected_keys = model.load_state_dict(torch.load(weights_path, map_location=device), strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>训练过程过于漫长，今后在时间充裕的时候可以试一试</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（四）使用pytorch搭建AlexNet并训练花分类数据集</title>
    <url>/2023/05/03/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAAlexNet%E5%B9%B6%E8%AE%AD%E7%BB%83%E8%8A%B1%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<h1 id="AlexNet网络架构"><a href="#AlexNet网络架构" class="headerlink" title="AlexNet网络架构"></a>AlexNet网络架构</h1><p><img src="/2023/05/03/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAAlexNet%E5%B9%B6%E8%AE%AD%E7%BB%83%E8%8A%B1%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86/AlexNet网络架构过程总结.png" alt="AlexNet网络架构过程总结"></p>
<h1 id="搭建AlexNet"><a href="#搭建AlexNet" class="headerlink" title="搭建AlexNet"></a>搭建AlexNet</h1><p>工程目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── Test2_alexnet</span><br><span class="line">	├── model.py（模型文件）  </span><br><span class="line">	├── train.py（调用模型训练）  </span><br><span class="line">	├── predict.py（调用模型进行预测） </span><br><span class="line">	└── class_indices.json（将索引和分类一一对应，代码运行之后会自动生成）</span><br><span class="line">└── data_set </span><br><span class="line">	├── split_data.py(自动将数据集划分成训练集train和验证集val（训练集：测试集 = 9 ：1）</span><br><span class="line">	└── flower_data</span><br><span class="line">		├── train(split_data.py分出来的)</span><br><span class="line">		├── flower_photos(split_data.py分出来的)</span><br><span class="line">		└── flower_photos</span><br><span class="line">			├── daisy 雏菊</span><br><span class="line">			├── dandelion 蒲公英</span><br><span class="line">			├── roses 玫瑰</span><br><span class="line">			├── sunflowers 向日葵</span><br><span class="line">			└── tulips 郁金香</span><br></pre></td></tr></table></figure>
<h2 id="模型文件model-py"><a href="#模型文件model-py" class="headerlink" title="模型文件model.py"></a>模型文件model.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">48</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">48</span>, <span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)   <span class="comment">#展平处理</span></span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>创建类AlexNet，继承于父类nn.Module</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br></pre></td></tr></table></figure>
<p>通过初始化函数来定义AlexNet网络在正向传播过程中所需要使用到的一些层结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># python2.0版本</span></span><br><span class="line">    <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">    <span class="comment"># python3.0版本</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br></pre></td></tr></table></figure>
<h3 id="nn-Sequential模块"><a href="#nn-Sequential模块" class="headerlink" title="nn.Sequential模块"></a>nn.Sequential模块</h3><p>这里与Pytorch官方demo不一样的是：使用到<strong>nn.Sequential模块</strong>。nn.Sequential能够将一系列的层结构进行打包，组合成一个新的结构，在这取名为features。features代表专门用于提取图像特征的结构。</p>
<p>为什么使用nn.Sequential模块？——精简代码，减少工作量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.features = nn.Sequential(</span><br></pre></td></tr></table></figure>
<h3 id="padding用法解释"><a href="#padding用法解释" class="headerlink" title="padding用法解释"></a>padding用法解释</h3><p>卷积核大小：11；卷积核个数原模型是96：由于数据集较小和加快运算速度，<strong>因此这里取一半48</strong>，经检测正确率相差不大；输入的图片是RGB的彩色图片：3；</p>
<p>padding有两种写的类型：一种是整型，一种是tuple类型。<strong>当padding=1时，代表在图片上下左右分别补一个单位的0</strong>。如果传入的是<strong>tuple(1, 2)：1代表上下方各补一行0；2表示左右两侧各补两列0</strong>。</p>
<p>如果想要实现第一层的padding在上一堂课中讲到，是在最左边补一列0，最上面补一行0，最右边补两列0，最下面补两行0。<strong>nn.ZeroPad2d((1,2,1,2))：左侧补一列，右侧补两列，上方补一行，下方补两行</strong>。</p>
<p>这里使用padding = 2，按照公式计算出来结果为55.25，在Pytorch中如果计算结果为小数，会自动将小数点去掉。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># input[3, 224, 224]    output[48, 55, 55]</span></span><br><span class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">48</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br></pre></td></tr></table></figure>
<p>inplace参数理解为Pytorch通过一种方法增加计算量，但降低内存使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"><span class="comment"># output[48, 27, 27]</span></span><br><span class="line">nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line"><span class="comment"># output[128, 27, 27]</span></span><br><span class="line">nn.Conv2d(<span class="number">48</span>, <span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"><span class="comment"># output[128, 13, 13]</span></span><br><span class="line">nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line"><span class="comment"># output[192, 13, 13]</span></span><br><span class="line">nn.Conv2d(<span class="number">128</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"><span class="comment"># output[192, 13, 13]</span></span><br><span class="line">nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"><span class="comment"># output[128, 13, 13]</span></span><br><span class="line">nn.Conv2d(<span class="number">192</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"><span class="comment"># output[128, 6, 6]</span></span><br><span class="line">nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br></pre></td></tr></table></figure>
<h3 id="classifier全连接层"><a href="#classifier全连接层" class="headerlink" title="classifier全连接层"></a>classifier全连接层</h3><p>classifier包含之后的三层全连接层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.classifier = nn.Sequential(</span><br><span class="line">    <span class="comment"># p代表失活比例，默认为0.5</span></span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># num_classes数据集类别的个数，5</span></span><br><span class="line">    nn.Linear(<span class="number">2048</span>, num_classes),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>当搭建网络过程中传入初始化权重init_weights=trus，会进入到初始化权重函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> init_weights:</span><br><span class="line">    self._initialize_weights()</span><br></pre></td></tr></table></figure>
<h3 id="forward函数"><a href="#forward函数" class="headerlink" title="forward函数"></a>forward函数</h3><p>forward函数中定义正向传播的过程。x代表输入的数据，数据指的是Tensor的通道排序：[batch, channel, height, width]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.features(x)</span><br><span class="line">    x = torch.flatten(x, start_dim=<span class="number">1</span>)   <span class="comment">#展平处理</span></span><br><span class="line">    x = self.classifier(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="initialize-weights函数"><a href="#initialize-weights函数" class="headerlink" title="_initialize_weights函数"></a>_initialize_weights函数</h3><p>初始化权重函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br></pre></td></tr></table></figure>
<p>遍历modules模块，modules定义中：返回一个迭代器，迭代器中会遍历网络中所有的模块。而言之，通过self.modules()，会迭代定义的每一个层结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br></pre></td></tr></table></figure>
<p>遍历层结构之后，判断属于哪一个类别，此处为判断是否为卷积层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">    nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>)</span><br><span class="line">    <span class="comment"># 如果偏置不为0的话，就以0来作为初始化</span></span><br><span class="line">    <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    	nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>遍历层结构之后，判断属于哪一个类别，此处为判断是否为全连接层。如果传进来的实例是全连接层，那么会通过normal_（正态分布）给权重weight赋值，均值=0，方差=0.01，偏置初始化0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">    nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练模型train-py"><a href="#训练模型train-py" class="headerlink" title="训练模型train.py"></a>训练模型train.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> AlexNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br><span class="line"></span><br><span class="line">data_transform = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>), </span><br><span class="line">        transforms.RandomHorizontalFlip(), </span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]),</span><br><span class="line">    <span class="string">&quot;val&quot;</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])&#125;</span><br><span class="line"></span><br><span class="line">data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))</span><br><span class="line">image_path = data_root + <span class="string">&quot;/data_set/flower_data/&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">train_dataset = datasets.ImageFolder(root=image_path + <span class="string">&quot;/train&quot;</span>,</span><br><span class="line">                                     transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">train_num = <span class="built_in">len</span>(train_dataset)</span><br><span class="line"></span><br><span class="line">flower_list = train_dataset.class_to_idx</span><br><span class="line">cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line">json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    train_dataset,batch_size=batch_size, shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">validate_dataset = datasets.ImageFolder(</span><br><span class="line">    root=image_path + <span class="string">&quot;/val&quot;</span>,transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line"></span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(</span><br><span class="line">    validate_dataset,batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_data_iter = <span class="built_in">iter</span>(validate_loader)</span><br><span class="line">test_image, test_label = test_data_iter.__next__()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>  <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % cla_dict[test_label[j].item()] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line">imshow(utils.make_grid(test_image))</span><br><span class="line"></span><br><span class="line">net = AlexNet(num_classes=<span class="number">5</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0002</span>)</span><br><span class="line"></span><br><span class="line">save_path = <span class="string">&#x27;./AlexNet.pth&#x27;</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    net.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(images.to(device))</span><br><span class="line">        loss = loss_function(outputs, labels.to(device))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        rate = (step + <span class="number">1</span>) / <span class="built_in">len</span>(train_loader)</span><br><span class="line">        a = <span class="string">&quot;*&quot;</span> * <span class="built_in">int</span>(rate * <span class="number">50</span>)</span><br><span class="line">        b = <span class="string">&quot;*&quot;</span> * <span class="built_in">int</span>((<span class="number">1</span> - rate) * <span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\rtrain loss: &#123;:^3.0f&#125;%[&#123;&#125;-&gt;&#123;&#125;]&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">int</span>(rate * <span class="number">100</span>), a, b, loss), end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(time.perf_counter() - t1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data_test <span class="keyword">in</span> validate_loader:</span><br><span class="line">            test_image, test_label = data_test</span><br><span class="line">            outputs = net(test_image.to(device))</span><br><span class="line">            predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            acc += torch.eq(predict_y, test_label.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line">        accurate_test = acc / val_num</span><br><span class="line">        <span class="keyword">if</span> accurate_test &gt; best_acc:</span><br><span class="line">            best_acc = accurate_test</span><br><span class="line">            torch.save(net.state_dict(), save_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / step, accurate_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试训练集"><a href="#测试训练集" class="headerlink" title="测试训练集"></a>测试训练集</h3><p>判断使用GPU还是CPU来训练，GPU会比CPU快几十倍，如果终端打印信息为cuda：0，则使用了GPU跑代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br></pre></td></tr></table></figure>
<p>RandomResizedCrop：初始化图像尺寸，将图片尺寸全部改为224*224；RandomHorizontalFlip：水平方向随机翻转图像的函数，强化测试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_transform = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>), <span class="comment">#随机裁剪到224*224</span></span><br><span class="line">                                 transforms.RandomHorizontalFlip(), <span class="comment">#水平方向随机翻转的函数</span></span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]),</span><br><span class="line">    <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])&#125;</span><br></pre></td></tr></table></figure>
<p>确认数据集所在文件位置，getcwd获取当前文件所在目录，..表示返回上层目录，拓展：../..表示返回上上层目录</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))</span><br><span class="line">image_path = data_root + <span class="string">&quot;/data_set/flower_data/&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">train_dataset = datasets.ImageFolder(root=image_path + <span class="string">&quot;/train&quot;</span>,</span><br><span class="line">                                     transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>train_num表示一共多少训练图片，用来后续计算准确率，正确个数 / train_num</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_num = <span class="built_in">len</span>(train_dataset)</span><br></pre></td></tr></table></figure>
<p>{‘daisy’：0, ‘dandelion’：1, ‘roses’：2, ‘sunflower’：3, ‘tulips’：4}分别对应雏菊、蒲公英、玫瑰、向日葵、郁金香</p>
<p>flower_list：获取分类的名称所对应的索引</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flower_list = train_dataset.class_to_idx</span><br></pre></td></tr></table></figure>
<p>遍历flower_list字典，将key和val反过来，是为了预测之后返回的索引能直接使用字典对应到所属的类别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br></pre></td></tr></table></figure>
<p>通过json将cla_dict字典编码成json的格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>打开class_indices.json文件，将json_str保存进去，为了方便预测时读取信息。这句代码将会在当前文件夹生成class_indices.json，如工程目录所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(json_str)</span><br></pre></td></tr></table></figure>
<p>进行训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                           batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                           num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">validate_dataset = datasets.ImageFolder(root=image_path + <span class="string">&quot;/val&quot;</span>,</span><br><span class="line">                                        transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                              batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                              num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_data_iter = <span class="built_in">iter</span>(validate_loader)</span><br><span class="line">test_image, test_label = test_data_iter.__next__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>  <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % cla_dict[test_label[j].item()] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line">imshow(utils.make_grid(test_image))</span><br></pre></td></tr></table></figure>
<h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p><img src="/2023/05/03/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAAlexNet%E5%B9%B6%E8%AE%AD%E7%BB%83%E8%8A%B1%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86/训练结果图.png" alt="训练结果图"></p>
<h3 id="构造模型"><a href="#构造模型" class="headerlink" title="构造模型"></a>构造模型</h3><p>注释掉测试训练集的输出代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = AlexNet(num_classes=<span class="number">5</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line"><span class="comment"># CrossEntropyLoss针对多类别的损失交叉熵函数</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 调试用来查看模型的参数</span></span><br><span class="line"><span class="comment"># pata = list(net.parameters())</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0002</span>)</span><br><span class="line"></span><br><span class="line">save_path = <span class="string">&#x27;./AlexNet.pth&#x27;</span></span><br><span class="line"><span class="comment"># 历史最优准确率初始化</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>进行10轮训练，并将结果动态打印在终端</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br></pre></td></tr></table></figure>
<h4 id="train训练"><a href="#train训练" class="headerlink" title="train训练"></a>train训练</h4><p>使用到Dropout，想要实现的是在训练过程中失活一部分神经元，而不想在预测过程中失活。因此通过net.train()和net.eval()来管理Dropout方法，在net.train()中开启Dropout方法，而在net.eval()中会关闭掉</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.train()</span><br><span class="line">running_loss = <span class="number">0.0</span></span><br><span class="line">t1 = time.perf_counter()</span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">    images, labels = data</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = net(images.to(device))</span><br><span class="line">    loss = loss_function(outputs, labels.to(device))</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    running_loss += loss.item()</span><br></pre></td></tr></table></figure>
<p>打印在训练过程中的训练进度，len(train_loader)获取训练一轮所需要的步数step + 1获取当前的轮数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">	rate = (step + <span class="number">1</span>) / <span class="built_in">len</span>(train_loader)</span><br><span class="line">    a = <span class="string">&quot;*&quot;</span> * <span class="built_in">int</span>(rate * <span class="number">50</span>)</span><br><span class="line">    b = <span class="string">&quot;*&quot;</span> * <span class="built_in">int</span>((<span class="number">1</span> - rate) * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\rtrain loss: &#123;:^3.0f&#125;%[&#123;&#125;-&gt;&#123;&#125;]&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">int</span>(rate * <span class="number">100</span>), a, b, loss), end=<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(time.perf_counter() - t1)</span><br></pre></td></tr></table></figure>
<h4 id="validate预测"><a href="#validate预测" class="headerlink" title="validate预测"></a>validate预测</h4><p>在net.eval()中会关闭Dropout方法，不让神经元失活</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">	<span class="keyword">for</span> data_test <span class="keyword">in</span> validate_loader:</span><br><span class="line">		test_image, test_label = data_test</span><br><span class="line">         outputs = net(test_image.to(device))</span><br><span class="line">         predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">         acc += torch.eq(predict_y, test_label.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line">     accurate_test = acc / val_num</span><br><span class="line">     <span class="keyword">if</span> accurate_test &gt; best_acc:</span><br><span class="line">         best_acc = accurate_test</span><br><span class="line">         torch.save(net.state_dict(), save_path)</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / step, accurate_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="打印结果"><a href="#打印结果" class="headerlink" title="打印结果"></a>打印结果</h3><p><img src="/2023/05/03/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAAlexNet%E5%B9%B6%E8%AE%AD%E7%BB%83%E8%8A%B1%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86/train准确率.png" alt="train准确率"></p>
<h2 id="预测模型predict-py"><a href="#预测模型predict-py" class="headerlink" title="预测模型predict.py"></a>预测模型predict.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> AlexNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在网上随便下载一张郁金香的图片放在当前文件夹，取名为tulip.jpg</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;tulip.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.imshow(img)</span><br><span class="line"><span class="comment"># [N, C, H, W]</span></span><br><span class="line"><span class="comment"># 预处理img中，已经默认将channel参数提到最前面</span></span><br><span class="line">img = data_transform(img)</span><br><span class="line"><span class="comment"># unsqueeze表示升维，因为图片只有长高深三个维度，需要新增一个batch的维度</span></span><br><span class="line"><span class="comment"># dim = 0，表示在第一个位置插入batch维度</span></span><br><span class="line">img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    json_file = <span class="built_in">open</span>(<span class="string">&#x27;./class_indices.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    class_indict = json.load(json_file)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line">    exit(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络</span></span><br><span class="line">model = AlexNet(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入网络模型</span></span><br><span class="line">model_weights_path = <span class="string">&quot;./AlexNet.pth&quot;</span></span><br><span class="line">model.load_state_dict(torch.load(model_weights_path))</span><br><span class="line"><span class="comment"># 关掉Dropout</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># img经过model得到正向传输，将batch参数压缩掉</span></span><br><span class="line">    output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">    predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 得到概率最大的类别名称</span></span><br><span class="line">    predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"><span class="comment"># 打印类别名称及对应概率</span></span><br><span class="line"><span class="built_in">print</span>(class_indict[<span class="built_in">str</span>(predict_cla)], predict[predict_cla].item())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p><img src="/2023/05/03/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAAlexNet%E5%B9%B6%E8%AE%AD%E7%BB%83%E8%8A%B1%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86/预测结果.png" alt="预测结果"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如果有需要用到AlexNet训练自己的图片，可以在data_set中更改为自己的图片和对应类别，需要注意的是需要在代码中将num_classes的数值定为自己数据集的分类数量。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（十）使用pytorch搭建ResNet并基于迁移学习训练</title>
    <url>/2023/05/13/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNet%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>工程目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── Test5_resnet</span><br><span class="line">	├── model.py（模型文件）  </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,resNet.pth）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	├── resnet-pre.pth（用于迁移学习时，提前下载好官方的resNet权重脚本）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<p>原论文中分别对应18、34、50、101、152层的网络结构参数一览表</p>
<p><img src="/2023/05/13/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNet%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/%E5%8E%9F%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8.png" alt="原论文中的参数列表"></p>
<h1><a href="http://model.py">model.py</a></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, </span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, blocks_num, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span>, groups=<span class="number">1</span>, width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.include_top = include_top</span><br><span class="line">        self.in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.width_per_group = width_per_group</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, self.in_channel, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>,</span><br><span class="line">                               padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, blocks_num[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, blocks_num[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, blocks_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, blocks_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># output size = (1, 1)</span></span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channel != channel * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel * block.expansion))</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel,</span><br><span class="line">                            channel,</span><br><span class="line">                            downsample=downsample,</span><br><span class="line">                            stride=stride,</span><br><span class="line">                            groups=self.groups,</span><br><span class="line">                            width_per_group=self.width_per_group))</span><br><span class="line">        self.in_channel = channel * block.expansion</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(block(self.in_channel,</span><br><span class="line">                                channel,</span><br><span class="line">                                groups=self.groups,</span><br><span class="line">                                width_per_group=self.width_per_group))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet50-19c8e357.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnext50_32x4d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnext101_32x8d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br></pre></td></tr></table></figure>
<h2 id="对应18、34层的残差结构">对应18、34层的残差结构</h2>
<p><img src="/2023/05/13/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNet%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/residual%E7%BB%93%E6%9E%84.png" alt="18和34层的residual结构18和34层"></p>
<p>首先定义一个类BasicBlock，对应着18层和34层所对应的残差结构，继承来自于nn.Module父类。<strong>包含实线与虚线残差结构的功能，依靠初始化函数中downsample参数进行分辨</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">       <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">	   <span class="comment"># ......</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>expansion参数对应着残差结构中，主分支所采用的卷积核的个数是否发生变化。例如图上（左）显示，其输入特征矩阵和输出特征矩阵的shape是一致的，因此由expansion = 1来表示卷积核的个数并没有发生变化，也就是1倍。</p>
<p>在之后搭建第50、101、152层的残差结构时，会发现输出特征矩阵的深度是输入特征矩阵的4倍，也就是说，残差结构中，第三层的卷积核个数是第一、二层的四倍，因此expansion = 4。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">expansion = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="定义初始函数">定义初始函数</h3>
<p>下采样参数downsample默认为none，所对应着虚线残差结构中的shortcut的1 x 1的卷积层。作用是对上一层的输出进行维度上的缩放，保证shortcut和本层的输出能够在同一维度上合并</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>conv1–out_channels</strong></p>
<ul>
<li>output_size = （input_size - 3 + 2 * 1 ）/ 1 + 1 = input_size（ shape保持不变）</li>
<li>当stride = 2时，对应的是虚线残差结构：output_size = （input_size - 3 + 2 * 1）/ 2 + 1 = input_size / 2 + 0.5 = input_size / 2（向下取整）</li>
</ul>
<p><strong>conv1–bias</strong></p>
<ul>
<li>bias=False，代表不使用bias参数，在上堂课中说明，使用Batch Normalization时，使用或者不使用bias的效果是一样的</li>
</ul>
<p><strong>bn1–out_channel</strong></p>
<ul>
<li>Batch Normalization：所输入的参数是对着应输入特征矩阵的深度，也就是对应着卷积层1输出特征矩阵的深度，也就是out_channel</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                       kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">self.relu = nn.ReLU()</span><br><span class="line">self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                       kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">self.downsample = downsample</span><br></pre></td></tr></table></figure>
<h3 id="正向传播函数">正向传播函数</h3>
<ul>
<li>identity = x：将x赋值给identity，也就是shortcut上的输出值</li>
<li>对下采样函数downsample进行判断，<strong>如果是None（没有输入下采样函数），则表示shortcut是实线，则可以跳过这部分，反之将输入特征矩阵x输入下采样函数downsample，得到shortcut函数的输出</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    identity = x</span><br><span class="line">    <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">    out = self.conv1(x)</span><br><span class="line">    out = self.bn1(out)</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    out = self.conv2(out)</span><br><span class="line">    out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">    out += identity</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h2 id="对应50、101、152层残差结构">对应50、101、152层残差结构</h2>
<p><img src="/2023/05/13/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNet%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/residual%E7%BB%93%E6%9E%84(50%E3%80%81101%E3%80%81152%E5%B1%82).png" alt="residual结构(50、101、152层)"></p>
<p>注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。<strong>但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，这么做的好处是能够在top1上提升大概0.5%的准确率</strong>。可参考<a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch">Resnet v1.5</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        <span class="comment"># ......</span></span></span><br><span class="line"><span class="params">                 </span></span><br><span class="line"><span class="params">    <span class="keyword">def</span> forward(<span class="params">self, x</span>):</span></span><br><span class="line"><span class="params">        <span class="comment"># ......</span></span></span><br><span class="line"><span class="params">        <span class="keyword">return</span> out</span></span><br></pre></td></tr></table></figure>
<p>在搭建第50、101、152层的残差结构时，会发现输出特征矩阵的深度是输入特征矩阵的4倍，也就是说，残差结构中，第三层的卷积核个数是第一、二层的四倍，因此expansion = 4。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">expansion = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h3 id="定义初始函数-2">定义初始函数</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br></pre></td></tr></table></figure>
<p>**conv1：**output_size = （input_size - 1 + 2 * 0 ）/ 1 + 1 = input_size（ shape保持不变）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                       kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(out_channel)</span><br></pre></td></tr></table></figure>
<p><strong>conv2：</strong></p>
<ul>
<li><code>实线</code>：stride默认=1，output_size = （input_size - 3 + 2 * 1 ）/ 1 + 1 = input_size + 0.5 = input_size （ shape保持不变）</li>
<li><code>虚线</code>：stride = 2，由参数传入，output_size = （input_size - 3 + 2 * 1 ）/ 2 + 1 = input_size / 2 + 0.5 = input_size / 2</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, </span><br><span class="line">                       kernel_size=<span class="number">3</span>, stride=stride, bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">self.bn2 = nn.BatchNorm2d(out_channel)</span><br></pre></td></tr></table></figure>
<p><strong>conv3：</strong></p>
<ul>
<li>output_size = （input_size - 1 + 2 * 0 ）/ 1 + 1 = input_size （高宽不变）</li>
<li>out_channels=out_channel * self.expansion：表示深度变为上一层输出特征矩阵深度的4倍（self.expansion = 4）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion,</span><br><span class="line">                       kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 这里我认为也可以改成self.bn3 = nn.BatchNorm2d(out_channels)</span></span><br><span class="line">self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)</span><br><span class="line">self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">self.downsample = downsample</span><br></pre></td></tr></table></figure>
<h3 id="正向传播函数-2">正向传播函数</h3>
<p>identity = x：将x赋值给identity，也就是shortcut上的输出值</p>
<p>对下采样函数downsample进行判断，<strong>如果是None（没有输入下采样函数），则表示shortcut是实线，则可以跳过这部分，反之则对应虚线的残差结构，将输入特征矩阵x输入下采样函数downsample，得到shortcut函数的输出</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    identity = x</span><br><span class="line">    <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">    out = self.conv1(x)</span><br><span class="line">    out = self.bn1(out)</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    out = self.conv2(out)</span><br><span class="line">    out = self.bn2(out)</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    out = self.conv3(out)</span><br><span class="line">    out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">    out += identity</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h2 id="ResNet网络框架">ResNet网络框架</h2>
<p>在初始化函数当中，<code>传入的block</code>就是对应的残差结构，会根据定义的层结构传入不同的block，例如传入是18、34层的残差结构，那么就是BasicBlock，如果传入的是50、101、152层的残差结构，那么就是Bottleneck;</p>
<p><code>blocks_num</code>：传入的是一个列表类型，对应的是使用残差结构的数目。例如对应使用34层的残差结构，那么根据参数表来看，blocks_num = [ 3, 4, 6, 3 ]；对于101层就是[ 3, 4, 23, 3 ]</p>
<p><code>include_top</code>：是为了方便以后能在ResNet网络基础上搭建更加复杂的网络，本节课并没有使用到，但代码中实现了该方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, blocks_num, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span>, groups=<span class="number">1</span>, width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="定义初始函数-3">定义初始函数</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, blocks_num, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span> </span>):</span><br><span class="line">	<span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">	self.include_top = include_top</span><br></pre></td></tr></table></figure>
<p>根据参数表，无论在哪一个层结构下，在经过Maxpooling下采样层之后，输出特征矩阵的深度都为64</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.in_channel = <span class="number">64</span></span><br></pre></td></tr></table></figure>
<p><strong>conv1</strong>：首先输入的是RGB彩色图像，因此先输入3。对应参数表中7x7的卷积层，特征矩阵深度还是64，没有发生变化，为了使特征矩阵的高和宽缩减为原来的一半，因此kernel_size = 7，padding = 3, stride = 2。</p>
<p><strong>output_size = （ input_size - 7 + 2 * 3 ）/ 2 + 1 = input_size / 2 + 0.5 = intput_size / 2</strong>（向下取整）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">3</span>, self.in_channel, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>,</span><br><span class="line">                       padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><strong>maxpool</strong>：kernel_size = 3, 特征矩阵深度还是64，为了使特征矩阵的高和宽缩减为原来的一半，因此pading = 1，stride = 2</p>
<p><strong>output_size = （ input_size - 3 + 2 * 1 ）/ 2 + 1 = input_size / 2 + 0.5 = intput_size / 2</strong>（向下取整）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>接下来定义layer1、layer2、layer3、layer4</p>
<p>其中layer1对应的是参数表中conv2所对应的一系列残差结构；layer2对应的是vonv3所对应的一系列残差结构；layer3对应是conv4；layer4对应conv5。这一系列layer是通过_make_layer函数生成的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.layer1 = self._make_layer(block, <span class="number">64</span>, blocks_num[<span class="number">0</span>])</span><br><span class="line">self.layer2 = self._make_layer(block, <span class="number">128</span>, blocks_num[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">self.layer3 = self._make_layer(block, <span class="number">256</span>, blocks_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">self.layer4 = self._make_layer(block, <span class="number">512</span>, blocks_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>在输入时已经将include_top默认为True，通过自适应的平均池化下采样操作AdaptiveAvgPool2d，无论输入特征矩阵的高和宽是多少，都会以(1, 1)的形式，输出高和宽为1的特征矩阵。</p>
<p>再通过全连接层，也就是输出节点层，通过nn.Linear类进行定义。输入的节点个数，也就是通过平均池化下采样层之后所得到特征矩阵展平后的节点个数。由于通过平均池化下采样之后得到的特征矩阵的高和宽都是1，那么展平后的节点个数即特征矩阵的深度。</p>
<blockquote>
<p>对于18、34层而言，通过conv5.x经过一系列残差结构输出的特征矩阵的深度为512，所以输入数据为512 * block.expansion，block.expansion = 1；</p>
<p>对于50、101、152层来说，通过conv5.x经过一系列残差结构输出的特征矩阵的深度为2048，也就是512的4倍，正好此时block.expansion = 4。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self.include_top:</span><br><span class="line">    self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># output size = (1, 1)</span></span><br><span class="line">    self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br></pre></td></tr></table></figure>
<p>最后对卷积层进行初始化操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">        nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="make-layer函数">_make_layer函数</h3>
<p><code>block</code>：对应的是BasicBlock（18、34层）或者Bottleneck（50、101、152层）</p>
<p><code>channel</code>：对应的是残差结构中卷积层所使用卷积核的个数，例如layer1对应的是conv2.1中卷积核的个数64，layer2对应的是conv3.1卷积核的个数128，layer3对应conv4.1卷积核个数是256，layer4对应conv5.1卷积核个数是512</p>
<p><code>block_num</code>：表示该层一共包含多少个残差结构，例如在34层残差机构当中，conv2.x中一共包含3个，conv3.x包含4个，conv4.x包含6个，conv5.x包含3个</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">    downsample = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p><strong>18、34层的网络结构会跳过该判断语句，50、101、152层的网络结构会执行该判断下的语句，即生成下采样函数downsample。</strong></p>
<p>在layer1中，因没有输入stride，所以默认stride = 1，因此判断语句前半段不成立。in_channel判断是否等于channel * block.expansion。当在18、34层残差结构时，由于block.expansion = 1，而channel对应layer1中输入为64，所以二者相等，判断失效。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channel != channel * block.expansion:</span><br><span class="line">    downsample = nn.Sequential(</span><br><span class="line">        nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(channel * block.expansion))</span><br></pre></td></tr></table></figure>
<p>首先定义layers的列表，block对应的是BasicBlock（18、34层）或者Bottleneck（50、101、152层）。等于是将网络结构中虚线残差结构的conv2.1、conv3.1、conv4.1、conv5.1的输出特征矩阵以列表的形式存放在layers列表中，在本次循环中存放进的是conv2.1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layers = []</span><br><span class="line">layers.append(block(self.in_channel,</span><br><span class="line">                    channel,</span><br><span class="line">                    downsample=downsample,</span><br><span class="line">                    stride=stride))</span><br><span class="line">self.in_channel = channel * block.expansion</span><br></pre></td></tr></table></figure>
<p>以循环的方式将实线残差结构依次存放仅layers列表中。range（1， block_num）中从1开始，因为上面的步骤已经将0做好了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">    layers.append(block(self.in_channel, channel))</span><br><span class="line"><span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p>结合以上对layer1的过程描述，<code>_make_layer函数</code>实际上是将conv2.x、conv3.x、conv4.x、conv5.x每一层中虚线和实线对应的特征矩阵存放进对应的layers列表中。例如50层的conv2.x，layer1对应是[ 虚线，实线，实线 ]。</p>
<h3 id="正向传播函数-3">正向传播函数</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># conv1</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.maxpool(x)</span><br><span class="line">    </span><br><span class="line">	<span class="comment"># conv2</span></span><br><span class="line">    x = self.layer1(x)</span><br><span class="line">    <span class="comment"># conv3</span></span><br><span class="line">    x = self.layer2(x)</span><br><span class="line">    <span class="comment"># conv4</span></span><br><span class="line">    x = self.layer3(x)</span><br><span class="line">    <span class="comment"># conv5</span></span><br><span class="line">    x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.include_top:</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment"># 展平处理</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 全连接</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="定义ResNet网络架构的函数">定义ResNet网络架构的函数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 18层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet18</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"><span class="comment"># 34层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"><span class="comment"># 50层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"><span class="comment"># 101层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"><span class="comment"># 152层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet152</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br></pre></td></tr></table></figure>
<h1><a href="http://train.py">train.py</a></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> resnet34</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br><span class="line"></span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                   transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])&#125;</span><br><span class="line"></span><br><span class="line">    data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))  <span class="comment"># get data root path</span></span><br><span class="line">    image_path = os.path.join(data_root, <span class="string">&quot;data_set&quot;</span>, <span class="string">&quot;flower_data&quot;</span>)  <span class="comment"># flower data set path</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;train&quot;</span>),</span><br><span class="line">                                         transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">    train_num = <span class="built_in">len</span>(train_dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line">    flower_list = train_dataset.class_to_idx</span><br><span class="line">    cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line">    <span class="comment"># write dict into json file</span></span><br><span class="line">    json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">16</span></span><br><span class="line">    nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line"></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                               batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                               num_workers=nw)</span><br><span class="line"></span><br><span class="line">    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>),</span><br><span class="line">                                            transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">    val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">    validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                                  batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                  num_workers=nw)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(train_num,</span><br><span class="line">                                                                           val_num))</span><br><span class="line"></span><br><span class="line">    net = resnet34()</span><br><span class="line">    <span class="comment"># load pretrain weights</span></span><br><span class="line">    <span class="comment"># download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./resnet34-pre.pth&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; does not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line">    net.load_state_dict(torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line">    <span class="comment"># for param in net.parameters():</span></span><br><span class="line">    <span class="comment">#     param.requires_grad = False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># change fc layer structure</span></span><br><span class="line">    in_channel = net.fc.in_features</span><br><span class="line">    net.fc = nn.Linear(in_channel, <span class="number">5</span>)</span><br><span class="line">    net.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define loss function</span></span><br><span class="line">    loss_function = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># construct an optimizer</span></span><br><span class="line">    params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    optimizer = optim.Adam(params, lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">    epochs = <span class="number">3</span></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line">    save_path = <span class="string">&#x27;./resNet34.pth&#x27;</span></span><br><span class="line">    train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># train</span></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        train_bar = tqdm(train_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">            images, labels = data</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            logits = net(images.to(device))</span><br><span class="line">            loss = loss_function(logits, labels.to(device))</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print statistics</span></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                     epochs,</span><br><span class="line">                                                                     loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">            <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">                val_images, val_labels = val_data</span><br><span class="line">                outputs = net(val_images.to(device))</span><br><span class="line">                <span class="comment"># loss = loss_function(outputs, test_labels)</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">                val_bar.desc = <span class="string">&quot;valid epoch[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                           epochs)</span><br><span class="line"></span><br><span class="line">        val_accurate = acc / val_num</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">            best_acc = val_accurate</span><br><span class="line">            torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p><strong>训练结果</strong>（迁移学习），准确率最终能达到93%</p>
<p><img src="/2023/05/13/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNet%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/train%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="train迁移学习训练结果"></p>
<p>如果不想使用迁移学习的方法，可以将以下代码注释</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_weight_path = <span class="string">&quot;./resnet34-pre.pth&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; does not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line">net.load_state_dict(torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"><span class="comment"># for param in net.parameters():</span></span><br><span class="line"><span class="comment">#     param.requires_grad = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># change fc layer structure</span></span><br><span class="line">in_channel = net.fc.in_features</span><br><span class="line">net.fc = nn.Linear(in_channel, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>并将传入resnet实例化参数的地方net = resnet34()传入num_classes = 5</p>
<h1><a href="http://predict.py">predict.py</a></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> resnet34</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(<span class="number">256</span>),</span><br><span class="line">         transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    img_path = <span class="string">&quot;./tulip.jpg&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    img = data_transform(img)</span><br><span class="line">    <span class="comment"># expand batch dimension</span></span><br><span class="line">    img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    model = resnet34(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    weights_path = <span class="string">&quot;./resNet34.pth&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(weights_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(weights_path)</span><br><span class="line">    model.load_state_dict(torch.load(weights_path, map_location=device))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prediction</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># predict class</span></span><br><span class="line">        output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">        predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">        predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">    print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                                 predict[predict_cla].numpy())</span><br><span class="line">    plt.title(print_res)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                                  predict[i].numpy()))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>预测结果（迁移学习）</p>
<p><img src="/2023/05/13/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNet%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/predict%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="predict迁移学习预测结果"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（六）使用pytorch搭建VGG网络</title>
    <url>/2023/05/07/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVGG%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p><img src="/2023/05/07/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAVGG%E7%BD%91%E7%BB%9C/VGG网络架构表.png" alt="VGG网络架构表"></p>
<p>在上堂课讲解了D模型，今次以实操的形式，针对模型A、B、D、E做一个代码的实现。将代码构造分为两部分：提取特征网络结构（图中橙色框）、分类网络结构（图中绿色框）</p>
<h1 id="VGG网络架构"><a href="#VGG网络架构" class="headerlink" title="VGG网络架构"></a>VGG网络架构</h1><h2 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># official pretrain weights</span></span><br><span class="line">model_urls = &#123;</span><br><span class="line">    <span class="string">&#x27;vgg11&#x27;</span>: <span class="string">&#x27;https://download.pytorch.org/models/vgg11-bbd30ac9.pth&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;vgg13&#x27;</span>: <span class="string">&#x27;https://download.pytorch.org/models/vgg13-c768596a.pth&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;vgg16&#x27;</span>: <span class="string">&#x27;https://download.pytorch.org/models/vgg16-397923af.pth&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;vgg19&#x27;</span>: <span class="string">&#x27;https://download.pytorch.org/models/vgg19-dcbb9e9d.pth&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, num_classes=<span class="number">1000</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG, self).__init__()</span><br><span class="line">        self.features = features</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 3 x 224 x 224</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        <span class="comment"># N x 512 x 7 x 7</span></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># N x 512*7*7</span></span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_features</span>(<span class="params">cfg: <span class="built_in">list</span></span>):</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&quot;M&quot;</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            layers += [conv2d, nn.ReLU(<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">cfgs = &#123;</span><br><span class="line">    <span class="string">&#x27;vgg11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">model_name=<span class="string">&quot;vgg16&quot;</span>, **kwargs</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cfg = cfgs[model_name]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Warning: model number &#123;&#125; not in cfgs dict!&quot;</span>.<span class="built_in">format</span>(model_name))</span><br><span class="line">        exit(-<span class="number">1</span>)</span><br><span class="line">    model = VGG(make_features(cfg), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>定义一个字典文件，字典中每个key代表着一个模型的配置文件。</p>
<ul>
<li>eg：vgg11：对应A配置（模型A），即11层的网络结构（11层指的是卷积层+全连接层的个数）；</li>
<li>vgg13：对应配置B（模型B），即13层网络结构；</li>
<li>vgg16：对应配置D（模型D），即16层网络结构；</li>
<li>vgg19：对应配置E（模型E），即19层网络结构；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cfgs = &#123;</span><br><span class="line">    <span class="string">&#x27;vgg11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>含义解释：eg：vgg11的配置文件，对应的值是一个列表，其中列表的数字代表着卷积层中卷积核的个数，其中的字符M代表的是池化层的结构。</p>
<p>对应着表中配置A的内容，第一层是一个3*3的卷积，有64个卷积核，因此在配置列表中的第一个元素是64；第二层是一个最大池化下采样层，因此配置列表中对应的是一个M字符；第三层是一个卷积层大小为3*3的卷积核，有128个卷积核，因此在配置列表中对应的元素为128；第四层是一个最大池化下采样层，对应一个M字符；</p>
<p>接下来经过的是2个3*3的卷积核个数为256的卷积核，因此配置列表中对应两个256；再接下来是一个最大池化下采样层，对应一个M字符；接下来经过的是2个3*3的卷积核个数为512的卷积核，因此配置列表中对应两个512；再接下来是一个最大池化下采样层，对应一个M字符；接下来经过的是2个3*3的卷积核个数为512的卷积核，因此配置列表中对应两个512；再接下来是一个最大池化下采样层，对应一个M字符；</p>
<h3 id="提取特征网络结构"><a href="#提取特征网络结构" class="headerlink" title="提取特征网络结构"></a>提取特征网络结构</h3><p>提取特征网络结构，传入参数为列表类型的配置变量，在运行过程中，只需要传入cfgs对应配置的列表即可。</p>
<p>假设传入一个列表，首先定义空列表layers，用来存放创建的每一层结构；紧接着定义in_channels变量，因为输入的是rgb彩色图像，所以输入通道为3；接下来通过一个for循环来遍历配置列表。如果当前的配置元素是一个M字符，则该层是一个最大池化层，那么创建一个最大池化下采样层，就是nn.MaxPool2d。上节课讲到，在VGG网络中，所有的最大池化下采样池化核大小都为2，步距也都为2，因此kernel_size=2, stride=2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_features</span>(<span class="params">cfg: <span class="built_in">list</span></span>):</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&quot;M&quot;</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            layers += [conv2d, nn.ReLU(<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p>如果当前的配置元素不为M字符，则该层是一个卷积层。</p>
<p>创建一个卷积操作nn.Conv2d，在第一层卷积层中，输入的深度是彩色图像的3，因此第一个值是in_channels；输出的特征矩阵的深度对应着卷积核的个数，因此在第一层卷积层中，v对应64。上堂课中讲到，在VGG中，所有的卷积核为3*3，步距为1，padding为1（stride默认为1，所以没写）。因为每一个卷积层都需要采用ReLU激活函数，所以将刚刚定义好的卷积层和ReLU激活函数拼接在一起，并添加在事先定义好的layers列表中。</p>
<p>当特征矩阵通过该层卷积之后，输出深度变为v，因此执行in_channels = v，因此再下一层卷积层时，它的in_channel会自动变为上一层的卷积层的输出特征矩阵的深度。</p>
<p>通过for循环遍历配置列表，能得到一个有卷积操作和池化操作所组成的一个列表。</p>
<p>接下来通过nn.Sequential函数，将layers列表通过非关键字参数的形式传入进去。在代码中layers前有一个*，代表着是通过非关键字参数传入函数。</p>
<p>原因：在Sequential类中给出了两个使用示例：第一个是最常用的，通过将一个个非关键字参数输入到Sequential类中，就能生成一个新的网络层结构；第二种通过一个有序的字典的形式进行输入。我们这是通过一个非关键字参数的形式输入，因此加了一个*。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># class Sequential(Module):</span></span><br><span class="line">    model = nn.Sequential(</span><br><span class="line">              nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">              nn.ReLU(),</span><br><span class="line">              nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">              nn.ReLU()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Using Sequential with OrderedDict. This is functionally the</span></span><br><span class="line">    <span class="comment"># same as the above code</span></span><br><span class="line">    model = nn.Sequential(OrderedDict([</span><br><span class="line">              (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">              (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">              (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">              (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">            ]))</span><br></pre></td></tr></table></figure>
<p><strong>（函数被调用的时候，使用*解包一个可迭代对象作为函数的参数，字典对象可以使用两个参数，解包后将作为关键字参数传递给函数；解包：将序列里面的元素一个个拆开）</strong></p>
<h3 id="分类网络结构"><a href="#分类网络结构" class="headerlink" title="分类网络结构"></a>分类网络结构</h3><p>定义VGG类，继承于nn.Module父类，在初始化函数中，传进参数有features（提取特征网络结构），num_classes（分类类别个数），init_weights（是否对网络进行权重初始化）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, num_classes=<span class="number">1000</span>, init_weights=<span class="literal">False</span></span>):</span><br></pre></td></tr></table></figure>
<p>通过nn.Sequential生成分类网络结构，在VGG网络结构中，图像通过提取特征网络结构之后，会生成一个7*7*512的特征矩阵，如果要进行全连接操作，需要先进行展平处理。在全连接操作之前，加入Dropout函数，目的是为了减少过拟合，有50%的几率随机失活神经元。第一层全连接层，原论文当中应该是4096，但为了减少训练参数，这里减半2048</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.classifier = nn.Sequential(</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">2048</span>),</span><br><span class="line">    nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>),</span><br><span class="line">    nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>是否需要对网络对参数可视化，如果为init_weights为true，就会进入到事先定义的初始化权重函数中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> init_weights:</span><br><span class="line">    self._initialize_weights()</span><br></pre></td></tr></table></figure>
<p>正向传播过程：输入x为输入的图像数据，features：提取特征网络结构，接着对输出进行展平处理，展平之后再将输出的特征矩阵输入到classifier函数中，最后得到输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># N x 3 x 224 x 224</span></span><br><span class="line">    x = self.features(x)</span><br><span class="line">    <span class="comment"># N x 512 x 7 x 7</span></span><br><span class="line">    x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># N x 512*7*7</span></span><br><span class="line">    x = self.classifier(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>在初始化权重函数中，会遍历网络的每一个子模块，即每一层。如果遍历的当前层是卷积层，就用xavier_uniform_初始化方法取初始化卷积层权重，如果有使用到偏置的话，会初始化偏置为0。</p>
<p>如果遍历当前层是全连接层，那么同样使用xavier_uniform_初始化方法去初始化全连接层权重，同样讲偏置为0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">            <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>实例化所给定的配置模型，通过传入变量model_name，需要实例化哪一个模型配置，默认为vgg16。</p>
<p>以此代码为例：将vgg16传入到vgg函数中，cfg = cfgs[vgg16]，即vgg16关键字对应的配置列表内容，再通过VGG类实例化VGG网络（首先传入第一个参数是features（来源于make_features），后面两个*对应的变量是一个可变函数的字典变量，通过在调用VGG函数时所传入的字典变量，这个字典变量可能包含了分类个数，是否初始化权重的bool变量）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">model_name=<span class="string">&quot;vgg16&quot;</span>, **kwargs</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cfg = cfgs[model_name]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Warning: model number &#123;&#125; not in cfgs dict!&quot;</span>.<span class="built_in">format</span>(model_name))</span><br><span class="line">        exit(-<span class="number">1</span>)</span><br><span class="line">    model = VGG(make_features(cfg), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> vgg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br><span class="line"></span><br><span class="line">data_transform = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                 transforms.RandomHorizontalFlip(),</span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]),</span><br><span class="line">    <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># getcwd获取当前文件所在目录，..返回上层目录</span></span><br><span class="line">data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))  <span class="comment"># get data root path</span></span><br><span class="line">image_path = data_root + <span class="string">&quot;/data_set/flower_data/&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">train_dataset = datasets.ImageFolder(root=image_path + <span class="string">&quot;/train&quot;</span>,</span><br><span class="line">                                     transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">train_num = <span class="built_in">len</span>(train_dataset)</span><br><span class="line"><span class="comment"># 雏菊         蒲公英          玫瑰      向日葵         郁金香</span></span><br><span class="line"><span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line"><span class="comment"># 获取分类的名称所对应的索引</span></span><br><span class="line">flower_list = train_dataset.class_to_idx</span><br><span class="line"><span class="comment"># 遍历flower_list字典，将key和val反过来，是为了预测之后返回的索引能直接使用字典对应到所属的类别</span></span><br><span class="line">cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line"><span class="comment"># 通过json将cla_dict字典编码成json的格式</span></span><br><span class="line">json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 打开class_indices.json文件，将json_str保存进去，为了方便预测时读取信息</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                           batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                           num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">validate_dataset = datasets.ImageFolder(root=image_path + <span class="string">&quot;/val&quot;</span>,</span><br><span class="line">                                        transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                              batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                              num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test_data_iter = iter(validate_loader)</span></span><br><span class="line"><span class="comment"># test_image, test_label = test_data_iter.__next__()</span></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&#x27;vgg16&#x27;</span></span><br><span class="line">net = vgg(model_name=model_name, num_classes=<span class="number">5</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line"><span class="comment"># CrossEntropyLoss针对多类别的损失交叉熵函数</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 调试用来查看模型的参数</span></span><br><span class="line"><span class="comment"># pata = list(net.parameters())</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0002</span>)</span><br><span class="line"></span><br><span class="line">save_path = <span class="string">&#x27;./&#123;&#125;Net.pth&#x27;</span>.<span class="built_in">format</span>(model_name)</span><br><span class="line"><span class="comment"># 历史最优准确率初始化</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    <span class="comment"># 使用到Dropout，想要实现的是在训练过程中失活一部分神经元，而不想在预测过程中失活</span></span><br><span class="line">    <span class="comment"># 因此通过net.train()和net.eval()来管理Dropout方法，在net.train()中开启Dropout方法，而在net.eval()中会关闭掉</span></span><br><span class="line">    net.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    t1 = time.perf_counter()</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(images.to(device))</span><br><span class="line">        loss = loss_function(outputs, labels.to(device))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="comment"># 打印在训练过程中的训练进度，len(train_loader)获取训练一轮所需要的步数step + 1获取当前的轮数</span></span><br><span class="line">        rate = (step + <span class="number">1</span>) / <span class="built_in">len</span>(train_loader)</span><br><span class="line">        a = <span class="string">&quot;*&quot;</span> * <span class="built_in">int</span>(rate * <span class="number">50</span>)</span><br><span class="line">        b = <span class="string">&quot;*&quot;</span> * <span class="built_in">int</span>((<span class="number">1</span> - rate) * <span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\rtrain loss: &#123;:^3.0f&#125;%[&#123;&#125;-&gt;&#123;&#125;]&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">int</span>(rate * <span class="number">100</span>), a, b, loss), end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(time.perf_counter() - t1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># validate</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data_test <span class="keyword">in</span> validate_loader:</span><br><span class="line">            test_image, test_label = data_test</span><br><span class="line">            outputs = net(test_image.to(device))</span><br><span class="line">            predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            acc += torch.eq(predict_y, test_label.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line">        accurate_test = acc / val_num</span><br><span class="line">        <span class="keyword">if</span> accurate_test &gt; best_acc:</span><br><span class="line">            best_acc = accurate_test</span><br><span class="line">            torch.save(net.state_dict(), save_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / step, accurate_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>代码内容和在AlexNet中一致，不做特别讲解，额外说明一下图像初始化中的某处特殊性，及修改了调用VGG函数的部分参数</p>
<h3 id="说明transforms-Normalize"><a href="#说明transforms-Normalize" class="headerlink" title="说明transforms.Normalize"></a>说明transforms.Normalize</h3><p>在大多数VGG使用论文中，大部分会在预处理第一步，将RGB三个通道分别减去[123.68,116.78,103.94]，这三个值分别对应imageNet的图像数据集的所有数据三个通道的均值。</p>
<p>但这里并没有减去均值，因为这里搭建的VGG模型是从头开始训练的，如果需要基于迁移学习的方式进行再训练的话，就需要减去均值，因为预训练的模型是基于imageNet数据集进行训练的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_transform = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                 transforms.RandomHorizontalFlip(),</span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]),</span><br></pre></td></tr></table></figure>
<h3 id="调用vgg函数"><a href="#调用vgg函数" class="headerlink" title="调用vgg函数"></a>调用vgg函数</h3><p>通过model_name确认调用哪一个配置文件，num_classes和init_weights参数输入进去之后，会保存在model中**kwargs的可变长度字典当中。当调用VGG类时，即可调用相应的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_name = <span class="string">&#x27;vgg16&#x27;</span></span><br><span class="line">net = vgg(model_name=model_name, num_classes=<span class="number">5</span>, init_weights=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>因为VGG网络非常大，而训练集样本太小（只有3k+），是无法充分训练vgg网络，因此不做训练展示，且网络太大， 训练起来可能需要几个小时，在训练中能够达到的准确率大概在80%，时间漫长效果不太好，不建议日常使用。</p>
<p>如果需要使用VGG网络，建议使用迁移学习的方法。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（十二）使用pytorch搭建ResNeXt并基于迁移学习训练</title>
    <url>/2023/05/21/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNeXt%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h1>ResNet-50与ResNeXt-50（32x4d）</h1>
<img src="/2023/05/21/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNeXt%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/ResNet-50与ResNeXt-50（32x4d）参数.png" alt="ResNet-50与ResNeXt-50（32x4d）网络结构参数" style="border-width: 5px; border-style: solid; border-color: rgb(224, 224, 224);  ">
<p>ResNet网络结构中的较深层结构（50层及以上结构）所采用的是上图（最左侧）的block结构，在ResNeXt网络结构中所对应采用的是上图（中间）的block结构。</p>
<p>区别在于结构显示中的第二层的3x3卷积层，对于普通的block结构（如最左侧），采用普通的3x3进行卷积，而对于ResNeXt的block（如中间），第二层是group conv。</p>
<p><strong>ResNet-50和ResNeXt结构</strong></p>
<blockquote>
<p><strong>相同点：</strong></p>
<ul>
<li>整体框架一致。首先经过一个7x7的卷积层将输入的特征矩阵深度从3变为64，高宽不变；之后经过3x3的最大池化下采样层，深度不变，高宽从112变为56；之后重复堆叠block，且堆叠次数一致，图中都是 [ 3，4，6，3 ]；之后是平均池化、全连接层，最后是softmax概率输出。</li>
<li><strong>在每一个网络结构相对应的block中，输出的特征矩阵的深度是一致的。</strong></li>
</ul>
<p><strong>不同点：</strong></p>
<p>以conv2为例，在ResNet网络结构中第一层采用1x1卷积层的个数是64，但在对应ResNeXt网络结构中的个数是普通block结构中采用卷积核个数的2倍，在下一层3x3的卷积层中，ResNet采用64个卷积核，而ResNeXt中分成了32group，每个group采用4个卷积核，所以ResNeXt第二层中采用128个卷积核。</p>
<p>因此，<strong>在每一层block结构中，ResNeXt的第1、2层的卷积核个数的都是ResNet中对应block层数的2倍。</strong></p>
</blockquote>
<h1>工程目录</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── Test5_resnext</span><br><span class="line">	├── model.py（模型文件）  </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,resNext.pth）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	├── resnext50_32x4d.pth（用于迁移学习时，提前下载好官方的resNet权重脚本）</span><br><span class="line">	├── batch_predict.py（批量预测图片分类）</span><br><span class="line">	└── data</span><br><span class="line">		└── imgs（批量数据图片）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<h1><a href="http://model.py">model.py</a></h1>
<h2 id="修改Bottleneck类">修改Bottleneck类</h2>
<p>在初始化函数中参数传递加入groups和width_per_group</p>
<ul>
<li>groups：分组数（例如上图中的32）</li>
<li>width_per_group：（例如上图conv2中的4：指每个group中卷积核的个数）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             groups=<span class="number">1</span>, width_per_group=<span class="number">64</span></span>):</span><br><span class="line">    <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line"></span><br><span class="line">    width = <span class="built_in">int</span>(out_channel * (width_per_group / <span class="number">64.</span>)) * groups</span><br><span class="line"></span><br><span class="line">    self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,</span><br><span class="line">                           kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    self.bn1 = nn.BatchNorm2d(width)</span><br><span class="line">    <span class="comment"># -----------------------------------------</span></span><br><span class="line">    self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,</span><br><span class="line">                           kernel_size=<span class="number">3</span>, stride=stride, bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">    self.bn2 = nn.BatchNorm2d(width)</span><br><span class="line">    <span class="comment"># -----------------------------------------</span></span><br><span class="line">    self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,</span><br><span class="line">                           kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)</span><br><span class="line">    self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    self.downsample = downsample</span><br></pre></td></tr></table></figure>
<p>其中：当group和width_per_group采用默认值时，width输出值为out_channel。当采用ResNeXt结构时，以conv2为例，groups= 32，width_per_group = 128，则<strong>width = （ 4 * （ 128 / 64 ））* 32 = 256</strong>。</p>
<p>因此本句代码意义为<strong>在ResNeXt结构中，输出特征矩阵的channel是输入特征矩阵channel的2倍，因此可以通过本条语句，得出ResNet和ResNeXt网络结构在block中第1，2层卷积层所采用的卷积核的个数。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">width = <span class="built_in">int</span>(out_channel * (width_per_group / <span class="number">64.</span>)) * groups</span><br></pre></td></tr></table></figure>
<p>注意：在conv3中的out_channels=out_channel*self.expansion，以conv2为例，上一层out_channels = 64，因此在本语句中<strong>总体依旧是输出特征矩阵是上一层（block上一层）的4倍。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,</span><br><span class="line">                           kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="实例化Bottleneck">实例化Bottleneck</h2>
<p>ResNet网络结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet50-19c8e357.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], </span><br><span class="line">                  num_classes=num_classes, </span><br><span class="line">                  include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], </span><br><span class="line">                  num_classes=num_classes, </span><br><span class="line">                  include_top=include_top)</span><br></pre></td></tr></table></figure>
<p>ResNeXt网络结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnext50_32x4d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnext101_32x8d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br></pre></td></tr></table></figure>
<h2 id="修改ResNet类">修改ResNet类</h2>
<p>初始化函数及_make_layer函数的参数传递加入groups和width_per_group</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 block,</span></span><br><span class="line"><span class="params">                 blocks_num,</span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 include_top=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.include_top = include_top</span><br><span class="line">        self.in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.width_per_group = width_per_group</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, self.in_channel, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>,</span><br><span class="line">                               padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, blocks_num[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, blocks_num[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, blocks_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, blocks_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># output size = (1, 1)</span></span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channel != channel * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel * block.expansion))</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel,</span><br><span class="line">                            channel,</span><br><span class="line">                            downsample=downsample,</span><br><span class="line">                            stride=stride,</span><br><span class="line">                            groups=self.groups,</span><br><span class="line">                            width_per_group=self.width_per_group))</span><br><span class="line">        self.in_channel = channel * block.expansion</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(block(self.in_channel,</span><br><span class="line">                                channel,</span><br><span class="line">                                groups=self.groups,</span><br><span class="line">                                width_per_group=self.width_per_group))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<h1><a href="http://train.py">train.py</a></h1>
<p>修改调用model.py函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> resnext50_32x4d</span><br><span class="line">net = resnext50_32x4d()</span><br></pre></td></tr></table></figure>
<p>修改调用迁移学习权重路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_weight_path = <span class="string">&quot;./resnext50_32x4d.pth&quot;</span></span><br></pre></td></tr></table></figure>
<p>另：因为机器撑不住，所以我这把batch_size改为4</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                           batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                           num_workers=nw)</span><br><span class="line"></span><br><span class="line">validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>),</span><br><span class="line">                                        transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                              batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                              num_workers=nw)</span><br></pre></td></tr></table></figure>
<h2 id="训练结果">训练结果</h2>
<p><img src="/2023/05/21/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNeXt%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/train%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="train训练结果"></p>
<h1><a href="http://predict.py">predict.py</a></h1>
<p>修改调用model.py函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> resnext50_32x4d</span><br><span class="line">model = resnext50_32x4d(num_classes=<span class="number">5</span>).to(device)</span><br></pre></td></tr></table></figure>
<p>修改使用权重路径（train.py中通过迁移学习产生的权重pth文件）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights_path = <span class="string">&quot;./resNext50.pth&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="预测结果">预测结果</h2>
<p><img src="/2023/05/21/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNeXt%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="预测结果"></p>
<h1>批量预测batch_predict.py</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> resnext50_32x4d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(<span class="number">256</span>),</span><br><span class="line">         transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    <span class="comment"># 指向需要遍历预测的图像文件夹</span></span><br><span class="line">    imgs_root = <span class="string">&quot;data/imgs&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(imgs_root), <span class="string">f&quot;file: &#x27;<span class="subst">&#123;imgs_root&#125;</span>&#x27; dose not exist.&quot;</span></span><br><span class="line">    <span class="comment"># 读取指定文件夹下所有jpg图像路径</span></span><br><span class="line">    img_path_list = [os.path.join(imgs_root, i) <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(imgs_root) <span class="keyword">if</span> i.endswith(<span class="string">&quot;.jpg&quot;</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">f&quot;file: &#x27;<span class="subst">&#123;json_path&#125;</span>&#x27; dose not exist.&quot;</span></span><br><span class="line"></span><br><span class="line">    json_file = <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    class_indict = json.load(json_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    model = resnext50_32x4d(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    weights_path = <span class="string">&quot;./resnext50.pth&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(weights_path), <span class="string">f&quot;file: &#x27;<span class="subst">&#123;weights_path&#125;</span>&#x27; dose not exist.&quot;</span></span><br><span class="line">    model.load_state_dict(torch.load(weights_path, map_location=device))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prediction</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    batch_size = <span class="number">8</span>  <span class="comment"># 每次预测时将多少张图片打包成一个batch</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> ids <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(img_path_list) // batch_size):</span><br><span class="line">            img_list = []</span><br><span class="line">            <span class="keyword">for</span> img_path <span class="keyword">in</span> img_path_list[ids * batch_size: (ids + <span class="number">1</span>) * batch_size]:</span><br><span class="line">                <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">f&quot;file: &#x27;<span class="subst">&#123;img_path&#125;</span>&#x27; dose not exist.&quot;</span></span><br><span class="line">                img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">                img = data_transform(img)</span><br><span class="line">                img_list.append(img)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># batch img</span></span><br><span class="line">            <span class="comment"># 将img_list列表中的所有图像打包成一个batch</span></span><br><span class="line">            batch_img = torch.stack(img_list, dim=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># predict class</span></span><br><span class="line">            output = model(batch_img.to(device)).cpu()</span><br><span class="line">            predict = torch.softmax(output, dim=<span class="number">1</span>)</span><br><span class="line">            probs, classes = torch.<span class="built_in">max</span>(predict, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> idx, (pro, cla) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(probs, classes)):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;image: &#123;&#125;  class: &#123;&#125;  prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(img_path_list[ids * batch_size + idx],</span><br><span class="line">                                                                 class_indict[<span class="built_in">str</span>(cla.numpy())],</span><br><span class="line">                                                                 pro.numpy()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="批量预测结果">批量预测结果</h2>
<p><img src="/2023/05/21/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAResNeXt%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/%E6%89%B9%E9%87%8F%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="批量预测结果"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型之CNN（十三）MobileNetv1v2v3网络详解</title>
    <url>/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1>MobileNet详解</h1>
<p>在传统的卷积神经网络中，内存需求大、运算量大，导致无法在移动设备以及嵌入式设备上运行。在之前学习的网络结构中，例如VGG16的权重大小有400+M、ResNet的152层模型权重大小能到达600+M。如此大的模型参数是不可能在移动设备以及嵌入式设备上运行的。</p>
<p>MobileNet网络是由由google团队在2017年提出的，专注于移动端或者嵌入式设备中的轻量级CNN网络。相比传统卷积神经网络，在准确率小幅降低的前提下大大减少模型参数与运算量。(相比VGG16准确率减少了0.9%，但模型参数只有VGG的1/32)。</p>
<h2 id="MobileNet-v1">MobileNet v1</h2>
<p><strong>MobileNetv1版本</strong>原论文：<a href="https://arxiv.org/abs/1704.04861">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p>
<p>论文网络中的亮点：</p>
<ul>
<li><strong>Depthwise Convolution</strong>（简称DW卷积，大大减少运算量和参数数量）</li>
<li>增加超参数$\alpha$、$\beta$（人为设定，$\alpha$控制卷积层卷积核的个数、$\beta$控制输入图像大小）</li>
</ul>
<h3 id="DW卷积">DW卷积</h3>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/DW卷积.png" alt="DW卷积" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>传统卷积过程</strong></p>
<blockquote>
<p>输入一个深度为3的特征矩阵，经过4个卷积核进行卷积，每个卷积核的深度与输入特征矩阵的深度是相同的，所以每个卷积核深度也为3。又因输出特征矩阵的深度是由卷积核的个数决定的，因此最终输出特征矩阵的深度为4。</p>
<p><strong>总结：</strong></p>
<ul>
<li>卷积核channel = 输入特征矩阵channel</li>
<li>输出特征矩阵channel = 卷积核个数</li>
</ul>
</blockquote>
<p><strong>DW卷积过程</strong></p>
<blockquote>
<p>输入一个深度为3的特征矩阵，经过3个卷积核进行卷积，且卷积核深度为1，也就是说一个卷积核对应输入特征矩阵的一个channel，再得到相应的输出特征矩阵的一个channel。既然一个卷积核负责输入特征矩阵的一个深度，那么卷积核的个数应该和输入特征矩阵的深度相同。又因为每一个卷积核与输入特征矩阵的一个channel进行卷积之后得到一个输出特征矩阵的channel，那么输出特征矩阵的深度与卷积核的个数相同，也和输入特征矩阵的深度相同。</p>
<p><strong>总结：</strong></p>
<ul>
<li>卷积核channel = 1</li>
<li>输入特征矩阵channel = 卷积核个数 = 输出特征矩阵channel</li>
</ul>
</blockquote>
<h3 id="Depthwise-Separable-Conv"><strong>Depthwise Separable Conv</strong></h3>
<blockquote>
<p>也叫做深度可分的卷积操作，由两部分组成，第一部分由DW卷积，第二部分为PW卷积。PW卷积实际上相当于普通卷积，只不过卷积核的大小为1。由下图所示，卷积层中卷积核的深度 = 1。</p>
<p>通常情况下DW卷积和PW卷积是一起使用的。</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/PW卷积.png" alt="PW卷积" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="DW-PW卷积和普通卷积的区别">DW&amp;PW卷积和普通卷积的区别</h3>
<p>下图（上左）为普通卷积操作，下图（下左）为DW卷积操作，下图（下右）为PW卷积操作，二者结合为深度可分卷积操作。</p>
<p>在普通卷积中，通过卷积操作之后，得到的是channel = 4的特征矩阵；在DW&amp;PW卷积操作之后，得到的同样是channel = 4的特征矩阵。</p>
<blockquote>
<p>参数代表含义：$D_F$：输入特征矩阵的宽高；$D_K$：卷积核的大小；M：输入特征矩阵的深度；N：输出特征矩阵的深度</p>
<p><strong>普通卷积计算量</strong>：卷积核的高x卷积核的宽x输入特征矩阵的深度x输出特征矩阵的深度x输入特征矩阵的高x输入特征矩阵的宽（默认stride = 1）</p>
<p><strong>DW&amp;PW卷积计算量</strong>：</p>
<ul>
<li>DW：卷积核的大小x输入特征矩阵的深度x输入特征矩阵的高宽</li>
<li>PW（相当于普通的卷积）：卷积核的高和宽（都是1）x输入特征矩阵的深度x输出特征矩阵的深度x输入特征矩阵的高宽（默认stride = 1）</li>
</ul>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/深度可分卷积和普通卷积的参数区别.png" alt="深度可分卷积和普通卷积的参数区别" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>理论上，普通卷积计算量是DW&amp;PW卷积的8~9倍</strong></p>
<h3 id="MobileNet-v1网络结构参数">MobileNet v1网络结构参数</h3>
<p>Filter Shape：3 x 3 x 3 x 32分别指的是卷积核的高宽、深度、个数</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv1网络参数.png" alt="MobileNetv1网络参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>超参数$\alpha$、$\beta$</strong></p>
<ul>
<li>$\alpha$：卷积核个数的倍率，用来控制卷积过程中所采用卷积核的个数</li>
<li>$\beta$：分辨率的参数，用来控制输入图像的大小</li>
</ul>
<p>大多数人发现，DW卷积在训练完之后的部分卷积核容易废掉，即卷积核参数大部分为0，也就是说DW卷积核没有起到作用。针对这个问题，在MobileNet v2版本中有一定的改善。</p>
<h2 id="MobileNet-v2">MobileNet v2</h2>
<p>MobileNet v2网络是由google团队在2018年提出的，相比MobileNet v1网络，准确率更高，模型更小。原论文：<a href="https://arxiv.org/abs/1801.04381">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>
<p>网络中的亮点：</p>
<ul>
<li>Inverted Residuals（倒残差结构）</li>
<li>Linear Bottlenecks</li>
</ul>
<h3 id="Inverted-Residuals">Inverted Residuals</h3>
<p><strong>Residual block</strong></p>
<blockquote>
<p>对输入特征矩阵采用1x1卷积核进行降维处理，之后经过3x3的卷积核及逆行卷积处理，再通过1x1的卷积核进行升维，之后再输出特征矩阵。<strong>这就形成两边大中间小的瓶颈结构。</strong></p>
<p>注意：采用ReLU激活函数</p>
</blockquote>
<p><strong>Inverted residual block</strong></p>
<blockquote>
<p>对输入特征矩阵采用1x1卷积核进行升维处理，之后经过3x3的卷积核进行DW卷积，再通过1x1的卷积核进行降维处理，之后在输出特征矩阵。<strong>这就形成两边小中间大的橄榄球结构。</strong></p>
<p>注意：采用ReLU6激活函数</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inverted Residual结构.png" alt="Inverted Residual结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>ReLU6激活函数</strong></p>
<blockquote>
<p>在普通的ReLU激活函数中，当输入值&lt;0时，默认将输出置0；当输入值&gt;0时，即不进行处理。在ReLU6激活函数中，在输入值&lt;0，以及 [ 0, 6 ] 的区间时处理都一致，但在输入值&gt;6时，将会将输出值一直为6。</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/ReLU6激活函数.png" alt="ReLU6激活函数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); zoom: 50%;">
<h3 id="Linear-Bottlenecks">Linear Bottlenecks</h3>
<p>在论文中是针对Inverted residual结构的最后一个1x1的卷积层，使用线性激活函数而不是之前的ReLU激活函数。在下图中是作者在原文中描述为什么在最后一个1x1卷积层不使用ReLU激活函数原因的实验证明。</p>
<blockquote>
<p>首先输入的是一个2维矩阵，channel = 1；之后采用不同维度的matrix T 将输入进行变化，变化到更高的维度，再通过ReLU激活函数得到输出值。之后再使用T矩阵的逆矩阵$T^{-1}$，将输出的矩阵还原回2维的矩阵。当matrix T的维度是2，3的时候（即还原回去之后），所对应的即是图中的Output/dim = 2，Output/dim = 3</p>
<p>由图中可以看到，当被还原之后，图像中已经丢失了很多信息，随着matrix T维度不断的加深，丢失的信息就越来越少。因此<strong>ReLU激活函数对低维特征信息造成大量损失，而对于高维特征信息造成的损失很小</strong></p>
</blockquote>
<p>而在Inverted residual结构当中，是两边细、中间粗的结构，因此在输出时是一个低维的特征向量，需要一个线性的激活函数来替代ReLU激活函数，来避免特征信息的损失。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Linear Bottleneck.png" alt="Linear Bottleneck" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>Inverted residual网络结构图</strong></p>
<blockquote>
<p>Inverted residual网络结构如下图（左），首先通过1x1的卷积层进行卷积，通过ReLU6激活函数进行激活，之后通过DW卷积，卷积核大小为3x3，同样通过ReLU6激活函数激活，最后经过卷积核大小为1x1的卷积处理，使用线性激活函数。</p>
<p>过程信息为下图（右）表格，首先通过1x1卷积层进行升维处理（输出深度为tk，也就是说这里采用的卷积核的个数是tk个）；在第二层中，输入特征矩阵的深度为上一层的输出，即tk，这里采用卷积核大小为3x3的DW卷积，stride = s，由前文可知此处深度不变，依旧是tk，但高宽缩减s倍；最后一层通过1x1卷积核进行降维处理，卷积核个数为k‘。</p>
</blockquote>
<p>注意：**在MobileNet v2版本中，并不是每一个Inverted residual结构中都有shortcut。**在论文中，表述当stride = 1时，是有shortcut，当stride = 2时，是没有shortcut。但通过搭建网络结构之后，会发现论文此处表述有误，<strong>正确表述应该为：当stride = 1且输入特征矩阵与输出特征矩阵shape相同时才有shortcut连接。</strong></p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Inverted residual网络结构图.png" alt="Inverted residual网络结构图" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="MobileNet-v2网络结构参数"><strong>MobileNet v2网络结构参数</strong></h3>
<p><strong>相关参数介绍：</strong></p>
<ul>
<li>t：扩展因子，对应着第一层1x1的卷积层所采用的卷积核个数的拓展倍率</li>
<li>c：时输出特征矩阵的深度channel，即是上图中提到的k’，控制着输出特征矩阵的深度</li>
<li>n：bottleneck的重复次数，此处的bottleneck指的是论文中的<strong>Inverted residual结构</strong></li>
<li>s：步距，<strong>仅仅针对每一个block所对应的第一层bottleneck的步距（一个block由一系列bottleneck组成），其他都为1</strong>。假设当n = 2时，即bottleneck需要重复2遍，对于这个结构而言，第一层的步距是2，第二层s为1。</li>
</ul>
<p>注意：在第一个bottleneck中，t = 1，即该处的扩展因子为1，也就是说在第一层卷积层是没有对特征矩阵的深度做调整的。<strong>因此在搭建网络时，因为这一层卷积层没有起到升维或者降维作用，所以会被跳过，直接进入3x3的卷积层中处理。</strong></p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet v2网络结构参数.png" alt="Mobilenet v2网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>如何理解有无shortcut？</strong></p>
<blockquote>
<p>以输入为$14^2$x64这一行的block来举例，这一行的block采用了3个bottleneck结构，stride = 1，即此处block的第一层卷积层的stride = 1，如果按照官方标注而言，应该需要由shortcut。</p>
<p>但实际上这里不可能有shortcut，因此此处的输入特征矩阵的深度为64，但是输出特征矩阵的深度是96。也就是说，如果有shortcut的话，那么输出特征矩阵的深度应该是64，但从表中可知经过一系列操作后输出的特征矩阵深度是96，所以两个特征矩阵的深度是不相同的，无法相加。</p>
<p>在第二层的时候，s = 1，这里的输入特征矩阵深度是上一层的输出特征矩阵深度，即96，对于第二层bottleneck而言，输入的特征矩阵深度十96，其输出特征矩阵的深度也是96，有因为stride = 1，特征矩阵的高宽不会发生变化，因此满足输入特征矩阵和输出特征矩阵的shape保持一致的条件，此时才能使用shortcut。</p>
<p>因此，<strong>只有当stride = 1且输入特征矩阵与输出特征矩阵shape相同时才有shortcut连接。</strong></p>
</blockquote>
<p><strong>conv2d 1x1</strong></p>
<p>网络结构最后一层是卷积层，输入特征矩阵为1x1x1280，实际上这一层就是一个全连接层，功能上二者一模一样。k表示的是分类的类别个数，若针对ImageNet而言，这里的k = 1000。</p>
<h3 id="性能对比">性能对比</h3>
<p>表头对应为：准确率、模型参数、运算量、运算时间。</p>
<ul>
<li>下图（上）是针对在分类任务中的性能对比，MobileNetV2（1.4）指的是$\alpha$ = 1.4（倍率因子）；</li>
<li>下图（下）是针对在目标检测中的效果对比，将MNet V2与SSDLite联合使用，将MNet v2作为前置网络，SSDLite将其中的一些卷积层换为深度可分卷积，也就是DW卷积+PW卷积。</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/性能对比.png" alt="性能对比" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h2 id="MobileNet-v3">MobileNet v3</h2>
<p>原论文：<a href="https://arxiv.org/abs/1905.02244">Searching for MobileNetV3</a></p>
<p>论文中的亮点：</p>
<ul>
<li>更新Block（bneck）（即对Inverted residual结构基础上，进行简单改动）</li>
<li>使用NAS搜索参数（Neural Architecture Search）</li>
<li>重新设计耗时层结构（NAS搜索之后得到的网络，之后对网络的每一层的推理时间进行分析，针对某些耗时的层结构做进一步的优化）</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv3L对比v3S对比v2.png" alt="MobileNetv3L对比v3S对比v2" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>性能对比</strong></p>
<p>在原论文的摘要中，作者提到，v3版本的Large对比v2而言在图像分类任务当中准确率上升有3.2%，延迟有降低20%。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/MobileNetv3性能对比.png" alt="MobileNetv3性能对比" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="更新Block">更新Block</h3>
<p>在v3当中，做出了Block的更新：</p>
<ul>
<li>加入SE模块，即通道的注意力机制模块</li>
<li>更新了激活函数</li>
</ul>
<p><strong>Mobilenet V2：bottleneck with residual</strong></p>
<blockquote>
<p>在v2当中，会首先通过1x1的卷积核对输入特征矩阵进行升维处理，之后通过BN，ReLU6激活函数，之后是通过3x3的DW卷积，之后进行BN，ReLU激活函数激活，最后通过1x1卷积层进行降维处理，之后只跟了一个BN结构，并没有像前面几层一样还有一个ReLU6激活函数。最后有一个shortcut相加处理。</p>
<p>注意：<strong>当stride = 1且input_c = output_c才有shortcut连接</strong></p>
</blockquote>
<p><strong>Mobilenet V3 block</strong></p>
<p><strong>加入SE模块</strong></p>
<blockquote>
<p>大部分框架与v2保持一致，但是在第二层卷积层中加入了SE注意力机制。也就是说针对得到的特征矩阵，对每个channel进行池化处理，那么特征矩阵的channel是多少，得到的1维向量就有多少元素，之后再通过两个全连接层得到输出向量。</p>
<p>注意：</p>
<ul>
<li><strong>对于第一个全连接层，此处的节点个数 = 该处卷积层特征矩阵channel的1/4</strong>（在v3原论文中作者有给出）</li>
<li>第二层全连接层的节点个数 = 该处卷积层特征矩阵channel</li>
<li>对于输出的向量，可以理解为是对该层卷积层特征矩阵的每一个channel分析出了一个权重关系，觉得比较重要的channel会赋予更大的权重，觉得不是那么重要的channel的维度上就对应较小的权重。</li>
</ul>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3更新Block结构.png" alt="更新Block结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>理解两处的全连接层</strong></p>
<blockquote>
<p>假设此处特征矩阵的channel = 2，首先通过平均池化下采样层操作对每一个channel取一个均值，由于有两个channel，所以得到一个元素个数为2的向量。之后再依次通过两个全连接层，得到输出。</p>
<p>针对FC1，节点个数= 输入特征矩阵channel的1/4，实际情况中channel会很多，不会出现小于4的情况。之后跟着一个ReLU激活函数。对于FC2而言，节点个数 =  输入特征矩阵channel = 2，之后跟着H-sig激活函数（之后讲解）。</p>
<p>之后会得到有2个元素的向量，每一个元素对应的就是每一个channel所对应的权重。比如预测第一个channel的权重 = 0.5，那么就将0.5与第一个channel的当中的所有元素相乘得到一个新的channel数据。</p>
</blockquote>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3两处全连接层.png" alt="两处全连接层理解" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p><strong>更新激活函数</strong></p>
<p>在v3的Block中所使用的激活函数标注的都是NL激活函数，即非线性激活函数的意思。因为在不同的层之间所使用的激活函数都不一样，因此在下图中并未给出明确的激活函数名称。</p>
<p>注意：在最后一个标注的1x1降维的卷积层中，是没有使用激活函数的，也可以说使用了一个线性激活函数y = x。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/v3更新Block结构.png" alt="更新Block结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="重新设计耗时层结构">重新设计耗时层结构</h3>
<ul>
<li>
<p>减少第一个卷积层的卷积核个数（32-&gt;16）</p>
<blockquote>
<p>​		在原论文当中，作者提到当卷积核个数变化了之后，准确率是没有变化的，那么就达成了在维持准确率不降的前提下减少计算量的目的，这里大概减少2ms的时间</p>
</blockquote>
</li>
<li>
<p>精简Last Stage</p>
<blockquote>
<p>​		在通过NAS搜索出来的网络结构的最后一个部分叫做Original Last Stage，作者在原论文中发现，这一个流程是比较耗时的，因此作者针对这个结构做出精简化，于是提出了Efficient Last Stage。</p>
<p>​		在精简之后发现，第一个卷积层是没有发生变化的，紧接着直接进行平均池化操作，然后跟着两个卷积核输出。和下图（上）的结构对比而言，减少了许多层结构。调整之后，作者发现准确率无多少变化，但执行时间上减少了7ms，这7ms占据整个推理时间的11%。</p>
</blockquote>
</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/重新设计耗时层结构.png" alt="重新设计耗时层结构" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="重新设计激活函数">重新设计激活函数</h3>
<p>在v2网络结构中，基本使用的激活函数都是ReLU6，目前来说比较常用的激活函数叫做swish激活函数，也就是输入值*sigmoid函数。使用swish激活函数确实会提高网络的准确率，但是在计算、求导上非常复杂，对量化过程也不友好（将模型部署到硬件设备上）。</p>
<p>于是作者提出h-swish激活函数，也就是输入值*h-sigmoid函数，其中h-sigmoid = ReLU6（x+3）/6，也就是在ReLU函数的输入值基础上+3的值除以6。</p>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/重新设计激活函数.png" alt="重新设计激活函数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="V3-Large网络结构参数">V3-Large网络结构参数</h3>
<ul>
<li><code>input</code>：输入特征矩阵的长宽深；</li>
<li><code>Operator</code>：对应操作，例如第一层进行conv2d操作；bneck指进行的block操作，其中3x3指的是下图（左）中的DW卷积中卷积核的大小</li>
<li><code>exp size</code>：代表第一个升维的1x1卷积需要将维度升到多少</li>
<li><code>#out</code>：输出特征矩阵的channel；</li>
<li><code>SE</code>：是否使用注意力机制</li>
<li><code>NL</code>：使用的激活函数，HS指H-swish激活函数，RE指ReLU6激活函数</li>
<li><code>s</code>：表示DW卷积中的步距stride</li>
</ul>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet v3网络结构参数.png" alt="Mobilenet v3-Large网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<p>最后两层有写到NBN结构，指的是不去使用，和全连接层的作用是差不多的，这里直接使用卷积结构。</p>
<p>注意：在第一个bneck结构的第一层卷积层之中，因为i输入的特征矩阵channel = 16且输出的特征矩阵channel = 16，因此没有进行升维和降维，且没有SE结构，直接进行1x1的卷积层输出就没有了，因此在搭建网络中，会省略这一步。</p>
<h3 id="V3-Small网络结构参数">V3-Small网络结构参数</h3>
<img src="/2023/05/22/MobileNetv1v2v3%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/Mobilenet v3-Small网络结构参数.png" alt="Mobilenet v3-Small网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN网络详解</tag>
      </tags>
  </entry>
  <entry>
    <title>使用pytorch搭建MobileNetV2V3并基于迁移学习训练</title>
    <url>/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h1>MobileNet v2v3网络搭建</h1>
<p><strong>工程目录</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── Test6_mobilenet</span><br><span class="line">	├── model_v2.py（MobileNet v2模型文件）  </span><br><span class="line">	├── model_v3.py（MobileNet v3模型文件）  </span><br><span class="line">	├── train.py（调用模型训练，自动生成class_indices.json,MobileNetV2/V3.pth文件）</span><br><span class="line">	├── predict.py（调用模型进行预测）</span><br><span class="line">	├── tulip.jpg（用来根据前期的训练结果来predict图片类型）</span><br><span class="line">	├── mobilenet_v2.pth（用于迁移学习时，提前下载好官方的MobileNet v2权重脚本）</span><br><span class="line">	└── mobilenet_v3_large.pth（用于迁移学习时，提前下载好官方的mobilenet_v3_large权重脚本）</span><br><span class="line">└── data_set</span><br><span class="line">	└── data数据集</span><br></pre></td></tr></table></figure>
<h2 id="model-v2-py">model_v2.py</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNReLU</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, expand_ratio</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, alpha=<span class="number">1.0</span>, round_nearest=<span class="number">8</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h3 id="网络结构参数"><strong>网络结构参数</strong></h3>
<blockquote>
<p>先通过普通卷积，之后进行一个Inverted residual结构（n：倒残差结构重复几遍），一直重复倒残差结构，再通过1x1的普通卷积操作，紧接着平均池化下采样，最后是通过1x1卷积得到最终输出（作用同全连接层）</p>
</blockquote>
<p>搭建该网络重点在于<strong>搭建好Inverted residual结构</strong></p>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenetv2网络结构参数.png" alt="Mobilenetv2网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<h3 id="ConvBNReLU类">ConvBNReLU类</h3>
<p>ConvBNReLU类是包含conv、BN和ReLU6激活函数的组合层。在MNv2网络中，所有卷积层以及上节课所讲的DW卷积操作，基本上都是由卷积+BN+ReLU6激活函数组成的。</p>
<p><strong>唯一不同的是在Inverted residual结构中的第三层，也就是使用1x1的卷积对特征矩阵进行降维处理，这里使用线性激活函数</strong>，其他地方就都是通过卷积+BN+ReLU6激活函数组成。</p>
<p>因此创建ConvBNReLU类，继承来自<strong>nn.Sequential父类</strong>（nn.Sequential不需要写forward函数，此处参照pytorch的官方实现）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNReLU</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span></span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNReLU, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, groups=groups, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Inverted-residual结构">Inverted residual结构</h3>
<p>类似于ResNet中的残差结构，只不过普通的ResNet的残差结构是两头粗中间细的结构，但倒残差结构是两头细中间粗的结构。</p>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Inverted residual参数结构.png" alt="Inverted residual结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<blockquote>
<p>第一层：普通卷积层，卷积核大小为1x1，激活函数是ReLU6，采用卷积核个数为tk（t是倍率因子，用来扩大特征矩阵深度）；</p>
<p>第二层：DW卷积，卷积核大小为3x3，步距stride = s（传入参数），激活函数是ReLU6，卷积核个数 = 输入特征矩阵深度 = 输出特征矩阵深度，缩减高宽；</p>
<p>第三层：普通1x1卷积层，采用线性激活函数，卷积核个数为k‘（人为设定）</p>
</blockquote>
<p><strong>__init__函数</strong></p>
<ul>
<li>初始化函数传入参数<code>expand_ratio</code>：拓展因子，即上图表中的t</li>
<li>定义隐层<code>hidden_channel</code>：第一层卷积核的个数，即上图表中的tk</li>
<li><code>self.use_shortcut</code>：是一个bool变量，判断在正向传播过程中是否采用shortcut（只有当stride = 1且输入特征矩阵的shape与输出特征矩阵的shape保持一致市才能使用sortcut）</li>
<li>定义层列表<code>layers</code>：判断expand_ratio是否等于1（在v2网络结构参数表中的第一个bottleneck的第一层中，n = 1，s = 1，因此特征矩阵的shape都未发生变化，因此在搭建过程中进行跳过），当expand_ratio = 1时，跳过该处1x1卷积层。</li>
</ul>
<blockquote>
<p>假设此处expand_ratio！= 1，那么就需要在layers列表中添加一个卷积层，调用ConvBNReLU层结构（输入特征矩阵深度，hidden_channel，由上图（右表）得知第一层卷积核大小为1x1）。</p>
</blockquote>
<ul>
<li>调用<code>layers.extend</code>函数：添加一系列层结构，实际上通过extend和append函数添加层结构是一样的，只不过extend函数能够实现一次性批量插入多个元素。</li>
</ul>
<blockquote>
<p>对于Inverted residual结构，第二层是DW卷积，因此此处调用定义好的ConvBNReLU层结构，输入特征矩阵深度为上一层的输出特征矩阵深度hidden_channel，输出深度同样为hidden_channel（因为DW卷积层的输入特征矩阵深度和输出特征矩阵深度是一样的）</p>
<p>stride是传入的参数，group默认为1时是普通卷积，当传入与输入特征矩阵深度相同的个数的话，就是DW卷积。而此处输入特征矩阵的深度是hidden_channel，因此此处groups=hidden_channel。</p>
<p>第三层是1x1的普通卷积，所采用的的激活函数是线性激活函数（y = x），不能使用定义好的ConvBNReLU，因此使用原始的Conv2d，输入特征矩阵深度为上一层输出特征矩阵深度，输出特征矩阵深度为传入参数out_channel，卷积核大小为1</p>
<p>最后使用Batch Normalization标准化处理。</p>
<p>注意：<strong>因为第三层使用线性激活函数y = x，等于不对输入做任何处理，因此不需要再额外定义一个激活层函数，因为不添加激活层函数，就等于linear激活函数</strong></p>
</blockquote>
<ul>
<li>最后通过nn.Sequential类将*layers参数传入进去（*用于将层列表解包为位置参数，这使我们能够像船体单个参数一样将层列表传递给Sequential模块</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, expand_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        hidden_channel = in_channel * expand_ratio</span><br><span class="line">        self.use_shortcut = stride == <span class="number">1</span> <span class="keyword">and</span> in_channel == out_channel</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">if</span> expand_ratio != <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 1x1 pointwise conv</span></span><br><span class="line">            layers.append(ConvBNReLU(in_channel, hidden_channel, kernel_size=<span class="number">1</span>))</span><br><span class="line">        layers.extend([</span><br><span class="line">            <span class="comment"># 3x3 depthwise conv</span></span><br><span class="line">            ConvBNReLU(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel),</span><br><span class="line">            <span class="comment"># 1x1 pointwise conv(linear)</span></span><br><span class="line">            nn.Conv2d(hidden_channel, out_channel, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p><strong>forward函数</strong></p>
<p>x为输入特征矩阵</p>
<p>首先进行判断，是否使用shortcut，如果使用即返回特征矩阵与shortcut相加之后的输出特征矩阵；如果不使用shortcut，则直接返回主分支上的输出特征矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">	<span class="keyword">if</span> self.use_shortcut:</span><br><span class="line">		<span class="keyword">return</span> x + self.conv(x)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure>
<h3 id="make-divisible函数">_make_divisible函数</h3>
<p>**是为了将卷积核个数（输出的通道个数）调整为输入round_nearest参数的整数倍。**搭建中采用round_nearest=8，也就是要将卷积核的个数设置为8的整数倍。</p>
<p>目的：为了更好的调用硬件设备，比如多GPU变形运算，或者多机器分布式运算</p>
<ul>
<li><code>ch</code>：传入的卷积核个数（输出特征矩阵的channel）</li>
<li><code>divisor</code>：传入round_nearest基数，即将卷积核个数ch调整为divisor的整数倍</li>
<li><code>min_ch</code>：最小通道数，如果为None，就将min_ch设置为divisor</li>
<li><code>new_ch</code>：即将卷积核个数调整为离它最近的8的倍数的值</li>
<li>之后进行判断new_ch是否小于传入ch的0.9倍，如果小于，则加上一个divisor（为了确保new_ch向下取整的时候，不会减少超过10%）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function is taken from the original tf repo.</span></span><br><span class="line"><span class="string">    It ensures that all layers have a channel number that is divisible by 8</span></span><br><span class="line"><span class="string">    It can be seen here:</span></span><br><span class="line"><span class="string">    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> min_ch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        min_ch = divisor</span><br><span class="line">    new_ch = <span class="built_in">max</span>(min_ch, <span class="built_in">int</span>(ch + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_ch &lt; <span class="number">0.9</span> * ch:</span><br><span class="line">        new_ch += divisor</span><br><span class="line">    <span class="keyword">return</span> new_ch</span><br></pre></td></tr></table></figure>
<h3 id="定义MNv2网络">定义MNv2网络</h3>
<ul>
<li><code>block</code>：将前面定义的InvertedResidual类传给block</li>
<li><code>input_channel</code>：是第一层卷积层所采用的卷积核的个数，也就是下一层输出特征矩阵的深度，为8的倍数</li>
<li><code>last_channel</code>：指的是参数表中1x1的卷积核，输出特征矩阵深度为1280</li>
<li><code>inverted_residual_setting</code>：根据参数表创建list列表，列表中的元素对应着参数表中每一行的参数t（拓展因子）、c（输出channel）、n（倒残差结构重复次数）、s（每一个block第一层卷积层的步距）</li>
<li><code>features</code>：在其中先添加第一个卷积层，输入channel即彩色图片</li>
</ul>
<blockquote>
<p>通过循环定义一系列的Inverted residual结构。将每一层output_channel通过_make_divisible函数进行调整，再进行n次的Inverted residual（在循环中除了列表中的stride = s，其他都是 = 1）</p>
<p>在经过一系列Inverted residual处理后，接下来的是一个1x1的卷积层，直接使用ConvBNReLU</p>
</blockquote>
<ul>
<li>
<p><code>self.features</code>：通常将以上一系列层结构统称为特征提取层，所以Sequential将刚刚定义好的一系列层结构通过位置参数的形式传入进去，打包成一个整体。</p>
</li>
<li>
<p>定义由<code>self.avgpool</code>和<code>self.classifier</code>组成的分类器</p>
<blockquote>
<p>首先定义平均池化下采样层（采用自适应的平均池化下采样操作，给定输出特征矩阵的高宽为1）</p>
<p>再通过nn.Sequential将Dropout层和全连接层组合在一起</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, alpha=<span class="number">1.0</span>, round_nearest=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV2, self).__init__()</span><br><span class="line">        block = InvertedResidual</span><br><span class="line">        input_channel = _make_divisible(<span class="number">32</span> * alpha, round_nearest)</span><br><span class="line">        last_channel = _make_divisible(<span class="number">1280</span> * alpha, round_nearest)</span><br><span class="line"></span><br><span class="line">        inverted_residual_setting = [</span><br><span class="line">            <span class="comment"># t, c, n, s</span></span><br><span class="line">            [<span class="number">1</span>, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">24</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">96</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">160</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">320</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        features = []</span><br><span class="line">        <span class="comment"># conv1 layer</span></span><br><span class="line">        features.append(ConvBNReLU(<span class="number">3</span>, input_channel, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># building inverted residual residual blockes</span></span><br><span class="line">        <span class="keyword">for</span> t, c, n, s <span class="keyword">in</span> inverted_residual_setting:</span><br><span class="line">            output_channel = _make_divisible(c * alpha, round_nearest)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                stride = s <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                features.append(block(input_channel, output_channel, stride, expand_ratio=t))</span><br><span class="line">                input_channel = output_channel</span><br><span class="line">        <span class="comment"># building last several layers</span></span><br><span class="line">        features.append(ConvBNReLU(input_channel, last_channel, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># combine feature layers</span></span><br><span class="line">        self.features = nn.Sequential(*features)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building classifier</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(last_channel, num_classes)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><strong>权重初始化</strong></p>
<ul>
<li>遍历每一个子模块，如果是卷积层，就对权重进行初始化，如果存在偏置，则将偏置设置为0；</li>
<li>如果子模块是一个BN层，则将方差设置为1，均值设置为0；</li>
<li>如果子模块是一个全连接层的话，对权重进行初始化，normal_函数为一个正态分布函数，将权重调整为均值为0.0，方差为0.01的正态分布；将偏置设置为0</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># weight initialization</span></span><br><span class="line">	<span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">			nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>)</span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				nn.init.zeros_(m.bias)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">			nn.init.ones_(m.weight)</span><br><span class="line">			nn.init.zeros_(m.bias)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">			nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">			nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>正向传播过程</strong></p>
<p>首先将输入特征矩阵输入进特征提取部分，通过平均池化下采样得到输出，再对输出进行展平处理，最后通过分类器得到最终输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">	x = self.features(x)</span><br><span class="line">	x = self.avgpool(x)</span><br><span class="line">	x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">	x = self.classifier(x)</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="model-v3-py">model_v3.py</h2>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/Mobilenet v3网络结构参数.png" alt="Mobilenet v3网络结构参数" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="comment">#同MobileNet v2中的_make_divisible作用一致</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 out_planes: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 kernel_size: <span class="built_in">int</span> = <span class="number">3</span>, </span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span> = <span class="number">1</span>, </span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeExcitation</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_c: <span class="built_in">int</span>, squeeze_factor: <span class="built_in">int</span> = <span class="number">4</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualConfig</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_c: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 kernel: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 expanded_c: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 out_c: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 use_se: <span class="built_in">bool</span>, </span></span><br><span class="line"><span class="params">                 activation: <span class="built_in">str</span>, </span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjust_channels</span>(<span class="params">channels: <span class="built_in">int</span>, width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="keyword">return</span> _make_divisible(channels * width_multi, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cnf: InvertedResidualConfig, norm_layer: <span class="type">Callable</span>[..., nn.Module]</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 inverted_residual_setting: <span class="type">List</span>[InvertedResidualConfig], </span></span><br><span class="line"><span class="params">                 last_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 block: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_large</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_small</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<h3 id="make-divisible函数-2">_make_divisible函数</h3>
<p>同MobileNet v2中的_make_divisible作用一致，**是为了将卷积核个数（输出的通道个数）调整为输入round_nearest参数的整数倍。**搭建中采用round_nearest=8，也就是要将卷积核的个数设置为8的整数倍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function is taken from the original tf repo.</span></span><br><span class="line"><span class="string">    It ensures that all layers have a channel number that is divisible by 8</span></span><br><span class="line"><span class="string">    It can be seen here:</span></span><br><span class="line"><span class="string">    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> min_ch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        min_ch = divisor</span><br><span class="line">    new_ch = <span class="built_in">max</span>(min_ch, <span class="built_in">int</span>(ch + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_ch &lt; <span class="number">0.9</span> * ch:</span><br><span class="line">        new_ch += divisor</span><br><span class="line">    <span class="keyword">return</span> new_ch</span><br></pre></td></tr></table></figure>
<h3 id="ConvBNActivation类">ConvBNActivation类</h3>
<p>ConvBNActivation类是包含conv、BN和激活函数的组合层。</p>
<ul>
<li><code>in_planes</code>：输入特征矩阵的深度</li>
<li><code>out_planes</code>：输出特征矩阵的深度，对应卷积核的个数</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层</li>
<li><code>activation_layer</code>：对应激活函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_planes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel_size: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> activation_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            activation_layer = nn.ReLU6</span><br><span class="line">        <span class="built_in">super</span>(ConvBNActivation, self).__init__(nn.Conv2d(in_channels=in_planes,</span><br><span class="line">                                                         out_channels=out_planes,</span><br><span class="line">                                                         kernel_size=kernel_size,</span><br><span class="line">                                                         stride=stride,</span><br><span class="line">                                                         padding=padding,</span><br><span class="line">                                                         groups=groups,</span><br><span class="line">                                                         bias=<span class="literal">False</span>),</span><br><span class="line">                                               norm_layer(out_planes),</span><br><span class="line">                                               activation_layer(inplace=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<h3 id="SqueezeExcitation模块">SqueezeExcitation模块</h3>
<p><strong>初始化函数</strong></p>
<p>相当于两个全连接层。</p>
<ul>
<li><strong>对于第一个全连接层，此处的节点个数 = 该处输入的特征矩阵channel的1/4</strong>（在v3原论文中作者有给出）</li>
<li>第二层全连接层的节点个数 = 该处输入的特征矩阵channel</li>
<li>第一个全连接层的激活函数是ReLU，第二个全连接层的激活函数是hard-sigmoid</li>
</ul>
<img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/SE注意力机制模块.png" alt="SE注意力机制模块" style="border-width: 1px; border-style: solid; border-color: rgb(224, 224, 224); ">
<ul>
<li><code>squeeze_factor</code>：因为第一个全连接层的节点个数是输入的特征矩阵channel的1/4，所以这里存放的是分母4；</li>
<li><code>squeeze_c</code>：调用_make_divisible方法调整到离该数最近的8的整数倍的数字；</li>
<li><code>self.fc1</code>：使用卷积来作为全连接层，作用与全连接层一样</li>
<li><code>self.fc2</code>：输入channel是上一层的输出channel，所以是squeeze_c，又因为SE机制最终输出需要和输入特征矩阵的channel保持一致，所以输出channel为input_c</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeExcitation</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_c: <span class="built_in">int</span>, squeeze_factor: <span class="built_in">int</span> = <span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SqueezeExcitation, self).__init__()</span><br><span class="line">        squeeze_c = _make_divisible(input_c // squeeze_factor, <span class="number">8</span>)</span><br><span class="line">        self.fc1 = nn.Conv2d(input_c, squeeze_c, <span class="number">1</span>)</span><br><span class="line">        self.fc2 = nn.Conv2d(squeeze_c, input_c, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>正向传播函数</strong></p>
<p>如上图（下）第三个特征矩阵需要进行池化操作，因此进行自适应池化下采样操作，并且返回特征矩阵的高宽是1x1。再将拿到的输出通过全连接层，relu激活函数，第二层全连接层，和一个h-sigmoid激活函数，得到输出。<strong>这里的输出就是第二层全连接层的输出，也就是得到了不同channel对应的权重</strong>。</p>
<p>接下来需要将输出与原特征矩阵相乘，得到通过SE模块之后的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor: <span class="comment"># 返回Tensor结构</span></span><br><span class="line">       scale = F.adaptive_avg_pool2d(x, output_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">       scale = self.fc1(scale)</span><br><span class="line">       scale = F.relu(scale, inplace=<span class="literal">True</span>)</span><br><span class="line">       scale = self.fc2(scale)</span><br><span class="line">       scale = F.hardsigmoid(scale, inplace=<span class="literal">True</span>)</span><br><span class="line">       <span class="keyword">return</span> scale * x</span><br></pre></td></tr></table></figure>
<h3 id="InvertedResidualConfig类">InvertedResidualConfig类</h3>
<p>InvertedResidualConfig对应的是MobileNetV3中的每一个bneck结构的参数配置，其中有</p>
<ul>
<li><code>input_c</code>：输入特征矩阵的大小（主要指channel）；</li>
<li><code>kernel</code>：每一层使用的kernel_size（即DW卷积中的卷积核大小）；</li>
<li><code>expanded_c</code>：第一层卷积层所采用的的卷积核的个数；</li>
<li><code>out_c</code>：最后一层1x1的卷积层输出得到特征矩阵的channel；</li>
<li><code>use_se</code>：是否使用SE注意力机制；</li>
<li><code>activation</code>：采用的激活函数，RE表示采用ReLU激活函数，HS表示采用H-Swish激活函数；</li>
<li><code>stride</code>：指的是DW卷积所对应的步距；</li>
<li><code>width_multi</code>：就是MNv2中提到的$\alpha$参数，用来调节每一个卷积层所使用channel的倍率因子。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualConfig</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 input_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 expanded_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_c: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 use_se: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">                 stride: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">        self.input_c = self.adjust_channels(input_c, width_multi)</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.expanded_c = self.adjust_channels(expanded_c, width_multi)</span><br><span class="line">        self.out_c = self.adjust_channels(out_c, width_multi)</span><br><span class="line">        self.use_se = use_se</span><br><span class="line">        self.use_hs = activation == <span class="string">&quot;HS&quot;</span>  <span class="comment"># whether using h-swish activation</span></span><br><span class="line">        self.stride = stride</span><br></pre></td></tr></table></figure>
<p><strong>adjust_channels函数</strong></p>
<p>是一个静态方法，其实也是直接调用了_make_divisible方法，传入参数为channel和$\alpha$倍率因子，最终得到channel和$\alpha$的乘积离8最近的整数倍的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adjust_channels</span>(<span class="params">channels: <span class="built_in">int</span>, width_multi: <span class="built_in">float</span></span>):</span><br><span class="line">	<span class="keyword">return</span> _make_divisible(channels * width_multi, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h3 id="InvertedResidual类">InvertedResidual类</h3>
<p><strong>初始化函数</strong></p>
<ul>
<li><code>cnf</code>：前文提到的InvertedResidualConfig配置文件；</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层</li>
<li><code>cnf.stride</code>：判断步距是否为1或2，因为在网络参数表中，步距只有1和2两种情况，当出现第三种情况时，就是不合法的步距情况；再判断</li>
<li><code>self.use_res_connect</code>：是否使用shortcut连接，shortcut只有在stride == 1且input_c == output_c时才有；</li>
<li><code>activation_layer</code>：判断使用ReLU或者H-Swish激活函数（官方是在1.7及以上版本中才有官方实现的H-Swish和H-Sigmoid激活函数，如果需要使用MNv3网络的话，得把pytorch版本更新至1.7及以上）</li>
<li>expand区域指在InvertedResidual结构中的第一个1x1卷积层进行升维处理，因为第一个kneck存在输入特征矩阵的channel和输出特征矩阵的channel相等，因此可以跳过，所以会进行判断cnf.expanded_c != cnf.input_c；</li>
<li>depthwise区域为dw卷积区域</li>
</ul>
<blockquote>
<p><code>groups</code>：由于DW卷积是针对每一个channel都单独使用一个channel为1的卷积核来进行卷及处理，所以groups和channel的个数是保持一致的，所以groups=cnf.expanded_c</p>
</blockquote>
<ul>
<li>project区域是InvertedResidual结构中1x1卷积中的降维部分，activation_layer=nn.Identity中的Identity其实就是线性y = x，没有做任何处理；</li>
<li></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 cnf: InvertedResidualConfig,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Callable</span>[..., nn.Module]</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cnf.stride <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;illegal stride value.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.use_res_connect = (cnf.stride == <span class="number">1</span> <span class="keyword">and</span> cnf.input_c == cnf.out_c)</span><br><span class="line"></span><br><span class="line">        layers: <span class="type">List</span>[nn.Module] = []</span><br><span class="line">        activation_layer = nn.Hardswish <span class="keyword">if</span> cnf.use_hs <span class="keyword">else</span> nn.ReLU</span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand</span></span><br><span class="line">        <span class="keyword">if</span> cnf.expanded_c != cnf.input_c:</span><br><span class="line">            layers.append(ConvBNActivation(cnf.input_c,</span><br><span class="line">                                           cnf.expanded_c,</span><br><span class="line">                                           kernel_size=<span class="number">1</span>,</span><br><span class="line">                                           norm_layer=norm_layer,</span><br><span class="line">                                           activation_layer=activation_layer))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># depthwise</span></span><br><span class="line">        layers.append(ConvBNActivation(cnf.expanded_c,</span><br><span class="line">                                       cnf.expanded_c,</span><br><span class="line">                                       kernel_size=cnf.kernel,</span><br><span class="line">                                       stride=cnf.stride,</span><br><span class="line">                                       groups=cnf.expanded_c,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=activation_layer))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cnf.use_se:</span><br><span class="line">            layers.append(SqueezeExcitation(cnf.expanded_c))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project</span></span><br><span class="line">        layers.append(ConvBNActivation(cnf.expanded_c,</span><br><span class="line">                                       cnf.out_c,</span><br><span class="line">                                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=nn.Identity))</span><br><span class="line"></span><br><span class="line">        self.block = nn.Sequential(*layers)</span><br><span class="line">        self.out_channels = cnf.out_c</span><br></pre></td></tr></table></figure>
<p><strong>正向传播函数</strong></p>
<p>将特征矩阵传入block方法得到主分支输出特征矩阵，再经过判断是否有shortcut，如果有则相加，无则直接输出主分支特征矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">	result = self.block(x)</span><br><span class="line">	<span class="keyword">if</span> self.use_res_connect:</span><br><span class="line">		result += x</span><br><span class="line">	<span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="定义MNv3类">定义MNv3类</h3>
<p><strong>初始化函数</strong></p>
<ul>
<li><code>inverted_residual_setting</code>：对应一系列bneck结构参数的列表；</li>
<li><code>last_channel</code>：对应是MNv3网络参数表中倒数第二个全连接层的输出节点的个数；</li>
<li><code>block</code>：就是之前定义的InvertedResidual模块；</li>
<li><code>norm_layer</code>：对应的在卷积后接的BN层；</li>
</ul>
<p>当inverted_residual_setting为空或者不是一个list的话都会报错。</p>
<p><strong>building first layer</strong>创建第一层conv2d</p>
<blockquote>
<p>firstconv_output_c获取第一个bneck结构的输入特征矩阵的channel，所对应的是第一个卷积层输出的channel；</p>
<p>ConvBNActivation对应的是第一个conv2d，无论是v3-Large还是v3-Small，第一个都是3x3的卷积层。所以先创建了一个3x3的卷积层。</p>
</blockquote>
<p><strong>building inverted residual blocks</strong>创建block模块</p>
<blockquote>
<p>遍历每一个bneck结构，将每一层的配置文件和norm_layer都传给block，将创建好的每一个block结构，也就是InvertedResidual模块给填进layers当中。</p>
</blockquote>
<p><strong>building last several layers</strong>创建平均池化下采样层和几个卷积层</p>
<blockquote>
<p><code>lastconv_input_c</code>：最后一个bneck模块的输出特征矩阵channel；</p>
<p><code>lastconv_output_c</code>：无论是v3-Large还是v3-Small，lastconv_output_c都是lastconv_input_c的6倍</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 inverted_residual_setting: <span class="type">List</span>[InvertedResidualConfig],</span></span><br><span class="line"><span class="params">                 last_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 block: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV3, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inverted_residual_setting:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;The inverted_residual_setting should not be empty.&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> (<span class="built_in">isinstance</span>(inverted_residual_setting, <span class="type">List</span>) <span class="keyword">and</span></span><br><span class="line">                  <span class="built_in">all</span>([<span class="built_in">isinstance</span>(s, InvertedResidualConfig) <span class="keyword">for</span> s <span class="keyword">in</span> inverted_residual_setting])):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;The inverted_residual_setting should be List[InvertedResidualConfig]&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            block = InvertedResidual</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = partial(nn.BatchNorm2d, eps=<span class="number">0.001</span>, momentum=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">        layers: <span class="type">List</span>[nn.Module] = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building first layer</span></span><br><span class="line">        firstconv_output_c = inverted_residual_setting[<span class="number">0</span>].input_c</span><br><span class="line">        layers.append(ConvBNActivation(<span class="number">3</span>,</span><br><span class="line">                                       firstconv_output_c,</span><br><span class="line">                                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                                       stride=<span class="number">2</span>,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=nn.Hardswish))</span><br><span class="line">        <span class="comment"># building inverted residual blocks</span></span><br><span class="line">        <span class="keyword">for</span> cnf <span class="keyword">in</span> inverted_residual_setting:</span><br><span class="line">            layers.append(block(cnf, norm_layer))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># building last several layers</span></span><br><span class="line">        lastconv_input_c = inverted_residual_setting[-<span class="number">1</span>].out_c</span><br><span class="line">        lastconv_output_c = <span class="number">6</span> * lastconv_input_c</span><br><span class="line">        layers.append(ConvBNActivation(lastconv_input_c,</span><br><span class="line">                                       lastconv_output_c,</span><br><span class="line">                                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                                       norm_layer=norm_layer,</span><br><span class="line">                                       activation_layer=nn.Hardswish))</span><br><span class="line">        self.features = nn.Sequential(*layers)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.classifier = nn.Sequential(nn.Linear(lastconv_output_c, last_channel),</span><br><span class="line">                                        nn.Hardswish(inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Dropout(p=<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Linear(last_channel, num_classes))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initial weights</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.ones_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br></pre></td></tr></table></figure>
<p><strong>正向传播函数</strong></p>
<p>将输入特征矩阵依次通过特征提取、平均池化、展平和classifier处理得到输出结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">return</span> self._forward_impl(x)</span><br></pre></td></tr></table></figure>
<h3 id="定义mobilenet-v3-large">定义mobilenet_v3_large</h3>
<ul>
<li><code>width_multi</code>：$\alpha$超参数，用来调整channel；</li>
<li><code>bneck_conf</code>：同样使用partial方法，给InvertedResidualConfig方法传入默认参数width_multi，即$\alpha$超参数；</li>
<li><code>adjust_channels</code>：即InvertedResidualConfig类中的adjust_channels方法，使用partial方法，给InvertedResidualConfig.adjust_channels方法传入默认参数width_multi，即$\alpha$超参数；</li>
<li><code>reduce_divider</code>：对应最后三层bneck结构，对卷积的channel进行了调整，这里默认为False，指不做调整，如果设置为True，可以减少一些参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_large</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Constructs a large MobileNetV3 architecture from</span></span><br><span class="line"><span class="string">    &quot;Searching for MobileNetV3&quot; &lt;https://arxiv.org/abs/1905.02244&gt;.</span></span><br><span class="line"><span class="string">    weights_link:</span></span><br><span class="line"><span class="string">    https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes</span></span><br><span class="line"><span class="string">        reduced_tail (bool): If True, reduces the channel counts of all feature layers</span></span><br><span class="line"><span class="string">            between C4 and C5 by 2. It is used to reduce the channel redundancy in the</span></span><br><span class="line"><span class="string">            backbone for Detection and Segmentation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    width_multi = <span class="number">1.0</span></span><br><span class="line">    bneck_conf = partial(InvertedResidualConfig, width_multi=width_multi)</span><br><span class="line">    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_multi=width_multi)</span><br><span class="line"></span><br><span class="line">    reduce_divider = <span class="number">2</span> <span class="keyword">if</span> reduced_tail <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    inverted_residual_setting = [</span><br><span class="line">        <span class="comment"># input_c, kernel, expanded_c, out_c, use_se, activation, stride</span></span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C1</span></span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">3</span>, <span class="number">72</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">5</span>, <span class="number">72</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C2</span></span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C3</span></span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">200</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">184</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">184</span>, <span class="number">80</span>, <span class="literal">False</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">80</span>, <span class="number">3</span>, <span class="number">480</span>, <span class="number">112</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">112</span>, <span class="number">3</span>, <span class="number">672</span>, <span class="number">112</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">112</span>, <span class="number">5</span>, <span class="number">672</span>, <span class="number">160</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C4</span></span><br><span class="line">        bneck_conf(<span class="number">160</span> // reduce_divider, <span class="number">5</span>, <span class="number">960</span> // reduce_divider, <span class="number">160</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">160</span> // reduce_divider, <span class="number">5</span>, <span class="number">960</span> // reduce_divider, <span class="number">160</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">    ]</span><br><span class="line">    last_channel = adjust_channels(<span class="number">1280</span> // reduce_divider)  <span class="comment"># C5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> MobileNetV3(inverted_residual_setting=inverted_residual_setting,</span><br><span class="line">                       last_channel=last_channel,</span><br><span class="line">                       num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<h3 id="定义mobilenet-v3-small">定义mobilenet_v3_small</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mobilenet_v3_small</span>(<span class="params">num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                       reduced_tail: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; MobileNetV3:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Constructs a large MobileNetV3 architecture from</span></span><br><span class="line"><span class="string">    &quot;Searching for MobileNetV3&quot; &lt;https://arxiv.org/abs/1905.02244&gt;.</span></span><br><span class="line"><span class="string">    weights_link:</span></span><br><span class="line"><span class="string">    https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes</span></span><br><span class="line"><span class="string">        reduced_tail (bool): If True, reduces the channel counts of all feature layers</span></span><br><span class="line"><span class="string">            between C4 and C5 by 2. It is used to reduce the channel redundancy in the</span></span><br><span class="line"><span class="string">            backbone for Detection and Segmentation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    width_multi = <span class="number">1.0</span></span><br><span class="line">    bneck_conf = partial(InvertedResidualConfig, width_multi=width_multi)</span><br><span class="line">    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_multi=width_multi)</span><br><span class="line"></span><br><span class="line">    reduce_divider = <span class="number">2</span> <span class="keyword">if</span> reduced_tail <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    inverted_residual_setting = [</span><br><span class="line">        <span class="comment"># input_c, kernel, expanded_c, out_c, use_se, activation, stride</span></span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="literal">True</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C1</span></span><br><span class="line">        bneck_conf(<span class="number">16</span>, <span class="number">3</span>, <span class="number">72</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">2</span>),  <span class="comment"># C2</span></span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">3</span>, <span class="number">88</span>, <span class="number">24</span>, <span class="literal">False</span>, <span class="string">&quot;RE&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">24</span>, <span class="number">5</span>, <span class="number">96</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C3</span></span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">240</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">240</span>, <span class="number">40</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">40</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">48</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">48</span>, <span class="number">5</span>, <span class="number">144</span>, <span class="number">48</span>, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">48</span>, <span class="number">5</span>, <span class="number">288</span>, <span class="number">96</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">2</span>),  <span class="comment"># C4</span></span><br><span class="line">        bneck_conf(<span class="number">96</span> // reduce_divider, <span class="number">5</span>, <span class="number">576</span> // reduce_divider, <span class="number">96</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        bneck_conf(<span class="number">96</span> // reduce_divider, <span class="number">5</span>, <span class="number">576</span> // reduce_divider, <span class="number">96</span> // reduce_divider, <span class="literal">True</span>, <span class="string">&quot;HS&quot;</span>, <span class="number">1</span>)</span><br><span class="line">    ]</span><br><span class="line">    last_channel = adjust_channels(<span class="number">1024</span> // reduce_divider)  <span class="comment"># C5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> MobileNetV3(inverted_residual_setting=inverted_residual_setting,</span><br><span class="line">                       last_channel=last_channel,</span><br><span class="line">                       num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<h2 id="train-py"><a href="http://train.py">train.py</a></h2>
<p>下载官方权重文件：输入import torchvision.model.mobilenet，ctrl+左键进入函数当中，会有显示下载链接</p>
<p>该训练脚本与前期使用的AlexNet、VGG、GooglNet以及ResNet所使用的训练脚本基本一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># 训练MobileNetv2</span></span><br><span class="line"><span class="keyword">from</span> model_v2 <span class="keyword">import</span> MobileNetV2</span><br><span class="line"><span class="comment"># 训练MobileNetv3-Large</span></span><br><span class="line"><span class="keyword">from</span> model_v3 <span class="keyword">import</span> mobilenet_v3_large</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; device.&quot;</span>.<span class="built_in">format</span>(device))</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">16</span></span><br><span class="line">    epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                   transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])&#125;</span><br><span class="line"></span><br><span class="line">    data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;..&quot;</span>))  <span class="comment"># get data root path</span></span><br><span class="line">    image_path = os.path.join(data_root, <span class="string">&quot;data_set&quot;</span>, <span class="string">&quot;flower_data&quot;</span>)  <span class="comment"># flower data set path</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;train&quot;</span>),</span><br><span class="line">                                         transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">    train_num = <span class="built_in">len</span>(train_dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line">    flower_list = train_dataset.class_to_idx</span><br><span class="line">    cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line">    <span class="comment"># write dict into json file</span></span><br><span class="line">    json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json_file.write(json_str)</span><br><span class="line"></span><br><span class="line">    nw = <span class="built_in">min</span>([os.cpu_count(), batch_size <span class="keyword">if</span> batch_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, <span class="number">8</span>])  <span class="comment"># number of workers</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="built_in">format</span>(nw))</span><br><span class="line"></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                               batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                               num_workers=nw)</span><br><span class="line"></span><br><span class="line">    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>),</span><br><span class="line">                                            transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">    val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">    validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                                  batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                  num_workers=nw)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(train_num,</span><br><span class="line">                                                                           val_num))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    <span class="comment"># 训练MobileNetv2</span></span><br><span class="line">    net = MobileNetV2(num_classes=<span class="number">5</span>)</span><br><span class="line">	<span class="comment"># 训练MobileNetv3-Large</span></span><br><span class="line">    net = mobilenet_v3_large(num_classes=<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># load pretrain weights</span></span><br><span class="line">    <span class="comment"># download url: https://download.pytorch.org/models/mobilenet_v2-b0353104.pth</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./mobilenet_v2.pth&quot;</span> <span class="comment">#&quot;./mobilenet_v3_large.pth&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; dose not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line">    pre_weights = torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># delete classifier weights</span></span><br><span class="line">    pre_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pre_weights.items() <span class="keyword">if</span> net.state_dict()[k].numel() == v.numel()&#125;</span><br><span class="line">    missing_keys, unexpected_keys = net.load_state_dict(pre_dict, strict=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># freeze features weights</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.features.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    net.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define loss function</span></span><br><span class="line">    loss_function = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># construct an optimizer</span></span><br><span class="line">    params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    optimizer = optim.Adam(params, lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line">    save_path = <span class="string">&#x27;./MobileNetV2.pth&#x27;</span> <span class="comment"># save_path = &#x27;./MobileNetV3.pth&#x27;</span></span><br><span class="line">    train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># train</span></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        train_bar = tqdm(train_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">            images, labels = data</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            logits = net(images.to(device))</span><br><span class="line">            loss = loss_function(logits, labels.to(device))</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print statistics</span></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                     epochs,</span><br><span class="line">                                                                     loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">            <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">                val_images, val_labels = val_data</span><br><span class="line">                outputs = net(val_images.to(device))</span><br><span class="line">                <span class="comment"># loss = loss_function(outputs, test_labels)</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">                val_bar.desc = <span class="string">&quot;valid epoch[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                           epochs)</span><br><span class="line">        val_accurate = acc / val_num</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">            best_acc = val_accurate</span><br><span class="line">            torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>**需要注意的不同点：**模型权重加载部分</p>
<p>首先实例化模型，将类别个数设置为5；</p>
<p>通过torch.load函数载入预训练参数，载入进之后是一个字典类型。因为官方是在ImageNet数据集上进行预训练，所以最后一层全连接层的节点个数 = 1000，而我们这最后一层节点个数 = 5，所以最后一层不能用。</p>
<p>因此首先遍历权重字典，查找权重名称中是否含有classifier参数，如果有这个参数，说明是最后一层全连接层的参数。如果没有classifier，则直接保存进pre_dict字典变量当中。</p>
<p>再通过net.load_state_dict函数将不包含classifier全连接层的权重字典全部载入进去。</p>
<p>之后冻结特征提取部分的所有权重，通过遍历net.features下的所有参数，将参数的requires_grad全部设置为False，这样就不会对其进行求导及参数更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create model</span></span><br><span class="line"> net = MobileNetV2(num_classes=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># load pretrain weights</span></span><br><span class="line"> <span class="comment"># download url: https://download.pytorch.org/models/mobilenet_v2-b0353104.pth</span></span><br><span class="line"> model_weight_path = <span class="string">&quot;./mobilenet_v2.pth&quot;</span></span><br><span class="line"> <span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; dose not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line"> pre_weights = torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># delete classifier weights</span></span><br><span class="line"> pre_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pre_weights.items() <span class="keyword">if</span> <span class="string">&quot;classifier&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> k&#125;</span><br><span class="line"> missing_keys, unexpected_keys = net.load_state_dict(pre_dict, strict=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line"> <span class="comment"># freeze features weights</span></span><br><span class="line"> <span class="keyword">for</span> param <span class="keyword">in</span> net.features.parameters():</span><br><span class="line">     param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="MNv2训练结果"><strong>MNv2训练结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv2%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="MNv2训练结果"></p>
<h3 id="MNv3-Large训练结果"><strong>MNv3-Large训练结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv3%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.png" alt="MNv3训练结果"></p>
<h2 id="predict-py"><a href="http://predict.py">predict.py</a></h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># MNv2</span></span><br><span class="line"><span class="keyword">from</span> model_v2 <span class="keyword">import</span> MobileNetV2</span><br><span class="line"><span class="comment"># MNv3-Large</span></span><br><span class="line"><span class="keyword">from</span> model_v3 <span class="keyword">import</span> mobilenet_v3_large</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    data_transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize(<span class="number">256</span>),</span><br><span class="line">         transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load image</span></span><br><span class="line">    img_path = <span class="string">&quot;tulip.jpg&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    img = data_transform(img)</span><br><span class="line">    <span class="comment"># expand batch dimension</span></span><br><span class="line">    img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read class_indict</span></span><br><span class="line">    json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create model</span></span><br><span class="line">    <span class="comment"># MNv2</span></span><br><span class="line">    model = MobileNetV2(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line">    <span class="comment"># MNv3-Large</span></span><br><span class="line">    model = mobilenet_v3_large(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line">    <span class="comment"># load model weights</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./MobileNetV2.pth&quot;</span> <span class="comment"># model_weight_path = &quot;./MobileNetV3.pth&quot;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_weight_path, map_location=device))</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># predict class</span></span><br><span class="line">        output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">        predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">        predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">    print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                                 predict[predict_cla].numpy())</span><br><span class="line">    plt.title(print_res)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                                  predict[i].numpy()))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="MNv2预测结果"><strong>MNv2预测结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv2%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="MNv2预测结果"></p>
<h3 id="MNv3-Large预测结果"><strong>MNv3-Large预测结果</strong></h3>
<p><img src="/2023/05/23/%E4%BD%BF%E7%94%A8pytorch%E6%90%AD%E5%BB%BAMobileNetV2V3%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/MNv3%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png" alt="MNv3预测结果"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Pytorch搭建CNN</tag>
      </tags>
  </entry>
</search>
